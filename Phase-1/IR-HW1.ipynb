{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"direction:rtl;line-height:300%;\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "<div align=center>\n",
    "<font face=\"B Titr\" size=5>\n",
    "<p></p><p></p>\n",
    "بسمه تعالی\n",
    "<p></p>\n",
    "</font>\n",
    "<p></p>\n",
    "<font>\n",
    "<br>\n",
    "درس بازیابی پیشرفته اطلاعات\n",
    "<br>\n",
    "مدرس: دکتر بیگی\n",
    "</font>\n",
    "<p></p>\n",
    "<br>\n",
    "<font>\n",
    "<b>تمرین اول</b>\n",
    "</font>\n",
    "<br>\n",
    "<br>\n",
    "موعد تحویل: ۱۵ آبان <br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font>\n",
    "دانشگاه صنعتی شریف\n",
    "<br>\n",
    "دانشکده مهندسی کامپیوتر\n",
    "<br>\n",
    "<br>\n",
    "</font>\n",
    "</div>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "<font face=\"XB Zar\" size=6>\n",
    "<h1>مقدمه</h1>\n",
    "</font>\n",
    "<font face=\"XB Zar\" size=3>\n",
    "این تمرین به پیش‌پردازش متن، اصلاح پرسمان، ساخت نمایه، بازیابی boolean و فشرده‌سازی نمایه می‌پردازد.\n",
    "<br>\n",
    "دیتاستی که در اختبار شما قرار گرفته است شامل چکیده مقالات و id آن‌ها می‌باشد.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> آماده‌سازی دیتاست</h1>\n",
    "<p>\n",
    "دیتاستی که در اختیار شما قرار گرفته است، دارای سطر‌هایی می‌باشد که دارای مقدار NaN می‌باشد. برای اینکه بتوانید با این دیتاست کار کنید، باید ابتدا این سطر‌ها را حذف کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In the recent years many applications have eme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Many network-on-chip (NoC) designs focus on ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Parallelizing the memory accesses in a nested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This work surveys the major methods for model-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Failure detectors are classical mechanisms whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paperId                                           abstract\n",
       "0        1  In the recent years many applications have eme...\n",
       "1        2  Many network-on-chip (NoC) designs focus on ma...\n",
       "2        3  Parallelizing the memory accesses in a nested ...\n",
       "3        4  This work surveys the major methods for model-...\n",
       "4        5  Failure detectors are classical mechanisms whi..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset and remove rows with missing values\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3089.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1783.579313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1545.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3089.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4633.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6178.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           paperId\n",
       "count  6178.000000\n",
       "mean   3089.500000\n",
       "std    1783.579313\n",
       "min       1.000000\n",
       "25%    1545.250000\n",
       "50%    3089.500000\n",
       "75%    4633.750000\n",
       "max    6178.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paperId      0\n",
       "abstract    16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paperId</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>In the recent years many applications have eme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Many network-on-chip (NoC) designs focus on ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Parallelizing the memory accesses in a nested ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This work surveys the major methods for model-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Failure detectors are classical mechanisms whi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paperId                                           abstract\n",
       "0        1  In the recent years many applications have eme...\n",
       "1        2  Many network-on-chip (NoC) designs focus on ma...\n",
       "2        3  Parallelizing the memory accesses in a nested ...\n",
       "3        4  This work surveys the major methods for model-...\n",
       "4        5  Failure detectors are classical mechanisms whi..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the rows with missing values\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> Preprocessing (پیش پردازش)</h1>\n",
    "<p>\n",
    "بسیاری از داده ‌ها دارای مقادیر زیادی اطلاعات اضافه هستند که در پردازش ها به آن نیازی نیست و یا باعث ایجاد خطا میشوند.\n",
    "دراین بخش داده را از دیتابیس مورد نظر خوانده\n",
    "  و سپس پیش پردازش های مورد نیاز را اعمال کنید تا متن پیش پردازش شده را تولید کنید.\n",
    "پس از اتمام پیش پردازش سایر عملیات گفته شده در ادامه را بر\n",
    "روی متن ایجاد شده انجام میدهیم.\n",
    "\n",
    "\n",
    "کلاس\n",
    "\"Preprocessor\"\n",
    "عملیات پیش پردازش را انجام میدهد. نام توابع عمل های مورد نظر نوشته شده است و که با توجه به آن باید کد مخصوص هر یک نوشته شود. تابع\n",
    "\"preprocessor\"\n",
    "تابع اصلی این کلاس است که متن بدون پیش پردازش را گرفته و پردازش های مورد نظر را در آن اعمال میکند و متن مورد نظر را ایجاد میکند.\n",
    "\n",
    "در این بخش میتوانید از کتابخانه های آماده مانند\n",
    "<a href=\"https://www.nltk.org/\">NLTK</a>\n",
    "و\n",
    "<a href=\"https://spacy.io/\">SpaCy</a>\n",
    "استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import string\n",
    "\n",
    "class Preprocessor:\n",
    "\n",
    "    def __init__(self, stopwords_path=None):\n",
    "        # Create a variable of stop words.\n",
    "        if stopwords_path:\n",
    "            with open(stopwords_path, \"r\") as f:\n",
    "                self.stopwords = f.read().split(\"\\n\")\n",
    "        else:\n",
    "            self.stopwords = list(stopwords.words(\"english\"))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # The main function of the class.\n",
    "        text = self.normalize(text)\n",
    "        text = self.remove_links(text)\n",
    "        tokens = self.word_tokenize(text)\n",
    "        words = self.remove_punctuations(tokens)\n",
    "        words = self.remove_stopwords(words)\n",
    "        return words\n",
    "\n",
    "    def normalize(self, text=\"\", words=[]):\n",
    "        # Normalize text (lower case, stemming, lemmatization, etc.)\n",
    "        \n",
    "        if not words:\n",
    "            # tokenize the text\n",
    "            words = wordpunct_tokenize(text)\n",
    "        \n",
    "        # lower case the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # stemming\n",
    "        # stemmer = SnowballStemmer(\"english\")\n",
    "        # stemmed = [stemmer.stem(word) for word in words]\n",
    "        \n",
    "        \n",
    "        # lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
    "        lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "        \n",
    "        normalized_text = \" \".join(lemmatized)\n",
    "        \n",
    "        return normalized_text\n",
    "\n",
    "    def remove_links(self, text):\n",
    "        # Remove links\n",
    "        text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "        return text\n",
    "\n",
    "    def remove_punctuations(self, tokens):\n",
    "        # Remove punctutaions\n",
    "        words = [x for x in tokens if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "        return words\n",
    "\n",
    "    def word_tokenize(self, text):\n",
    "        # Tokenize text\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word not in self.stopwords]\n",
    "        return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>ساخت نمایه</h1>\n",
    "<p>\n",
    "شما در حال توسعه یک موتور جستجوی سریع هستید که از نمایه سازی پویا پشتیبانی می کند. موتور جستجو اسناد جدید را در قالب دسته‌هایی کوچک‌تر   \n",
    "(batch) هندل می‌کند. در پایان هر روز، این دسته‌ها با استفاده از استراتژی ادغام لگاریتمی ادغام می شوند. هدف به حداقل رساندن هزینه ادغام است.  \n",
    "مراحلی که باید برای حل این مسئله انجام دهید عبارتند از:\n",
    "<li>توکن‌بندی و نرمال‌سازی متن از اسناد.</li>\n",
    "    <li>ایجاد یک index مرتب‌ شده برای هر دسته از اسناد.</li>\n",
    "    <li>ادغام بهینه چند دسته از indexها با استفاده از یک استراتژی ادغام لگاریتمی.</li>\n",
    "وظیفه شما این است که بخش‌های خالی <strong>(مشخص شده به‌صورت {TODO})</strong> کد را پر کنید تا موتور جستجو عملی شود.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h3> دستورات </h3>\n",
    "<li>متد <code>sort_based_index_construction</code> از <code>DocumentBatch</code>:</li>\n",
    "<p>هر سند را با استفاده از تابع‌هایی که در قسمت قبل نوشتید، عمل preprocessing را روی آن انجام دهید.</p>\n",
    "<p>برای هر توکن، شناسه سند را به فهرست معکوس (inverted index) برای آن توکن اضافه کنید.</p>\n",
    "<li>متد <code>add_batch</code> در <code>FastSearchEngine</code></li>\n",
    "<p>فهرست معکوس برای دسته (batch) را ایجاد کنید.</p>\n",
    "<p>این دسته را به فهرست‌های روزانه اضافه کنید.</p>\n",
    "<li>متد <code>end_of_day_merge</code> از <code>FastSearchEngine:</code></li>\n",
    "<p>استراتژی ادغام لگاریتمی را پیاده‌سازی کنید تا به صورت بهینه فهرست‌های روزانه را با فهرست اصلی ادغام کنید.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'set'>, {'In': {1, 3, 5, 7, 8, 10, 15, 17, 20}, 'recent': {8, 1}, 'year': {17, 1, 14}, 'many': {16, 1, 20, 7, 12}, 'application': {1, 2, 3, 10, 11, 12, 13, 15}, 'emerged': {1}, 'demand': {1, 14, 15}, 'communicating': {1}, 'large': {3, 1, 11, 20}, 'number': {17, 1, 3, 7, 11, 14, 15}, 'small': {1, 11, 14}, 'size': {1, 10, 15}, 'low': {1, 3, 20, 13}, 'cost': {17, 1, 20, 4, 10, 14, 15}, 'node': {1, 15}, 'becomes': {1}, 'necessity': {1}, 'Such': {1}, 'involve': {1}, 'example': {1, 11}, 'radio': {1}, 'frequency': {17, 12, 1, 15}, 'identification': {1}, 'RFID': {1}, 'smart': {9, 1}, 'card': {1}, 'network': {16, 1, 2, 5, 7, 11, 15}, 'even': {1}, 'mobile': {1, 5, 15}, 'computing': {1}, 'device': {1, 6, 15}, 'general': {1}, 'A': {17, 1, 6, 9, 10}, 'critical': {17, 1, 2, 3, 14}, 'energy': {9, 2, 19, 1}, 'constraint': {17, 1}, 'imposed': {1}, 'communication': {16, 1, 5, 15}, 'access': {1, 2, 3, 10, 12, 15}, 'protocol': {1}, 'used': {1, 18, 2, 3, 6, 10, 15}, 'system': {1, 5, 6, 9, 10, 11, 12, 14, 15, 17, 20}, 'total': {1}, 'time': {1, 3, 4, 7, 9, 10, 12, 14, 15, 16, 17, 19}, 'need': {1, 10}, 'active': {1, 4}, 'transmitting': {1}, 'receiving': {1}, 'information': {1, 18, 4, 5, 12}, 'minimized': {1}, 'Another': {1}, 'area': {17, 19, 1, 15}, 'great': {1}, 'interest': {1, 5}, 'broadcast': {1}, 'based': {1, 4, 6, 7, 9, 12, 13, 15, 16, 19}, 'compared': {1, 2, 3, 20, 10}, 'traditional': {1, 12}, 'unicast': {1}, 'data': {1, 18, 2, 3, 10, 12, 15}, 'transfer': {1}, 'much': {1, 12}, 'efficient': {17, 1, 2, 3, 8, 9, 12}, 'disseminating': {1}, 'high': {1, 3, 5, 7, 10, 11, 13, 19}, 'degree': {1, 10, 6}, 'commonality': {1}, 'among': {1, 3, 20}, 'Unlike': {1, 14}, 'previous': {1, 18, 7}, 'work': {1, 2, 4, 7, 8, 10, 11, 17, 18, 19}, 'conserving': {1}, 'propose': {1, 3, 20, 10, 12, 14}, 'evaluate': {1, 18}, 'slot': {1}, 'state': {1, 3, 20, 5, 7, 10, 11}, 'delay': {1, 19}, 'kept': {1}, 'Many': {2}, 'chip': {16, 2, 3, 20, 7}, 'NoC': {16, 2, 7}, 'design': {17, 2, 19, 5, 6, 8, 15}, 'focus': {2}, 'maximizing': {2}, 'performance': {2, 10, 11, 12, 13, 15, 17, 20}, 'delivering': {17, 2}, 'core': {17, 2, 20, 7, 11, 14, 15}, 'later': {2}, 'needed': {2}, 'Yet': {2}, 'achieve': {2, 11, 12}, 'greater': {2}, 'efficiency': {17, 2, 8, 12, 14}, 'argue': {2}, 'important': {17, 2}, 'delivered': {2}, 'earlier': {2, 7}, 'To': {2, 8, 10, 11, 15}, 'address': {16, 2}, 'explore': {2}, 'criticality': {2}, 'CMPs': {2}, 'Caches': {2}, 'fetch': {2}, 'bulk': {2, 11}, 'block': {2}, 'multiple': {2, 3, 15}, 'word': {2}, 'Depending': {2}, 'memory': {17, 2, 3, 10, 12, 15}, 'pattern': {2, 3, 7}, 'right': {2}, 'away': {2}, 'fetched': {2}, 'soon': {2}, 'non': {2}, 'On': {2}, 'wide': {17, 2, 3}, 'range': {17, 2, 3}, 'perform': {2, 20}, 'limit': {2}, 'study': {8, 9, 2}, 'impact': {2, 20, 14}, 'Criticality': {2}, 'oblivious': {2}, 'waste': {2}, '37': {2, 3}, '5': {2, 3, 6, 11, 12}, 'idealized': {2}, 'Furthermore': {2}, '62': {2}, '3': {8, 2, 12, 6}, 'wasted': {2}, 'fetching': {2}, 'We': {2, 3, 4, 7, 9, 10, 12, 14}, 'present': {11, 2, 19, 7}, 'NoCNoC': {2}, 'practical': {2}, 'aware': {2}, 'achieves': {2, 11}, '60': {2}, 'saving': {2, 14}, 'loss': {2}, 'Our': {2, 12, 14}, 'move': {2, 4, 12}, 'towards': {2}, 'ideally': {2}, 'Massively': {11}, 'multithreaded': {11}, 'GPUs': {10, 11}, 'throughput': {11, 20, 6}, 'running': {11, 14}, 'thousand': {11}, 'thread': {11}, 'parallel': {3, 10, 11}, 'fully': {11, 5}, 'utilize': {11}, 'hardware': {8, 11}, 'contemporary': {10, 11}, 'workload': {11, 20, 12, 14}, 'spawn': {11}, 'GPU': {10, 11}, 'launching': {11}, 'task': {9, 11, 20}, 'kernel': {11}, 'contains': {11}, 'occupy': {11}, 'entire': {11, 20}, 'face': {11}, 'severe': {11}, 'underutilization': {11}, 'benefit': {10, 11}, 'vanish': {11}, 'narrow': {11}, 'e': {9, 11, 15}, 'contain': {11}, 'le': {3, 20, 7, 8, 11}, '512': {11}, 'Latency': {11}, 'sensitive': {10, 11, 14}, 'signal': {9, 11, 6}, 'image': {11, 18, 3, 6}, 'processing': {11, 3, 20, 15}, 'generate': {11}, 'relatively': {9, 11}, 'input': {8, 11, 6}, 'limited': {11}, 'parallelism': {11}, 'This': {4, 6, 11, 12, 15, 16, 17, 18, 19}, 'article': {16, 11}, 'Pagoda': {11}, 'runtime': {10, 11}, 'virtualizes': {11}, 'resource': {3, 11, 12, 14, 15}, 'using': {19, 3, 8, 11, 14, 15}, 'OS': {11}, 'like': {8, 11}, 'daemon': {11}, 'called': {11}, 'MasterKernel': {11}, 'Tasks': {11}, 'spawned': {11}, 'CPU': {11, 14}, 'onto': {11}, 'become': {16, 17, 11}, 'available': {11, 14}, 'scheduled': {11}, 'warp': {11}, 'granularity': {11}, 'level': {11, 3, 12}, 'control': {9, 11}, 'enables': {9, 11}, 'keep': {10, 11}, 'scheduling': {11}, 'executing': {11}, 'long': {9, 11}, 'free': {11, 6}, 'found': {11}, 'dramatically': {11}, 'reducing': {11}, 'Experimental': {3, 11, 20, 12}, 'result': {3, 5, 6, 7, 8, 11, 13, 14, 20}, 'real': {11, 15}, 'demonstrate': {11, 9, 19, 13}, 'geometric': {11}, 'mean': {11}, 'speedup': {11}, '52X': {11}, 'PThreads': {11}, '20': {11, 6}, '1': {8, 11, 10, 3}, '76X': {11}, 'CUDA': {11}, 'HyperQ': {11}, '44X': {11}, 'GeMTC': {11}, 'art': {3, 10, 11}, 'Emerging': {12}, 'hybrid': {12, 6}, 'comprise': {12}, 'technology': {8, 12, 15, 7}, 'Intel': {12}, '’': {12}, 'Optane': {12}, 'DC': {12}, 'Persistent': {12}, 'Memory': {12, 15}, 'exhibit': {12}, 'disparity': {12}, 'speed': {19, 12, 13, 15}, 'capacity': {10, 12}, 'ratio': {12}, 'heterogeneous': {12, 15}, 'component': {10, 3, 12, 6}, 'break': {10, 12}, 'assumption': {12, 5}, 'heuristic': {12}, 'designed': {12, 6}, 'DRAM': {12, 15}, 'platform': {12, 15}, 'High': {12}, 'feasible': {12}, 'via': {3, 12}, 'dynamic': {17, 3, 5, 9, 12}, 'movement': {12}, 'across': {10, 3, 20, 12}, 'unit': {10, 12}, 'maximizes': {9, 12}, 'use': {12}, 'ensuring': {10, 12}, 'aggregate': {12}, 'Newly': {12}, 'proposed': {3, 7, 8, 9, 12, 13, 17, 20}, 'solution': {16, 4, 12, 7}, 'model': {19, 4, 5, 9, 10, 12}, 'machine': {12, 14}, 'intelligence': {12}, 'optimize': {9, 12}, 'dynamically': {12, 14}, 'However': {8, 12, 15}, 'decision': {9, 4, 12}, 'empirical': {12}, 'selection': {12}, 'interval': {12}, 'left': {12}, 'experimental': {12}, 'evaluation': {12}, 'show': {18, 20, 7, 12, 14}, 'failure': {12, 5}, 'properly': {12}, 'conFigure': {12}, 'lead': {3, 12, 7}, '10': {12, 14}, '100': {8, 12}, 'degradation': {12}, 'given': {12}, 'policy': {9, 12, 15}, 'yet': {8, 12}, 'established': {12}, 'methodology': {16, 19, 12}, 'value': {12}, 'Cori': {12}, 'tuning': {12}, 'identifies': {12}, 'extract': {12}, 'necessary': {12, 7}, 'reuse': {3, 12}, 'guide': {12}, 'deliver': {12}, 'gain': {17, 4, 12, 14}, 'configures': {12}, 'provide': {18, 5, 7, 9, 10, 12}, 'within': {12, 6}, 'optimal': {9, 12, 13, 7}, 'one': {9, 3, 12, 7}, '\\\\': {12, 7}, 'quickly': {12}, 'random': {12}, 'brute': {12}, 'force': {12}, 'approach': {3, 20, 8, 9, 12}, 'System': {12}, 'validation': {12}, 'PMEM': {12}, 'confirms': {12}, 'practicality': {12}, 'Parallelizing': {3}, 'nested': {3}, 'loop': {3}, 'challenge': {16, 3, 14}, 'facilitate': {3}, 'pipelining': {3}, 'An': {3, 13, 7}, 'effective': {10, 3}, 'synthesis': {3}, 'field': {3}, 'programmable': {3}, 'gate': {3}, 'array': {3, 6}, 'map': {3}, 'bank': {3}, 'partitioning': {3}, 'technique': {17, 19, 20, 3, 8, 14}, 'paper': {19, 20, 3, 5, 7, 8}, 'algorithm': {3, 7}, 'overhead': {8, 3, 20}, 'complexity': {3, 20}, 'find': {10, 3}, 'video': {3}, 'amount': {3, 14}, 'reused': {3}, 'different': {3, 4, 7, 10, 14, 15}, 'iteration': {3}, 'nest': {3}, 'Motivated': {3}, 'observation': {3}, 'cache': {3}, 'reusable': {3}, 'register': {3}, 'organized': {3}, 'chain': {3}, 'The': {3, 4, 6, 8, 9, 10, 13, 14, 15, 16, 17, 20}, 'nonreusable': {3}, 'separated': {3}, 'several': {18, 3}, 'revise': {3}, 'existing': {3, 20, 15}, 'padding': {3}, 'method': {3, 4, 13}, 'cover': {3}, 'case': {3, 15}, 'occurring': {3}, 'frequently': {3}, 'wherein': {3}, 'certain': {16, 3, 7}, 'partition': {3}, 'vector': {3}, 'zero': {3}, 'demonstrated': {3}, 'term': {3, 15}, 'execution': {3, 15}, 'power': {3, 6, 7, 13, 15, 17, 19, 20}, 'consumption': {3, 9, 19, 7}, 'extracted': {3}, 'As': {3, 20}, 'testing': {3}, 'typically': {3, 15}, 'millisecond': {3}, 'And': {3}, 'required': {3}, 'reduced': {3}, '59': {3}, '7': {10, 3}, 'average': {9, 10, 3}, 'reduction': {3}, '78': {3}, '2': {8, 3, 6, 7}, 'look': {3}, 'table': {3}, '65': {8, 3}, 'flip': {3}, 'flop': {3}, 'DSP48Es': {3}, 'therefore': {3}, '74': {3}, '8': {8, 3}, 'Moreover': {3, 15}, 'storage': {3}, 'incurred': {3}, 'widely': {3}, 'filtering': {3}, 'survey': {4}, 'major': {4}, 'sensing': {4}, 'robotics': {4}, 'Active': {4}, 'incorporates': {4}, 'following': {4, 6}, 'aspect': {4, 6}, 'position': {4}, 'sensor': {4}, 'ii': {4}, 'make': {4}, 'next': {4}, 'action': {4}, 'order': {4}, 'maximize': {4}, 'minimize': {4, 7}, 'concentrate': {4}, 'second': {4}, '“': {4}, 'Where': {4}, 'robot': {4}, 'step': {17, 4}, '”': {4}, 'emphasis': {4}, 'Bayesian': {4}, 'problem': {9, 4, 5}, 'Pros': {4}, 'con': {4}, 'discussed': {4, 6}, 'Special': {4}, 'attention': {4}, 'paid': {4}, 'criterion': {4}, 'making': {4}, 'iterative': {13}, 'decoding': {13}, 'architecture': {17, 13}, 'stochastic': {9, 13}, 'computational': {13}, 'element': {13, 15}, 'Simulation': {17, 13}, 'simple': {13}, 'density': {17, 13}, 'parity': {8, 13}, 'check': {13}, 'code': {8, 13}, 'near': {13}, 'respect': {13}, 'maximum': {13, 6, 15}, 'likelihood': {13}, 'decoder': {13}, 'provides': {9, 10, 13}, 'alternative': {13}, 'analogue': {13}, 'increase': {9, 20, 17, 14}, 'public': {14}, 'cloud': {14}, 'datacenters': {14}, 'harvesting': {14}, 'allocated': {14}, 'temporarily': {14}, 'idling': {14}, 'customer': {14}, 'virtual': {14}, 'VMs': {14}, 'run': {14, 15}, 'batch': {14}, 'analytics': {14}, 'Even': {14}, 'translate': {17, 14}, 'substantial': {17, 14}, 'since': {10, 14}, 'provisioning': {14}, 'operating': {17, 14}, 'datacenter': {14}, 'hundred': {14}, 'million': {14}, 'dollar': {14}, 'per': {14}, 'main': {17, 14, 15}, 'harvest': {14}, 'idle': {14}, 'little': {14}, 'could': {8, 14}, 'latency': {14, 15}, 'service': {9, 14}, 'essentially': {14}, 'black': {14}, 'box': {14}, 'provider': {14}, 'introduce': {14}, 'ElasticVM': {14}, 'new': {16, 17, 5, 7, 14}, 'VM': {14}, 'type': {18, 14, 7}, 'cheaply': {14}, 'mainly': {14}, 'harvested': {14}, 'also': {17, 18, 19, 7, 14}, 'SmartHarvest': {14}, 'manages': {14}, 'ElasticVMs': {14}, 'fine': {14}, 'grained': {14}, 'window': {14}, 'us': {10, 14}, 'online': {14}, 'learning': {14}, 'predict': {14}, 'primary': {17, 14}, 'compute': {14}, 'safely': {14}, 'significant': {17, 14}, 'without': {20, 14}, 'increasing': {14, 15}, '99th': {14}, 'percentile': {14}, 'tail': {14}, 'static': {9, 17, 14}, 'rely': {14}, 'offline': {14}, 'profiling': {14}, 'robust': {14}, 'load': {9, 10, 14}, 'change': {14}, 'Finally': {14}, 'complementary': {6, 14}, 'optimization': {19, 14}, 'management': {20, 14}, 'Failure': {5}, 'detector': {5, 6}, 'classical': {5}, 'mechanism': {9, 5}, 'process': {17, 5}, 'help': {5, 15}, 'cope': {5}, 'self': {5, 6}, 'organizing': {5}, 'unstructured': {5}, 'wireless': {5}, 'Unreliable': {5}, 'class': {5}, '◊': {5}, 'S': {5}, 'special': {16, 10, 5}, 'meet': {16, 9, 5, 15}, 'weakest': {5}, 'able': {5}, 'solve': {5}, 'fundamental': {5}, 'dependable': {5}, 'Unfortunately': {5}, 'negative': {5}, 'implemented': {5, 6, 7}, 'unknown': {5}, 'membership': {5}, 'full': {5}, 'knowledge': {5}, 'well': {5}, 'connectivity': {5}, 'longer': {5}, 'appropriate': {5}, 'scenario': {5}, 'discussion': {5}, 'condition': {5, 7}, 'implement': {5}, 'define': {5}, 'namely': {5}, 'SM': {5}, 'adapts': {5}, 'property': {5}, 'four': {6}, 'stage': {6}, 'unidirectional': {6}, 'ring': {6}, 'space': {6}, 'optical': {6, 7}, 'interconnect': {16, 6, 15}, 'wa': {17, 6, 7}, 'analyzed': {6, 15}, 'characterized': {6}, 'metal': {6}, 'oxide': {6}, 'semiconductor': {6}, 'electro': {6}, 'optic': {6}, 'effect': {6}, 'backplane': {6}, 'demonstrator': {6}, 'fit': {6}, 'standard': {6}, 'VME': {6}, 'chassis': {6}, 'microlens': {6}, 'macrolens': {6}, 'relay': {6}, 'arranged': {6}, 'lens': {6}, 'waist': {6}, 'configuration': {6}, 'route': {6}, 'beam': {6}, 'supply': {10, 6}, 'transceiver': {6}, 'telecentric': {6}, 'parameter': {6, 15}, 'mapping': {6}, 'two': {15, 6, 7}, 'dimensional': {6}, 'alignment': {6}, 'tolerance': {6}, 'budget': {6}, 'overall': {6}, 'implementation': {8, 16, 6, 15}, 'including': {6}, 'characterization': {6}, 'subsystem': {6, 15}, 'prealignment': {6}, 'final': {6}, 'assembly': {6}, 'presented': {6}, 'adjusted': {6}, 'rotational': {6}, 'error': {8, 6}, '0': {10, 6}, '05': {6}, 'lateral': {6}, 'offset': {6}, 'mum': {6}, 'measured': {6}, 'good': {6}, 'agreement': {6}, 'lower': {6}, 'bound': {9, 18, 6, 15}, 'prediction': {6}, 'obtained': {8, 6}, 'theoretical': {6}, 'dB': {6}, 'fiber': {6}, 'modulator': {6}, '25': {6}, 'plane': {6}, 'Scalable': {15}, 'Bandwidth': {15}, 'Efficient': {15}, 'Subsystem': {15}, 'Design': {15}, 'Real': {15}, 'Time': {15}, 'Systems': {15}, 'multi': {17, 15}, 'processor': {17, 15, 7}, 'Dynamic': {17, 15}, 'Random': {15}, 'Access': {15}, 'shared': {10, 15}, 'reduce': {17, 15, 7}, 'enable': {15}, 'client': {15}, 'Since': {15}, 'firm': {15}, 'requirement': {15}, 'concurrently': {15}, 'impose': {15}, 'strict': {15}, 'worst': {15}, 'bandwidth': {10, 15}, 'These': {15}, 'must': {17, 15}, 'guaranteed': {15}, 'verification': {15}, 'effort': {15}, 'made': {15}, 'possible': {15}, 'consisting': {15}, 'controller': {15}, 'front': {15}, 'multiplex': {15}, 'request': {9, 10, 20, 15}, 'arriving': {15}, 'Existing': {15}, 'fixing': {15}, 'burst': {15}, 'page': {15}, 'response': {8, 9, 15}, 'predictable': {15}, 'arbitration': {15}, 'Division': {15}, 'Multiplexing': {15}, 'TDM': {15}, 'Round': {15}, 'Robin': {15}, 'RR': {15}, 'employed': {15}, 'formal': {15}, 'analysis': {10, 15}, 'g': {9, 15}, 'calculus': {15}, 'flow': {15}, 'ever': {15}, 'integrated': {15, 7}, 'clock': {15}, 'increased': {15}, 'factor': {15}, 'every': {17, 15}, 'generation': {15}, 'scaling': {15}, 'channel': {19, 15}, 'wider': {15}, 'interface': {15}, 'Wide': {15}, 'IO': {15}, 'introduced': {15}, 'targeting': {15}, 'battery': {15}, 'operated': {15}, 'support': {15}, 'upcoming': {15}, 'scalable': {15}, 'essential': {15}, 'bus': {15}, 'interconnects': {15}, 'centralized': {20, 15}, 'current': {15}, 'distributed': {20, 15}, 'either': {15}, 'suffer': {15}, 'poor': {15}, 'paradigm': {16}, 'emerging': {16}, 'effectively': {16}, 'presumably': {16}, 'overcome': {16}, 'interconnection': {16, 7}, 'already': {16}, 'exist': {16}, 'today': {16}, 'likely': {16}, 'occur': {16}, 'future': {16}, 'Effective': {16}, 'requires': {16}, 'developing': {16}, 'deploying': {16}, 'whole': {16}, 'set': {16, 10}, 'infrastructure': {16}, 'IPs': {16}, 'supporting': {16}, 'tool': {16}, 'issue': {16, 17}, 'illustrates': {16}, 'date': {8, 16}, 'engineer': {16}, 'successfully': {16}, 'deployed': {16}, 'NoCs': {16}, 'aggressive': {16}, 'specification': {16}, 'At': {16}, 'reveal': {16}, 'require': {16, 10}, 'indeed': {16}, 'panacea': {16}, 'quasi': {16}, 'tomorrow': {16}, '\\x92': {16}, 'SoCs': {16}, 'constructed': {7}, 'silicon': {7}, 'photonic': {7}, 'microrings': {7}, 'CMOS': {7}, 'die': {7}, 'Each': {7}, 'microring': {7}, 'ha': {8, 17, 20, 7}, 'switching': {7}, 'consuming': {7}, 'significantly': {8, 10, 20, 7}, 'Different': {7}, 'routing': {7}, 'scheme': {7}, 'looping': {7}, 'exploit': {7}, 'characteristic': {17, 7}, 'Benes': {7}, 'sufficient': {10, 7}, 'finding': {7}, 'Compared': {7}, 'inline': {7}, 'formula': {7}, 'tex': {7}, 'math': {7}, 'notation': {7}, 'LaTeX': {7}, 'n': {7}, 'cdot': {7}, 'Several': {8}, 'fault': {8}, 'secure': {8}, 'tolerant': {8}, 'digital': {8}, 'finite': {8}, 'FIR': {8}, 'filter': {8}, 'varying': {8}, 'coverage': {8}, 'specific': {8}, 'modern': {8}, 'nanometric': {8}, 'detecting': {8}, 'residue': {8}, 'reported': {8}, 'multiplier': {8}, 'accumulator': {8}, 'MACs': {8}, 'protected': {8}, 'version': {8}, 'transpose': {8}, 'synthesized': {8}, 'RC': {8}, 'Compiler': {8}, 'v': {8}, 'nm': {8}, 'leave': {8}, 'doubt': {8}, 'extremely': {8}, '16': {8}, 'bit': {8}, 'operand': {8}, 'mode': {8}, 'note': {8}, 'unlike': {8}, 'claimed': {8}, 'elementary': {8}, 'rule': {8}, 'applied': {8, 17}, 'circuit': {8}, 'guarantee': {8}, 'last': {17}, 'dissipation': {17}, 'par': {17}, 'computer': {17}, 'Whereas': {17}, 'past': {17}, 'job': {17}, 'architect': {17}, 'improvement': {17}, 'transistor': {17}, 'count': {17}, 'taken': {17}, 'account': {17}, 'While': {17}, 'successful': {17}, '40': {17}, '50': {17}, 'annual': {17}, 'previously': {17}, 'brushed': {17}, 'aside': {17}, 'eventually': {17}, 'caught': {17}, 'inexorable': {17}, 'Power': {17}, 'catalyzed': {17}, 'topic': {17}, 'resulting': {17, 10}, 'body': {17}, 'coupled': {17}, 'diminishing': {17}, 'cause': {17}, 'switch': {17}, 'single': {17, 10}, 'slowdown': {17}, 'book': {17}, 'aim': {17}, 'document': {17}, 'architectural': {17}, 'invented': {17}, 'hierarchy': {17}, 'situation': {17}, 'synthesizes': {17}, 'focusing': {17}, 'common': {17}, 'Table': {17}, 'Contents': {17}, 'Introduction': {17}, 'Modeling': {17}, 'Measurement': {17}, 'Using': {17}, 'Voltage': {17}, 'Frequency': {17}, 'Adjustments': {17}, 'Manage': {17}, 'Optimizing': {17}, 'Capacitance': {17}, 'Switching': {17}, 'Activity': {17}, 'Reduce': {17}, 'Managing': {17}, 'Static': {17}, 'Leakage': {17}, 'Conclusions': {17}, 'Reflecting': {18}, 'object': {18}, 'tea': {18}, 'pot': {18}, 'glass': {18}, 'diffusely': {18}, 'reflecting': {18}, 'user': {18}, 'shirt': {18}, 'spy': {18}, 'confidential': {18}, 'displayed': {18}, 'monitor': {10, 18}, 'First': {18}, 'reflection': {18}, 'eye': {18}, 'exploited': {18}, 'spying': {18}, 'Second': {18}, 'investigate': {18}, 'extent': {18}, 'reconstructed': {18}, 'diffuse': {18}, 'wall': {18}, 'clothes': {18}, 'theoretic': {18}, 'limiting': {18}, 'attack': {18}, 'Third': {18}, 'effectiveness': {18}, 'countermeasure': {18}, 'substantially': {18}, 'improves': {10, 18}, 'Backes': {18}, 'et': {18}, 'al': {18}, 'IEEE': {18}, 'Symposium': {18}, 'Security': {18}, 'Privacy': {18}, '2008': {18}, 'develop': {9}, 'market': {9}, 'building': {9}, 'microgrid': {9}, 'operator': {9}, 'SMO': {9}, 'offer': {9}, 'regulation': {9}, 'reserve': {9}, 'associated': {9}, 'obligation': {9}, 'fast': {9}, 'command': {9}, 'issued': {9}, 'wholesale': {9}, 'independent': {9, 10}, 'ISO': {9}, 'purchase': {9}, 'allows': {9}, 'behavior': {9, 10}, 'internal': {9}, 'price': {9}, 'feedback': {9}, 'quantity': {9}, 'transacted': {9}, 'period': {9}, 'hour': {9}, 'scale': {9}, 'During': {9}, 'follows': {9}, 'shorter': {9}, 'repeatedly': {9}, 'decrease': {9}, 'operational': {9}, 'selecting': {9}, 'short': {9}, 'pricing': {9}, 'program': {9}, 'utility': {9}, 'formulate': {9}, 'nonlinear': {9}, 'programming': {9}, 'upper': {9}, 'asymptotic': {9}, 'regime': {9}, 'shown': {9}, 'tight': {9}, 'approximation': {9}, 'Equally': {9}, 'importantly': {9}, 'framework': {9, 20}, 'u': {9}, 'determining': {9}, 'verify': {9}, 'validate': {9}, 'series': {9}, 'Monte': {9}, 'Carlo': {9}, 'simulation': {9}, 'controlled': {9}, 'trajectory': {9}, 'Data': {10}, 'intensive': {10}, 'put': {10}, 'immense': {10}, 'strain': {10}, 'Graphics': {10}, 'Processing': {10}, 'Units': {10}, 'cater': {10}, 'distribute': {10}, 'servicing': {10}, 'mostly': {10}, 'strategy': {10}, 'structure': {10}, 'Last': {10}, 'Level': {10}, 'Cache': {10}, 'LLC': {10}, 'organization': {10}, 'store': {10}, 'slice': {10}, 'Shared': {10}, 'hence': {10}, 'serialized': {10}, '—': {10}, 'provided': {10}, 'private': {10}, 'often': {10}, 'undesirable': {10}, 'reduces': {10, 19}, 'Selective': {10}, 'Replication': {10}, 'SelRep': {10}, 'selectively': {10}, 'replicates': {10}, 'read': {10}, 'improve': {10}, 'retains': {10}, 'cached': {10}, 'compile': {10}, 'dataflow': {10}, 'identify': {10}, 'purpose': {10}, 'instruction': {10}, 'caching': {10}, 'Leveraging': {10}, 'analytical': {10, 19}, 'chooses': {10}, 'replication': {10}, 'carefully': {10}, 'balance': {10, 20}, 'consistently': {10}, 'More': {10}, 'specifically': {10}, '19': {10}, '11': {10}, '61': {10}, '6': {10}, '31': {10}, 'baseline': {10}, 'Adaptive': {10}, 'respectively': {10}, 'For': {19}, 'first': {19}, 'systematically': {19}, 'develops': {19}, 'complete': {19}, 'serial': {19}, 'link': {19}, 'hierarchical': {19}, 'modeling': {19}, 'comprehensive': {19}, 'expression': {19}, 'analyzes': {19}, 'intrinsic': {19}, 'trade': {19}, 'offs': {19}, 'inductive': {19}, 'coupling': {19}, 'Further': {19}, 'equalization': {19}, 'potential': {19}, 'breaking': {19}, 'TCI': {19}, 'inductor': {19}, 'deep': {20}, 'submicron': {20}, 'era': {20}, 'thermal': {20}, 'hot': {20}, 'spot': {20}, 'temperature': {20}, 'gradient': {20}, 'reliability': {20}, 'leakage': {20}, 'difficult': {20}, 'manner': {20}, 'explosion': {20}, 'monitoring': {20}, 'balanced': {20}, 'profile': {20}, 'achieved': {20}, 'proactive': {20}, 'migration': {20}, 'neighboring': {20}, 'agent': {20}, 'residing': {20}, 'observes': {20}, 'local': {20}, 'communicates': {20}, 'nearest': {20}, 'neighbor': {20}, 'exchange': {20}, 'By': {20}, 'choosing': {20}, 'generating': {20}, 'emergency': {20}, 'maintains': {20}, 'avoids': {20}, 'unnecessary': {20}, 'generates': {20}, 'hotspot': {20}, 'smoother': {20}, 'higher': {20}})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "\n",
    "class DocumentBatch:\n",
    "    def __init__(self, docs):\n",
    "        self.documents = docs\n",
    "        self.index = defaultdict(set)\n",
    "        self.preprocessor = Preprocessor()\n",
    "\n",
    "    def sort_based_index_construction(self):\n",
    "        # {TODO}: Build an inverted index for the batch\n",
    "        for doc_id, doc in self.documents:\n",
    "            words = self.preprocessor.preprocess(doc)\n",
    "            for word in words:\n",
    "                self.index[word].add(doc_id)\n",
    "\n",
    "class FastSearchEngine:\n",
    "    def __init__(self):\n",
    "        self.main_index = defaultdict(set)\n",
    "        self.daily_indices = deque()\n",
    "\n",
    "    def add_batch(self, batch: DocumentBatch):\n",
    "        # {TODO}: Incorporate the new batch into the daily indices\n",
    "        batch.sort_based_index_construction()\n",
    "        self.daily_indices.append(batch.index)\n",
    "            \n",
    "            \n",
    "    def merge(self, first, second):\n",
    "        merged_index = first.copy()\n",
    "        for key in second.keys():\n",
    "            merged_index[key] = second[key].union(merged_index[key])\n",
    "        return merged_index\n",
    "        \n",
    "\n",
    "    def end_of_day_logarithmic_merge(self):\n",
    "        # {TODO}: Implement the logarithmic merge strategy\n",
    "        while len(self.daily_indices) > 1:\n",
    "            first = self.daily_indices.popleft()\n",
    "            second = self.daily_indices.popleft()\n",
    "            merged_index = self.merge(first, second)\n",
    "            self.daily_indices.append(merged_index)\n",
    "        self.main_index = self.merge(self.main_index, self.daily_indices.popleft())\n",
    "            \n",
    "\n",
    "\n",
    "# Divide the documents into groups to distribute them between servers. For example, let's consider two servers here.\n",
    "# Divide the documents of each server into batches, for instance, five batches.\n",
    "# Create an index for each batch and for each server, then merge them into the main index at the end of the day.\n",
    "# Repeat this process until all documents are processed.\n",
    "\n",
    "server_a_docs = []\n",
    "server_b_docs = []\n",
    "for doc in list(df.iterrows())[:10]:\n",
    "    server_a_docs.append((doc[1][\"paperId\"], doc[1][\"abstract\"]))\n",
    "    \n",
    "for doc in list(df.iterrows())[10:20]:\n",
    "    server_b_docs.append((doc[1][\"paperId\"], doc[1][\"abstract\"]))\n",
    "\n",
    "# Usage example\n",
    "# server_a_docs = [\"sample_1\", \"sample_2\", \"sample_3\", \"sample_4\", \"sample_5\", \"sample_6\", \"sample_7\", \"sample_8\", \"sample_9\", \"sample_10\"]\n",
    "# server_b_docs = [\"sample_11\", \"sample_12\", \"sample_13\", \"sample_14\", \"sample_15\", \"sample_16\", \"sample_17\", \"sample_18\", \"sample_19\", \"sample_20\"]\n",
    "search_engine = FastSearchEngine()\n",
    "for i in range(5):\n",
    "    server_a_batch = DocumentBatch(server_a_docs[i*2:i*2+2])\n",
    "    server_b_batch = DocumentBatch(server_b_docs[i*2:i*2+2])\n",
    "    \n",
    "    search_engine.add_batch(server_a_batch)\n",
    "    search_engine.add_batch(server_b_batch)\n",
    "\n",
    "    # At end of day\n",
    "    search_engine.end_of_day_logarithmic_merge()\n",
    "\n",
    "# Note that the above was just an example demonstrating the general process.\n",
    "# but you can take two servers and five batches for each server as a constant and implement the process for them.\n",
    "print(search_engine.main_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 8}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_engine.main_index[\"recent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1>Spell Correction (اصلاح پرسمان)</h1>\n",
    "<p>\n",
    "در بسیاری از اوقات پرسمان دادە شده توسط کاربر، ممکن است ناقص یا دارای غلط املایی باشد. برای رفع این مشکل در بسیاری از موتورهای جستجو راە حل هایی تدارک دیده شدە است. ابتدا این راە حل ها را شرح دهید و بیان کنید که یک موتور جست و جو بر چه اساسی پرسمان های اصلاح شده را به کاربر نمایش می دهد.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h2>پاسخ سوال بالا</h2>\n",
    "\n",
    "موتورهای جستجو برای ارتقاء کیفیت نتایج و کمک به کاربران در یافتن اطلاعات دلخواه خود، روش‌های پیچیده‌ای را برای حل مشکلات ناشی از پرسمان‌های ناکامل یا با غلط املایی به کار می‌برند. این روش‌ها شامل موارد زیر هستند:\n",
    "\n",
    "1. **تصحیح اتوماتیک املایی**: این قابلیت به صورت خودکار اشتباهات املایی متداول را شناسایی و اصلاح می‌کند. مثلاً اگر کاربر \"apple\" را به اشتباه \"aple\" وارد کند، موتور جستجو این خطا را تشخیص داده و جستجویی برای \"apple\" انجام می‌دهد.\n",
    "\n",
    "2. **پیشنهادهای جستجو**: به محض اینکه کاربر تایپ کردن را شروع کند، موتور جستجو بر اساس الگوریتم‌های پیش‌بینی واژگان، فهرستی از پیشنهادها را نشان می‌دهد تا کاربران به سرعت به عبارت جستجوی مد نظر خود دست یابند.\n",
    "\n",
    "3. **استفاده از مترادف‌ها و واژگان هم‌معنی**: موتورهای جستجو ممکن است برای دقت بیشتر در نتایج، کلماتی با معانی مشابه به واژه‌های جستجو شده (مثلاً \"خرید\" و \"فروش\") را در جستجوی خود در نظر بگیرند.\n",
    "\n",
    "4. **توجه به زمینه و تاریخچه جستجوی کاربر**: برخی از موتورهای جستجو با بررسی زمینه کلی پرسمان و همچنین تاریخچه جستجوی کاربر، سعی در فهم بهتر نیت واقعی کاربر دارند.\n",
    "\n",
    "### چگونگی نمایش پرسمان‌های ویرایش شده به کاربر\n",
    "\n",
    "زمانی که موتور جستجویی یک پرسمان را تعدیل می‌کند، بر اساس چند عامل تصمیم می‌گیرد که نتایج تصحیح شده را به کاربر ارائه دهد:\n",
    "\n",
    "- **اطمینان به صحت پرسمان تصحیح شده**: اگر موتور جستجو با بالاترین اطمینان تشخیص دهد که پرسمان وارد شده اشتباه است، به صورت خودکار پرسمان را به فرم اصلاح شده تغییر می‌دهد.\n",
    "- **ارائه هر دو نوع نتیجه**: در مواردی که اطمینان کمتری به خطا بودن واژه وجود داشته باشد، موتور جستجو ممکن است هم نتایج مبتنی بر پرسمان اولیه و هم نتایج مبتنی بر پرسمان اصلاح شده را عرضه کند.\n",
    "- **گزینه‌ای برای بازگشت به پرسمان اصلی**: اغلب موتورهای جستجو گزینه‌ای را فراهم می‌کنند تا کاربر بتواند در صورت تمایل به پرسمان اصلی خود بازگردد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<p>\n",
    "    در این بخش، ابتدا با استفاده از روش bigram لغات نزدیک به لغات اصلی را پیدا کنید و در آخر با معیار minimum edit distance لغتی جایگزین را برای لغت مورد نظر پیدا کنید.  سپس برای هر پرسمان ورودی کاربر، در صورت اشتباه بودن آن، آن را تصحیح کنید. برای stopword‌ها نیز می‌توانید از لیست موجود که در قسمت‌های قبل ساختید استفاده کنید.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138299"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural_words = [word.lower() for word in inaugural.words() if word.isalpha()]\n",
    "len(inaugural_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'recent',\n",
       " 'years',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'have',\n",
       " 'where',\n",
       " 'the',\n",
       " 'demand']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = [word.lower() for word in df[\"abstract\"].str.cat(sep=\" \").split() if word.isalpha()]\n",
    "data_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def create_bigram_index():\n",
    "    \"\"\"\n",
    "    Creates a bigram index for the spell correction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of bigrams and their occurence\n",
    "    \"\"\"\n",
    "    # TODO: Create the bigram index here\n",
    "    bigram: Dict[str, List[str]] = {}\n",
    "    \n",
    "    all_words = set(inaugural_words).union(set(data_words))\n",
    "    # all_words = set(data_words)\n",
    "    \n",
    "    for word in all_words:\n",
    "        for l in nltk.bigrams(word):\n",
    "            l = \"\".join(l)\n",
    "            if l in bigram:\n",
    "                bigram[l].append(word)\n",
    "            else:\n",
    "                bigram[l] = [word]\n",
    "                \n",
    "    \n",
    "    return bigram\n",
    "\n",
    "bigram_index = create_bigram_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wh is the most popular programming language'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "def spell_correction(query):\n",
    "    \"\"\"\n",
    "        Correct the give query text, if it is misspelled\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        query: str\n",
    "            The query text\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        corrected_query: str\n",
    "            The corrected text\n",
    "    \"\"\"\n",
    "    # TODO: correct the query\n",
    "    def correct_word(word):\n",
    "    \n",
    "        word_bigrams = [\"\".join(l) for l in nltk.bigrams(word)]\n",
    "    \n",
    "        words_list = []\n",
    "        for bg in word_bigrams:\n",
    "            words_list.append(set(bigram_index.get(bg, [])))\n",
    "        \n",
    "        # get the words that have at most three bigrams not in common with the word\n",
    "        all_words_selected = set.union(*words_list)\n",
    "        word_bigrams_freq = {w: 0 for w in all_words_selected}\n",
    "        for bg in word_bigrams:\n",
    "            for w in all_words_selected:\n",
    "                if bg in bigram_index and w in bigram_index[bg]:\n",
    "                    word_bigrams_freq[w] += 1\n",
    "        \n",
    "        words = [w for w in all_words_selected if abs(word_bigrams_freq[w] - len(word_bigrams)) <= 3]\n",
    "\n",
    "        if word in words:\n",
    "            return word\n",
    "    \n",
    "        # get the word with the minimum edit distance from the query\n",
    "        min_edit_distance = float(\"inf\")\n",
    "        closest_word = \"\"\n",
    "        for w in words:\n",
    "            edit_distance = nltk.edit_distance(w, word)\n",
    "            if edit_distance < min_edit_distance:\n",
    "                min_edit_distance = edit_distance\n",
    "                closest_word = w\n",
    "        return closest_word\n",
    "        \n",
    "    query = query.lower()\n",
    "    corrected_query = \" \".join([correct_word(word) for word in query.split()])\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Wht is the most populr progarmming lanuage?\"\n",
    "spell_correction(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "<h1> Boolean Retrieval </h1>\n",
    "<p>\n",
    " در این قسمت هدف طراحی یک سامانەی بازیابی اطلاعات boolean می‌باشد. \n",
    "\n",
    "برای این کار ابتدا پیش پردازش‌های مورد نیاز را مانند بخش قبل بر روی متون انجام دهید و در مرحله بعد ماتریس doc−term را ایجاد کنید. در نهایت کلاس BooleanRetrievalModel را تکمیل کنید که شامل توابع preprocess_query و find_siⅿiⅼar_docs است که توضیحات هرکدام در قسمت کد موجود است. هدف نهایی این است که هرگاه کوئری به تابع find_siⅿiⅼar_docs از کلاس BooleanRetrievalModel داده شود، شناسه k تا از داک‌هایی که شامل کوئری داده شده هستند برگردانده شوند.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and tokenize all documents\n",
    "\n",
    "# create a list of (docId, abstract) tuples from all documents\n",
    "docs = []\n",
    "for doc in df.iterrows():\n",
    "    docs.append((doc[1][\"paperId\"], doc[1][\"abstract\"]))\n",
    "\n",
    "search_engine = FastSearchEngine()\n",
    "batch_size = 200\n",
    "for i in range(0, len(docs), batch_size):\n",
    "    batch = DocumentBatch(docs[i:i+batch_size])\n",
    "    search_engine.add_batch(batch)\n",
    "\n",
    "search_engine.end_of_day_logarithmic_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc_term_matrix(search_engine):\n",
    "    \"\"\"\n",
    "    Create doc_term_matrix by using documents and unique words.\n",
    "    \"\"\"\n",
    "    \n",
    "    max_id = max([max(docs) for docs in search_engine.main_index.values()])\n",
    "\n",
    "    doc_term_matrix = {word: [0] * max_id for word in search_engine.main_index}\n",
    "\n",
    "    for word, docs in search_engine.main_index.items():\n",
    "        for doc in docs:\n",
    "            doc_term_matrix[word][doc - 1] = 1\n",
    "    \n",
    "    return doc_term_matrix, max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooleanRetrievalModel:\n",
    "    def __init__(self, search_engine, documents):\n",
    "        \"\"\"    \n",
    "        Set doc_term_matrix and initialize the model.\n",
    "        \"\"\"\n",
    "        self.search_engine = search_engine\n",
    "        self.documents = documents\n",
    "        self.doc_term_matrix, self.max_id = make_doc_term_matrix(search_engine)\n",
    "        self.preprocessor = Preprocessor()\n",
    "\n",
    "    def preprocess_query(self, query):\n",
    "        \"\"\"\n",
    "        Do necessary preprocessing here before using the query to find k similar docs.\n",
    "        Use methods from Preprocess section.\n",
    "        \"\"\"\n",
    "        query = self.preprocessor.preprocess(query)\n",
    "        processed_query = spell_correction(\" \".join(query))\n",
    "        return processed_query.split()\n",
    "    \n",
    "    def find_siⅿiⅼar_docs(self, query, k=20):\n",
    "        processed_query = self.preprocess_query(query)\n",
    "        \"\"\"\n",
    "        Find k similiar documents.\n",
    "        \"\"\"\n",
    "        similiar_docs_indices = np.zeros((len(processed_query), self.max_id))\n",
    "        \n",
    "        for i, word in enumerate(processed_query):\n",
    "            if word in self.search_engine.main_index:\n",
    "                similiar_docs_indices[i] = np.array(self.doc_term_matrix[word])\n",
    "                \n",
    "        # get the logical and of the documents\n",
    "        similiar_docs_indices = np.logical_and.reduce(similiar_docs_indices, axis=0)\n",
    "        similiar_docs_indices = (np.where(similiar_docs_indices == 1)[0] + 1)\n",
    "        \n",
    "        # check if the query is in the document\n",
    "        docs = self.documents[self.documents[\"paperId\"].isin(similiar_docs_indices)]\n",
    "        similiar_docs = []\n",
    "        processed_query = spell_correction(query).lower()\n",
    "        for doc in docs.iterrows():\n",
    "            if processed_query in doc[1][\"abstract\"].lower():\n",
    "                similiar_docs.append(doc[1][\"paperId\"])\n",
    "        \n",
    "        return similiar_docs[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "در این قسمت ۳ کوئری مختلف به دلخواه خود بزنید و لیست داکیومنت‌های مرتبط با آن‌ها را برگردانید. برای کوتاه‌تر شدن لیست جواب در هر کوئری می‌توانید از عملگر‌های منطقی مانند AND استفاده کنید.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3 queries and find similar docs for each of them\n",
    "boolean_retrieval_model = BooleanRetrievalModel(search_engine, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[286, 1112, 5202]\n"
     ]
    }
   ],
   "source": [
    "query1 = \"network protocols\"\n",
    "print(boolean_retrieval_model.find_siⅿiⅼar_docs(query1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333, 885, 947, 1521, 2387, 3317, 3758]\n"
     ]
    }
   ],
   "source": [
    "query2 = \"major reliability concern\"\n",
    "print(boolean_retrieval_model.find_siⅿiⅼar_docs(query2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[256]\n"
     ]
    }
   ],
   "source": [
    "query3 = \"stringent design constraints\"\n",
    "print(boolean_retrieval_model.find_siⅿiⅼar_docs(query3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl'>\n",
    "\n",
    "# ذخیره و فشرده‌سازی نمایه\n",
    "در این بخش، در ابتدا دو الگوریتم فشرده‌سازی gamma code و variable byte را پیاده‌سازی کنید.  \n",
    "سپس نمایه را به سه شکل زیر ذخیره کنید:\n",
    "- نمایه‌ی اصلی بدون فشرده‌سازی\n",
    "- نمایه‌ای که با استفاده از gamma code فشرده شده است.\n",
    "- نمایه‌ای که با استفاده از variable byte فشرده شده است.\n",
    "\n",
    "در ادامه اندازه‌ی هر کدام از سه فایل بالا را با استفاده از یک تابع به دست آورده و چاپ کنید.  \n",
    "همچنین باید تابع‌هایی برای decompress کردن نمایه‌های فشرده‌شده پیاده‌سازی کنید.\n",
    "\n",
    "**نکته‌ی ۱:** تمامی نمایه‌ها را نیز در کوئرا ارسال کنید. اگر حجم‌شان بیش‌تر از محدودیت کوئرا است، آن‌ها را در یک مکان دیگر آپلود کرده و لینک آن را در این فایل قرار دهید.  \n",
    "**نکته‌ی ۲:** توابع زیر صرفاً پیشنهادی هستند و هر گونه تغییر تا زمانی که کاربردهای مورد نظر پیاده شود، آزاد است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json(obj, path):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset(gap):\n",
    "    return bin(gap)[3:]\n",
    "\n",
    "def unary_code(gap):\n",
    "    return \"1\" * gap + \"0\"\n",
    "\n",
    "def get_length(offset):\n",
    "    return unary_code(len(offset))\n",
    "\n",
    "def gaps_list(postings):\n",
    "    first_posting = [postings[0]]\n",
    "    gaps = [(postings[i] - postings[i - 1]) for i in range(1, len(postings))]\n",
    "    return first_posting + gaps\n",
    "    \n",
    "    \n",
    "def gamma_encode(postings):\n",
    "    if not postings:\n",
    "        return \"\"\n",
    "    \n",
    "    gaps = gaps_list(postings)\n",
    "    gamma_code = [get_length(get_offset(gap)) + get_offset(gap) for gap in gaps]\n",
    "    \n",
    "    return \"\".join(gamma_code)\n",
    "    \n",
    "def compress_index_gamma_encoding(index, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting in index.items():\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            encoded_posting = gamma_encode(posting)\n",
    "            \n",
    "            rem = len(encoded_posting) % 8\n",
    "            octet_encoded_posting = '0b' + \\\n",
    "                                    '0' * (8 - rem) + \\\n",
    "                                    encoded_posting\n",
    "            # get each 8 bits and convert it to a byte\n",
    "            n_bytes = len(octet_encoded_posting) // 8\n",
    "            f.write(n_bytes.to_bytes(4, 'big'))\n",
    "            for i in range(n_bytes):\n",
    "                f.write(int(octet_encoded_posting[2+i*8:2+(i+1)*8], 2).to_bytes(1, 'big'))\n",
    "        \n",
    "\n",
    "def variable_byte_encode(n):\n",
    "    \"\"\"\n",
    "    Encode the given number using variable byte encoding.\n",
    "    \"\"\"\n",
    "    b = []\n",
    "    while True:\n",
    "        b = [n % 128] + b\n",
    "        n = n // 128\n",
    "        if n == 0:\n",
    "            break\n",
    "    b[-1] += 128\n",
    "    return bytes(b)\n",
    "\n",
    "def variable_byte_encode_posting(posting):\n",
    "    \"\"\"\n",
    "    Encode the given posting list using variable byte encoding.\n",
    "    \"\"\"\n",
    "    return b\"\".join([variable_byte_encode(n) for n in posting])\n",
    "\n",
    "def compress_index_variable_byte(index, path):\n",
    "    \"\"\"\n",
    "    Compress the given index using variable byte encoding.\n",
    "    \"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        for word, posting in index.items():\n",
    "            encoded_word = word.encode('utf-8')\n",
    "            f.write(len(encoded_word).to_bytes(1, 'big'))\n",
    "            f.write(encoded_word)\n",
    "            encoded_posting = variable_byte_encode_posting(posting)\n",
    "            encoding_length = len(encoded_posting)\n",
    "            f.write(encoding_length.to_bytes(4, 'big'))\n",
    "            f.write(encoded_posting)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in search_engine.main_index:\n",
    "    search_engine.main_index[term] = sorted(list(search_engine.main_index[term]))\n",
    "    \n",
    "write_json(search_engine.main_index, \"uncompressed_index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_index_gamma_encoding(search_engine.main_index, \"gamma_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_index_variable_byte(search_engine.main_index, \"variable_byte_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unary_decode(gap):\n",
    "    return sum(map(int, gap))\n",
    "    \n",
    "    \n",
    "def gamma_decode(gamma):\n",
    "    num, length, offset, aux, res = 0, \"\", \"\", 0, []\n",
    "    \n",
    "    # remove the first 0s\n",
    "    while gamma != \"\" and gamma[0] == \"0\":\n",
    "        gamma = gamma[1:]\n",
    "    \n",
    "    while gamma != \"\":\n",
    "        aux = gamma.find(\"0\")\n",
    "        length = gamma[:aux]\n",
    "        if length == \"\":\n",
    "            res.append(1); gamma = gamma[1:]\n",
    "        else:\n",
    "            offset = \"1\" + gamma[aux + 1:aux + 1 + unary_decode(length)]\n",
    "            res.append(int(offset, 2))\n",
    "            gamma = gamma[aux + 1 + unary_decode(length):]\n",
    "    return [int(p) for p in np.cumsum(res)]\n",
    "    \n",
    "    \n",
    "def decompress_index_gamma_encoding(path):\n",
    "    index = {}\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "            gamma_code = \"\"\n",
    "            for i in range(n_bytes):\n",
    "                bin_str = bin(int.from_bytes(f.read(1), 'big'))\n",
    "                # padding the zeros\n",
    "                gamma_code += bin_str[2:].zfill(8)\n",
    "                \n",
    "            index[word] = gamma_decode(gamma_code)\n",
    "            \n",
    "    return index\n",
    "\n",
    "\n",
    "def vb_decode(encoded: bytes):\n",
    "    numbers = []\n",
    "    n = 0\n",
    "    for b in encoded:\n",
    "        n = n * 128 + (b % 128)\n",
    "        if b >= 128:\n",
    "            numbers.append(n)\n",
    "            n = 0\n",
    "    return numbers\n",
    "\n",
    "\n",
    "def decompress_index_vb_encoding(path):\n",
    "    index = {}\n",
    "    with open(path, 'rb') as f:\n",
    "        while True:\n",
    "            word_length = f.read(1)\n",
    "            if not word_length:\n",
    "                break\n",
    "            word_length = int.from_bytes(word_length, 'big')\n",
    "            word = f.read(word_length).decode('utf-8')\n",
    "            posting_n_bytes = int.from_bytes(f.read(4), 'big')\n",
    "            postings = vb_decode(f.read(posting_n_bytes))\n",
    "            index[word] = postings\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_index_decompressed = decompress_index_gamma_encoding(\"gamma_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 11, 12, 15, 16, 17, 18, 19, 22, 23, 29, 30, 39, 41, 42, 46, 50, 51, 52, 54, 58, 59, 65, 66, 71, 74, 76, 79, 80, 84, 85, 86, 87, 89, 91, 96, 97, 101, 106, 107, 109, 110, 112, 114, 118, 121, 122, 124, 125, 129, 133, 134, 138, 140, 143, 144, 147, 151, 152, 153, 154, 155, 160, 162, 164, 168, 171, 175, 177, 178, 179, 182, 183, 185, 186, 188, 192, 193, 194, 195, 199, 201, 203, 205, 207, 210, 211, 216, 217, 221, 224, 228, 230, 232, 233, 241, 251, 253, 257, 259, 265, 270, 271, 277, 284, 285, 288, 289, 290, 292, 295, 301, 304, 305, 306, 307, 311, 314, 317, 318, 321, 324, 325, 326, 327, 329, 330, 334, 337, 339, 340, 342, 343, 346, 347, 351, 355, 356, 359, 361, 362, 363, 366, 367, 370, 373, 376, 377, 384, 390, 391, 393, 395, 398, 399, 400, 401, 404, 406, 407, 408, 409, 411, 413, 414, 415, 416, 418, 419, 428, 432, 434, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 448, 450, 451, 456, 458, 459, 463, 467, 470, 471, 472, 475, 477, 480, 481, 488, 489, 491, 494, 496, 497, 499, 502, 504, 505, 507, 510, 511, 514, 515, 520, 522, 523, 524, 525, 527, 532, 534, 537, 539, 540, 542, 550, 552, 553, 555, 560, 562, 566, 567, 570, 572, 576, 577, 580, 581, 585, 588, 592, 593, 597, 603, 604, 605, 607, 609, 612, 616, 617, 619, 626, 628, 631, 632, 634, 635, 638, 640, 642, 644, 649, 650, 651, 654, 656, 660, 661, 664, 666, 667, 669, 671, 674, 675, 676, 678, 682, 684, 686, 687, 689, 691, 692, 693, 699, 703, 705, 708, 715, 716, 717, 721, 729, 732, 737, 738, 740, 741, 746, 750, 760, 761, 762, 763, 765, 769, 771, 772, 774, 776, 778, 782, 787, 788, 789, 790, 793, 794, 796, 801, 806, 807, 809, 810, 812, 814, 818, 820, 822, 825, 826, 827, 828, 829, 836, 837, 840, 841, 844, 845, 850, 851, 852, 862, 863, 864, 865, 866, 867, 868, 870, 872, 876, 877, 882, 884, 885, 890, 892, 897, 899, 900, 904, 906, 911, 913, 915, 918, 920, 922, 924, 926, 927, 928, 936, 937, 938, 944, 945, 946, 947, 948, 949, 950, 951, 952, 955, 957, 961, 962, 963, 964, 966, 968, 970, 971, 974, 975, 976, 978, 980, 989, 990, 991, 992, 994, 995, 997, 998, 1003, 1006, 1008, 1010, 1015, 1016, 1019, 1023, 1026, 1027, 1033, 1034, 1036, 1038, 1049, 1052, 1057, 1058, 1063, 1064, 1067, 1069, 1073, 1076, 1077, 1078, 1080, 1084, 1086, 1087, 1091, 1095, 1096, 1098, 1099, 1105, 1107, 1108, 1110, 1113, 1114, 1117, 1122, 1125, 1129, 1133, 1136, 1141, 1144, 1145, 1148, 1150, 1152, 1154, 1156, 1157, 1158, 1161, 1162, 1163, 1164, 1165, 1166, 1168, 1173, 1174, 1178, 1179, 1183, 1186, 1189, 1192, 1196, 1197, 1199, 1201, 1202, 1203, 1206, 1207, 1212, 1213, 1214, 1217, 1218, 1219, 1220, 1222, 1225, 1230, 1231, 1232, 1234, 1239, 1240, 1242, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1255, 1260, 1262, 1263, 1266, 1268, 1269, 1270, 1272, 1274, 1275, 1276, 1277, 1279, 1281, 1286, 1287, 1288, 1290, 1292, 1294, 1297, 1302, 1303, 1309, 1310, 1315, 1319, 1323, 1324, 1325, 1328, 1331, 1333, 1335, 1341, 1342, 1345, 1346, 1354, 1359, 1361, 1368, 1369, 1371, 1373, 1379, 1380, 1384, 1387, 1391, 1396, 1397, 1398, 1400, 1403, 1405, 1406, 1407, 1412, 1416, 1420, 1429, 1431, 1432, 1435, 1436, 1437, 1438, 1439, 1440, 1442, 1443, 1448, 1453, 1455, 1457, 1459, 1460, 1461, 1464, 1466, 1467, 1472, 1473, 1483, 1485, 1487, 1490, 1494, 1499, 1500, 1505, 1507, 1508, 1510, 1511, 1513, 1514, 1519, 1520, 1522, 1525, 1533, 1534, 1541, 1542, 1543, 1546, 1550, 1556, 1563, 1565, 1568, 1574, 1579, 1580, 1582, 1585, 1586, 1590, 1592, 1593, 1595, 1596, 1598, 1600, 1602, 1604, 1606, 1612, 1615, 1617, 1622, 1623, 1624, 1626, 1627, 1629, 1632, 1634, 1635, 1637, 1638, 1640, 1645, 1647, 1649, 1650, 1659, 1661, 1665, 1666, 1668, 1674, 1675, 1676, 1678, 1679, 1682, 1684, 1685, 1688, 1689, 1695, 1699, 1703, 1704, 1708, 1709, 1712, 1715, 1716, 1718, 1722, 1723, 1725, 1726, 1728, 1731, 1735, 1738, 1746, 1758, 1759, 1760, 1765, 1766, 1767, 1770, 1771, 1774, 1783, 1784, 1785, 1786, 1793, 1795, 1797, 1799, 1800, 1804, 1807, 1811, 1813, 1814, 1826, 1835, 1836, 1837, 1838, 1840, 1843, 1844, 1848, 1849, 1855, 1862, 1868, 1871, 1873, 1875, 1877, 1878, 1879, 1880, 1881, 1883, 1884, 1886, 1888, 1893, 1897, 1900, 1904, 1905, 1906, 1909, 1910, 1911, 1914, 1919, 1922, 1926, 1929, 1930, 1932, 1934, 1941, 1947, 1950, 1951, 1953, 1955, 1960, 1961, 1969, 1978, 1979, 1982, 1985, 1986, 1991, 1992, 1993, 1995, 2001, 2007, 2012, 2013, 2014, 2016, 2018, 2019, 2023, 2024, 2025, 2026, 2031, 2034, 2036, 2042, 2045, 2047, 2049, 2050, 2053, 2056, 2057, 2058, 2061, 2063, 2071, 2073, 2076, 2079, 2087, 2090, 2091, 2095, 2099, 2100, 2101, 2105, 2107, 2110, 2111, 2115, 2117, 2118, 2119, 2120, 2127, 2128, 2129, 2131, 2133, 2135, 2137, 2138, 2141, 2142, 2144, 2145, 2149, 2151, 2154, 2156, 2158, 2159, 2161, 2162, 2163, 2164, 2165, 2166, 2171, 2174, 2176, 2178, 2179, 2181, 2184, 2188, 2191, 2192, 2198, 2203, 2205, 2206, 2207, 2208, 2217, 2220, 2221, 2224, 2225, 2227, 2228, 2230, 2231, 2235, 2237, 2238, 2239, 2243, 2245, 2246, 2247, 2250, 2252, 2254, 2256, 2259, 2266, 2267, 2271, 2272, 2273, 2275, 2276, 2277, 2279, 2283, 2286, 2288, 2289, 2294, 2297, 2299, 2301, 2309, 2311, 2312, 2313, 2314, 2318, 2319, 2323, 2325, 2327, 2330, 2332, 2333, 2335, 2336, 2337, 2339, 2341, 2349, 2352, 2353, 2358, 2359, 2360, 2362, 2364, 2370, 2376, 2379, 2380, 2381, 2382, 2383, 2387, 2388, 2389, 2391, 2392, 2393, 2397, 2400, 2402, 2407, 2411, 2414, 2416, 2417, 2419, 2420, 2425, 2426, 2427, 2434, 2435, 2436, 2439, 2442, 2444, 2447, 2448, 2449, 2451, 2452, 2453, 2455, 2456, 2460, 2461, 2463, 2464, 2466, 2467, 2472, 2473, 2477, 2478, 2479, 2483, 2491, 2493, 2497, 2499, 2503, 2514, 2518, 2519, 2525, 2528, 2529, 2533, 2536, 2539, 2542, 2543, 2544, 2548, 2551, 2553, 2554, 2557, 2561, 2563, 2565, 2566, 2568, 2570, 2574, 2577, 2580, 2581, 2582, 2583, 2587, 2591, 2592, 2598, 2602, 2604, 2607, 2608, 2612, 2618, 2620, 2624, 2628, 2631, 2637, 2641, 2643, 2644, 2645, 2650, 2652, 2656, 2658, 2659, 2663, 2665, 2666, 2670, 2672, 2673, 2674, 2676, 2681, 2687, 2689, 2691, 2696, 2697, 2699, 2701, 2703, 2705, 2706, 2710, 2711, 2713, 2715, 2719, 2722, 2724, 2729, 2731, 2733, 2737, 2744, 2751, 2758, 2759, 2760, 2761, 2762, 2763, 2767, 2770, 2773, 2775, 2779, 2782, 2783, 2788, 2790, 2793, 2794, 2795, 2796, 2799, 2803, 2808, 2809, 2812, 2815, 2816, 2817, 2818, 2821, 2823, 2824, 2825, 2828, 2830, 2831, 2832, 2833, 2838, 2840, 2842, 2843, 2849, 2850, 2852, 2857, 2858, 2861, 2862, 2865, 2866, 2867, 2871, 2872, 2874, 2875, 2880, 2887, 2893, 2895, 2902, 2903, 2904, 2906, 2907, 2909, 2910, 2914, 2915, 2921, 2922, 2925, 2932, 2934, 2939, 2940, 2941, 2943, 2951, 2952, 2956, 2957, 2961, 2962, 2963, 2967, 2968, 2969, 2972, 2973, 2975, 2977, 2980, 2981, 2982, 2985, 2989, 2995, 2997, 2999, 3002, 3004, 3005, 3007, 3009, 3011, 3018, 3023, 3028, 3031, 3036, 3038, 3040, 3041, 3042, 3043, 3045, 3046, 3047, 3048, 3054, 3055, 3056, 3061, 3062, 3063, 3064, 3067, 3068, 3078, 3079, 3082, 3083, 3084, 3086, 3087, 3089, 3092, 3095, 3096, 3098, 3105, 3107, 3113, 3114, 3115, 3120, 3122, 3123, 3130, 3132, 3135, 3139, 3140, 3146, 3147, 3150, 3151, 3164, 3165, 3168, 3171, 3174, 3178, 3179, 3181, 3183, 3185, 3186, 3187, 3188, 3190, 3194, 3196, 3197, 3198, 3201, 3203, 3207, 3210, 3212, 3215, 3216, 3217, 3218, 3219, 3221, 3223, 3224, 3225, 3229, 3231, 3232, 3233, 3238, 3239, 3240, 3248, 3249, 3250, 3251, 3253, 3255, 3256, 3259, 3260, 3261, 3273, 3275, 3276, 3278, 3279, 3283, 3285, 3287, 3290, 3298, 3299, 3300, 3302, 3306, 3312, 3314, 3319, 3323, 3324, 3326, 3328, 3329, 3334, 3337, 3340, 3341, 3342, 3343, 3350, 3355, 3356, 3361, 3369, 3370, 3376, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3391, 3392, 3395, 3397, 3398, 3399, 3400, 3401, 3406, 3418, 3420, 3422, 3426, 3429, 3432, 3435, 3436, 3439, 3442, 3443, 3446, 3454, 3455, 3462, 3463, 3464, 3466, 3467, 3468, 3471, 3473, 3477, 3479, 3487, 3491, 3492, 3493, 3496, 3498, 3499, 3502, 3506, 3507, 3509, 3510, 3512, 3513, 3515, 3518, 3521, 3523, 3524, 3525, 3527, 3530, 3532, 3533, 3534, 3535, 3537, 3538, 3544, 3548, 3549, 3551, 3554, 3555, 3556, 3559, 3560, 3562, 3568, 3569, 3570, 3573, 3575, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3585, 3589, 3591, 3594, 3599, 3603, 3608, 3609, 3610, 3613, 3626, 3630, 3631, 3636, 3640, 3641, 3642, 3645, 3647, 3648, 3652, 3653, 3656, 3659, 3661, 3662, 3663, 3667, 3671, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3685, 3686, 3687, 3692, 3693, 3698, 3703, 3704, 3706, 3708, 3709, 3711, 3716, 3717, 3724, 3729, 3733, 3734, 3735, 3737, 3738, 3740, 3741, 3747, 3751, 3753, 3756, 3761, 3763, 3765, 3766, 3767, 3769, 3770, 3779, 3781, 3782, 3784, 3785, 3788, 3789, 3792, 3795, 3802, 3806, 3808, 3810, 3811, 3812, 3813, 3814, 3818, 3822, 3823, 3824, 3825, 3826, 3827, 3829, 3830, 3831, 3835, 3840, 3843, 3846, 3848, 3849, 3853, 3854, 3857, 3858, 3859, 3861, 3863, 3866, 3868, 3869, 3875, 3878, 3881, 3884, 3885, 3886, 3888, 3890, 3893, 3894, 3900, 3902, 3904, 3905, 3908, 3909, 3918, 3922, 3923, 3927, 3930, 3931, 3933, 3936, 3939, 3941, 3944, 3945, 3946, 3947, 3948, 3951, 3954, 3955, 3957, 3959, 3961, 3962, 3963, 3964, 3965, 3970, 3972, 3973, 3975, 3979, 3984, 3990, 3991, 3993, 3994, 3996, 3999, 4005, 4011, 4014, 4020, 4021, 4023, 4025, 4027, 4030, 4035, 4036, 4038, 4039, 4041, 4042, 4047, 4052, 4061, 4075, 4077, 4088, 4092, 4097, 4098, 4099, 4108, 4112, 4116, 4117, 4119, 4123, 4126, 4131, 4139, 4140, 4141, 4149, 4153, 4154, 4161, 4163, 4164, 4170, 4173, 4174, 4177, 4180, 4181, 4183, 4185, 4187, 4190, 4192, 4194, 4196, 4198, 4199, 4201, 4202, 4204, 4210, 4212, 4214, 4215, 4218, 4222, 4223, 4226, 4229, 4230, 4231, 4232, 4233, 4235, 4240, 4241, 4244, 4246, 4251, 4253, 4254, 4259, 4261, 4262, 4263, 4266, 4270, 4271, 4274, 4275, 4276, 4277, 4280, 4281, 4282, 4285, 4288, 4289, 4290, 4291, 4292, 4293, 4295, 4296, 4297, 4300, 4302, 4306, 4310, 4311, 4312, 4315, 4317, 4321, 4323, 4324, 4325, 4330, 4332, 4337, 4340, 4342, 4344, 4347, 4350, 4352, 4354, 4356, 4358, 4366, 4373, 4377, 4379, 4381, 4382, 4383, 4389, 4393, 4397, 4402, 4403, 4406, 4407, 4408, 4413, 4416, 4420, 4430, 4432, 4437, 4440, 4442, 4444, 4450, 4452, 4453, 4454, 4458, 4463, 4465, 4466, 4470, 4472, 4476, 4477, 4481, 4482, 4484, 4489, 4491, 4492, 4493, 4495, 4496, 4503, 4506, 4512, 4519, 4520, 4522, 4524, 4525, 4526, 4529, 4530, 4531, 4536, 4542, 4552, 4553, 4561, 4562, 4571, 4578, 4579, 4580, 4581, 4585, 4588, 4589, 4590, 4593, 4596, 4597, 4598, 4600, 4601, 4603, 4604, 4609, 4612, 4615, 4616, 4621, 4625, 4626, 4627, 4628, 4630, 4633, 4638, 4639, 4641, 4643, 4644, 4645, 4647, 4651, 4652, 4657, 4661, 4665, 4667, 4668, 4670, 4671, 4676, 4677, 4685, 4686, 4687, 4688, 4689, 4691, 4692, 4697, 4698, 4699, 4704, 4711, 4712, 4714, 4715, 4718, 4719, 4724, 4725, 4727, 4731, 4737, 4739, 4740, 4741, 4742, 4744, 4745, 4746, 4748, 4749, 4751, 4752, 4753, 4759, 4762, 4767, 4768, 4771, 4778, 4782, 4783, 4791, 4792, 4800, 4802, 4803, 4805, 4806, 4810, 4812, 4814, 4816, 4817, 4819, 4821, 4824, 4829, 4831, 4832, 4834, 4835, 4836, 4841, 4843, 4847, 4849, 4850, 4851, 4852, 4853, 4855, 4858, 4859, 4862, 4863, 4864, 4865, 4867, 4872, 4873, 4874, 4884, 4887, 4888, 4891, 4896, 4898, 4900, 4901, 4902, 4903, 4908, 4912, 4915, 4922, 4923, 4930, 4932, 4933, 4934, 4935, 4936, 4939, 4941, 4944, 4946, 4947, 4949, 4950, 4951, 4953, 4954, 4955, 4958, 4963, 4964, 4967, 4972, 4973, 4974, 4975, 4976, 4978, 4979, 4980, 4982, 4985, 4986, 4988, 4989, 4998, 4999, 5003, 5004, 5005, 5006, 5007, 5008, 5010, 5011, 5014, 5019, 5025, 5030, 5034, 5035, 5043, 5045, 5046, 5049, 5054, 5059, 5060, 5068, 5070, 5071, 5076, 5079, 5081, 5082, 5085, 5087, 5093, 5095, 5097, 5099, 5100, 5102, 5107, 5109, 5114, 5117, 5119, 5123, 5127, 5130, 5132, 5137, 5139, 5140, 5142, 5145, 5147, 5151, 5152, 5153, 5158, 5159, 5161, 5165, 5167, 5168, 5172, 5173, 5175, 5176, 5181, 5187, 5188, 5191, 5194, 5196, 5197, 5198, 5201, 5203, 5204, 5209, 5211, 5212, 5215, 5221, 5226, 5227, 5231, 5232, 5236, 5237, 5240, 5244, 5247, 5254, 5263, 5264, 5271, 5274, 5275, 5279, 5281, 5282, 5284, 5286, 5288, 5290, 5292, 5293, 5294, 5295, 5301, 5305, 5308, 5309, 5313, 5317, 5319, 5321, 5322, 5323, 5324, 5325, 5326, 5329, 5330, 5331, 5332, 5333, 5337, 5338, 5339, 5341, 5342, 5343, 5346, 5349, 5355, 5356, 5357, 5358, 5360, 5362, 5364, 5366, 5370, 5373, 5374, 5375, 5379, 5380, 5383, 5385, 5387, 5392, 5395, 5396, 5398, 5399, 5401, 5403, 5406, 5411, 5412, 5413, 5415, 5416, 5419, 5421, 5422, 5423, 5428, 5429, 5431, 5434, 5438, 5443, 5444, 5447, 5448, 5449, 5451, 5452, 5454, 5456, 5457, 5459, 5461, 5462, 5463, 5467, 5478, 5479, 5480, 5481, 5482, 5485, 5488, 5489, 5490, 5493, 5494, 5495, 5498, 5499, 5505, 5507, 5508, 5512, 5515, 5520, 5521, 5522, 5523, 5524, 5531, 5534, 5535, 5537, 5543, 5546, 5549, 5554, 5556, 5558, 5559, 5560, 5561, 5563, 5565, 5566, 5570, 5573, 5574, 5577, 5579, 5580, 5584, 5585, 5591, 5606, 5611, 5614, 5615, 5623, 5624, 5626, 5628, 5631, 5632, 5633, 5634, 5635, 5636, 5639, 5640, 5645, 5648, 5651, 5654, 5655, 5658, 5660, 5661, 5662, 5663, 5666, 5670, 5671, 5674, 5679, 5681, 5686, 5687, 5690, 5692, 5697, 5698, 5700, 5702, 5703, 5705, 5706, 5710, 5712, 5713, 5714, 5716, 5718, 5720, 5723, 5724, 5726, 5728, 5732, 5733, 5737, 5741, 5742, 5744, 5746, 5747, 5749, 5752, 5755, 5758, 5761, 5771, 5773, 5775, 5776, 5777, 5779, 5780, 5784, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5797, 5804, 5805, 5807, 5808, 5810, 5811, 5812, 5817, 5820, 5821, 5828, 5829, 5830, 5832, 5835, 5838, 5840, 5842, 5844, 5851, 5852, 5858, 5862, 5865, 5866, 5867, 5868, 5869, 5870, 5873, 5875, 5877, 5879, 5880, 5881, 5882, 5884, 5886, 5892, 5893, 5897, 5899, 5900, 5904, 5906, 5909, 5910, 5912, 5913, 5916, 5917, 5918, 5919, 5926, 5929, 5930, 5931, 5933, 5935, 5936, 5938, 5939, 5946, 5949, 5951, 5956, 5957, 5964, 5968, 5970, 5974, 5976, 5977, 5979, 5987, 5988, 5992, 5994, 6003, 6005, 6006, 6014, 6015, 6016, 6020, 6024, 6031, 6032, 6033, 6034, 6036, 6039, 6041, 6043, 6044, 6045, 6053, 6056, 6060, 6063, 6065, 6066, 6069, 6072, 6073, 6074, 6076, 6078, 6079, 6081, 6086, 6088, 6093, 6094, 6103, 6104, 6107, 6108, 6110, 6112, 6113, 6117, 6119, 6121, 6124, 6126, 6136, 6138, 6140, 6141, 6142, 6143, 6145, 6148, 6149, 6151, 6153, 6159, 6160, 6161, 6163, 6164, 6165, 6176, 6177]\n"
     ]
    }
   ],
   "source": [
    "print(gamma_index_decompressed[\"This\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 11, 12, 15, 16, 17, 18, 19, 22, 23, 29, 30, 39, 41, 42, 46, 50, 51, 52, 54, 58, 59, 65, 66, 71, 74, 76, 79, 80, 84, 85, 86, 87, 89, 91, 96, 97, 101, 106, 107, 109, 110, 112, 114, 118, 121, 122, 124, 125, 129, 133, 134, 138, 140, 143, 144, 147, 151, 152, 153, 154, 155, 160, 162, 164, 168, 171, 175, 177, 178, 179, 182, 183, 185, 186, 188, 192, 193, 194, 195, 199, 201, 203, 205, 207, 210, 211, 216, 217, 221, 224, 228, 230, 232, 233, 241, 251, 253, 257, 259, 265, 270, 271, 277, 284, 285, 288, 289, 290, 292, 295, 301, 304, 305, 306, 307, 311, 314, 317, 318, 321, 324, 325, 326, 327, 329, 330, 334, 337, 339, 340, 342, 343, 346, 347, 351, 355, 356, 359, 361, 362, 363, 366, 367, 370, 373, 376, 377, 384, 390, 391, 393, 395, 398, 399, 400, 401, 404, 406, 407, 408, 409, 411, 413, 414, 415, 416, 418, 419, 428, 432, 434, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 448, 450, 451, 456, 458, 459, 463, 467, 470, 471, 472, 475, 477, 480, 481, 488, 489, 491, 494, 496, 497, 499, 502, 504, 505, 507, 510, 511, 514, 515, 520, 522, 523, 524, 525, 527, 532, 534, 537, 539, 540, 542, 550, 552, 553, 555, 560, 562, 566, 567, 570, 572, 576, 577, 580, 581, 585, 588, 592, 593, 597, 603, 604, 605, 607, 609, 612, 616, 617, 619, 626, 628, 631, 632, 634, 635, 638, 640, 642, 644, 649, 650, 651, 654, 656, 660, 661, 664, 666, 667, 669, 671, 674, 675, 676, 678, 682, 684, 686, 687, 689, 691, 692, 693, 699, 703, 705, 708, 715, 716, 717, 721, 729, 732, 737, 738, 740, 741, 746, 750, 760, 761, 762, 763, 765, 769, 771, 772, 774, 776, 778, 782, 787, 788, 789, 790, 793, 794, 796, 801, 806, 807, 809, 810, 812, 814, 818, 820, 822, 825, 826, 827, 828, 829, 836, 837, 840, 841, 844, 845, 850, 851, 852, 862, 863, 864, 865, 866, 867, 868, 870, 872, 876, 877, 882, 884, 885, 890, 892, 897, 899, 900, 904, 906, 911, 913, 915, 918, 920, 922, 924, 926, 927, 928, 936, 937, 938, 944, 945, 946, 947, 948, 949, 950, 951, 952, 955, 957, 961, 962, 963, 964, 966, 968, 970, 971, 974, 975, 976, 978, 980, 989, 990, 991, 992, 994, 995, 997, 998, 1003, 1006, 1008, 1010, 1015, 1016, 1019, 1023, 1026, 1027, 1033, 1034, 1036, 1038, 1049, 1052, 1057, 1058, 1063, 1064, 1067, 1069, 1073, 1076, 1077, 1078, 1080, 1084, 1086, 1087, 1091, 1095, 1096, 1098, 1099, 1105, 1107, 1108, 1110, 1113, 1114, 1117, 1122, 1125, 1129, 1133, 1136, 1141, 1144, 1145, 1148, 1150, 1152, 1154, 1156, 1157, 1158, 1161, 1162, 1163, 1164, 1165, 1166, 1168, 1173, 1174, 1178, 1179, 1183, 1186, 1189, 1192, 1196, 1197, 1199, 1201, 1202, 1203, 1206, 1207, 1212, 1213, 1214, 1217, 1218, 1219, 1220, 1222, 1225, 1230, 1231, 1232, 1234, 1239, 1240, 1242, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1255, 1260, 1262, 1263, 1266, 1268, 1269, 1270, 1272, 1274, 1275, 1276, 1277, 1279, 1281, 1286, 1287, 1288, 1290, 1292, 1294, 1297, 1302, 1303, 1309, 1310, 1315, 1319, 1323, 1324, 1325, 1328, 1331, 1333, 1335, 1341, 1342, 1345, 1346, 1354, 1359, 1361, 1368, 1369, 1371, 1373, 1379, 1380, 1384, 1387, 1391, 1396, 1397, 1398, 1400, 1403, 1405, 1406, 1407, 1412, 1416, 1420, 1429, 1431, 1432, 1435, 1436, 1437, 1438, 1439, 1440, 1442, 1443, 1448, 1453, 1455, 1457, 1459, 1460, 1461, 1464, 1466, 1467, 1472, 1473, 1483, 1485, 1487, 1490, 1494, 1499, 1500, 1505, 1507, 1508, 1510, 1511, 1513, 1514, 1519, 1520, 1522, 1525, 1533, 1534, 1541, 1542, 1543, 1546, 1550, 1556, 1563, 1565, 1568, 1574, 1579, 1580, 1582, 1585, 1586, 1590, 1592, 1593, 1595, 1596, 1598, 1600, 1602, 1604, 1606, 1612, 1615, 1617, 1622, 1623, 1624, 1626, 1627, 1629, 1632, 1634, 1635, 1637, 1638, 1640, 1645, 1647, 1649, 1650, 1659, 1661, 1665, 1666, 1668, 1674, 1675, 1676, 1678, 1679, 1682, 1684, 1685, 1688, 1689, 1695, 1699, 1703, 1704, 1708, 1709, 1712, 1715, 1716, 1718, 1722, 1723, 1725, 1726, 1728, 1731, 1735, 1738, 1746, 1758, 1759, 1760, 1765, 1766, 1767, 1770, 1771, 1774, 1783, 1784, 1785, 1786, 1793, 1795, 1797, 1799, 1800, 1804, 1807, 1811, 1813, 1814, 1826, 1835, 1836, 1837, 1838, 1840, 1843, 1844, 1848, 1849, 1855, 1862, 1868, 1871, 1873, 1875, 1877, 1878, 1879, 1880, 1881, 1883, 1884, 1886, 1888, 1893, 1897, 1900, 1904, 1905, 1906, 1909, 1910, 1911, 1914, 1919, 1922, 1926, 1929, 1930, 1932, 1934, 1941, 1947, 1950, 1951, 1953, 1955, 1960, 1961, 1969, 1978, 1979, 1982, 1985, 1986, 1991, 1992, 1993, 1995, 2001, 2007, 2012, 2013, 2014, 2016, 2018, 2019, 2023, 2024, 2025, 2026, 2031, 2034, 2036, 2042, 2045, 2047, 2049, 2050, 2053, 2056, 2057, 2058, 2061, 2063, 2071, 2073, 2076, 2079, 2087, 2090, 2091, 2095, 2099, 2100, 2101, 2105, 2107, 2110, 2111, 2115, 2117, 2118, 2119, 2120, 2127, 2128, 2129, 2131, 2133, 2135, 2137, 2138, 2141, 2142, 2144, 2145, 2149, 2151, 2154, 2156, 2158, 2159, 2161, 2162, 2163, 2164, 2165, 2166, 2171, 2174, 2176, 2178, 2179, 2181, 2184, 2188, 2191, 2192, 2198, 2203, 2205, 2206, 2207, 2208, 2217, 2220, 2221, 2224, 2225, 2227, 2228, 2230, 2231, 2235, 2237, 2238, 2239, 2243, 2245, 2246, 2247, 2250, 2252, 2254, 2256, 2259, 2266, 2267, 2271, 2272, 2273, 2275, 2276, 2277, 2279, 2283, 2286, 2288, 2289, 2294, 2297, 2299, 2301, 2309, 2311, 2312, 2313, 2314, 2318, 2319, 2323, 2325, 2327, 2330, 2332, 2333, 2335, 2336, 2337, 2339, 2341, 2349, 2352, 2353, 2358, 2359, 2360, 2362, 2364, 2370, 2376, 2379, 2380, 2381, 2382, 2383, 2387, 2388, 2389, 2391, 2392, 2393, 2397, 2400, 2402, 2407, 2411, 2414, 2416, 2417, 2419, 2420, 2425, 2426, 2427, 2434, 2435, 2436, 2439, 2442, 2444, 2447, 2448, 2449, 2451, 2452, 2453, 2455, 2456, 2460, 2461, 2463, 2464, 2466, 2467, 2472, 2473, 2477, 2478, 2479, 2483, 2491, 2493, 2497, 2499, 2503, 2514, 2518, 2519, 2525, 2528, 2529, 2533, 2536, 2539, 2542, 2543, 2544, 2548, 2551, 2553, 2554, 2557, 2561, 2563, 2565, 2566, 2568, 2570, 2574, 2577, 2580, 2581, 2582, 2583, 2587, 2591, 2592, 2598, 2602, 2604, 2607, 2608, 2612, 2618, 2620, 2624, 2628, 2631, 2637, 2641, 2643, 2644, 2645, 2650, 2652, 2656, 2658, 2659, 2663, 2665, 2666, 2670, 2672, 2673, 2674, 2676, 2681, 2687, 2689, 2691, 2696, 2697, 2699, 2701, 2703, 2705, 2706, 2710, 2711, 2713, 2715, 2719, 2722, 2724, 2729, 2731, 2733, 2737, 2744, 2751, 2758, 2759, 2760, 2761, 2762, 2763, 2767, 2770, 2773, 2775, 2779, 2782, 2783, 2788, 2790, 2793, 2794, 2795, 2796, 2799, 2803, 2808, 2809, 2812, 2815, 2816, 2817, 2818, 2821, 2823, 2824, 2825, 2828, 2830, 2831, 2832, 2833, 2838, 2840, 2842, 2843, 2849, 2850, 2852, 2857, 2858, 2861, 2862, 2865, 2866, 2867, 2871, 2872, 2874, 2875, 2880, 2887, 2893, 2895, 2902, 2903, 2904, 2906, 2907, 2909, 2910, 2914, 2915, 2921, 2922, 2925, 2932, 2934, 2939, 2940, 2941, 2943, 2951, 2952, 2956, 2957, 2961, 2962, 2963, 2967, 2968, 2969, 2972, 2973, 2975, 2977, 2980, 2981, 2982, 2985, 2989, 2995, 2997, 2999, 3002, 3004, 3005, 3007, 3009, 3011, 3018, 3023, 3028, 3031, 3036, 3038, 3040, 3041, 3042, 3043, 3045, 3046, 3047, 3048, 3054, 3055, 3056, 3061, 3062, 3063, 3064, 3067, 3068, 3078, 3079, 3082, 3083, 3084, 3086, 3087, 3089, 3092, 3095, 3096, 3098, 3105, 3107, 3113, 3114, 3115, 3120, 3122, 3123, 3130, 3132, 3135, 3139, 3140, 3146, 3147, 3150, 3151, 3164, 3165, 3168, 3171, 3174, 3178, 3179, 3181, 3183, 3185, 3186, 3187, 3188, 3190, 3194, 3196, 3197, 3198, 3201, 3203, 3207, 3210, 3212, 3215, 3216, 3217, 3218, 3219, 3221, 3223, 3224, 3225, 3229, 3231, 3232, 3233, 3238, 3239, 3240, 3248, 3249, 3250, 3251, 3253, 3255, 3256, 3259, 3260, 3261, 3273, 3275, 3276, 3278, 3279, 3283, 3285, 3287, 3290, 3298, 3299, 3300, 3302, 3306, 3312, 3314, 3319, 3323, 3324, 3326, 3328, 3329, 3334, 3337, 3340, 3341, 3342, 3343, 3350, 3355, 3356, 3361, 3369, 3370, 3376, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3391, 3392, 3395, 3397, 3398, 3399, 3400, 3401, 3406, 3418, 3420, 3422, 3426, 3429, 3432, 3435, 3436, 3439, 3442, 3443, 3446, 3454, 3455, 3462, 3463, 3464, 3466, 3467, 3468, 3471, 3473, 3477, 3479, 3487, 3491, 3492, 3493, 3496, 3498, 3499, 3502, 3506, 3507, 3509, 3510, 3512, 3513, 3515, 3518, 3521, 3523, 3524, 3525, 3527, 3530, 3532, 3533, 3534, 3535, 3537, 3538, 3544, 3548, 3549, 3551, 3554, 3555, 3556, 3559, 3560, 3562, 3568, 3569, 3570, 3573, 3575, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3585, 3589, 3591, 3594, 3599, 3603, 3608, 3609, 3610, 3613, 3626, 3630, 3631, 3636, 3640, 3641, 3642, 3645, 3647, 3648, 3652, 3653, 3656, 3659, 3661, 3662, 3663, 3667, 3671, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3685, 3686, 3687, 3692, 3693, 3698, 3703, 3704, 3706, 3708, 3709, 3711, 3716, 3717, 3724, 3729, 3733, 3734, 3735, 3737, 3738, 3740, 3741, 3747, 3751, 3753, 3756, 3761, 3763, 3765, 3766, 3767, 3769, 3770, 3779, 3781, 3782, 3784, 3785, 3788, 3789, 3792, 3795, 3802, 3806, 3808, 3810, 3811, 3812, 3813, 3814, 3818, 3822, 3823, 3824, 3825, 3826, 3827, 3829, 3830, 3831, 3835, 3840, 3843, 3846, 3848, 3849, 3853, 3854, 3857, 3858, 3859, 3861, 3863, 3866, 3868, 3869, 3875, 3878, 3881, 3884, 3885, 3886, 3888, 3890, 3893, 3894, 3900, 3902, 3904, 3905, 3908, 3909, 3918, 3922, 3923, 3927, 3930, 3931, 3933, 3936, 3939, 3941, 3944, 3945, 3946, 3947, 3948, 3951, 3954, 3955, 3957, 3959, 3961, 3962, 3963, 3964, 3965, 3970, 3972, 3973, 3975, 3979, 3984, 3990, 3991, 3993, 3994, 3996, 3999, 4005, 4011, 4014, 4020, 4021, 4023, 4025, 4027, 4030, 4035, 4036, 4038, 4039, 4041, 4042, 4047, 4052, 4061, 4075, 4077, 4088, 4092, 4097, 4098, 4099, 4108, 4112, 4116, 4117, 4119, 4123, 4126, 4131, 4139, 4140, 4141, 4149, 4153, 4154, 4161, 4163, 4164, 4170, 4173, 4174, 4177, 4180, 4181, 4183, 4185, 4187, 4190, 4192, 4194, 4196, 4198, 4199, 4201, 4202, 4204, 4210, 4212, 4214, 4215, 4218, 4222, 4223, 4226, 4229, 4230, 4231, 4232, 4233, 4235, 4240, 4241, 4244, 4246, 4251, 4253, 4254, 4259, 4261, 4262, 4263, 4266, 4270, 4271, 4274, 4275, 4276, 4277, 4280, 4281, 4282, 4285, 4288, 4289, 4290, 4291, 4292, 4293, 4295, 4296, 4297, 4300, 4302, 4306, 4310, 4311, 4312, 4315, 4317, 4321, 4323, 4324, 4325, 4330, 4332, 4337, 4340, 4342, 4344, 4347, 4350, 4352, 4354, 4356, 4358, 4366, 4373, 4377, 4379, 4381, 4382, 4383, 4389, 4393, 4397, 4402, 4403, 4406, 4407, 4408, 4413, 4416, 4420, 4430, 4432, 4437, 4440, 4442, 4444, 4450, 4452, 4453, 4454, 4458, 4463, 4465, 4466, 4470, 4472, 4476, 4477, 4481, 4482, 4484, 4489, 4491, 4492, 4493, 4495, 4496, 4503, 4506, 4512, 4519, 4520, 4522, 4524, 4525, 4526, 4529, 4530, 4531, 4536, 4542, 4552, 4553, 4561, 4562, 4571, 4578, 4579, 4580, 4581, 4585, 4588, 4589, 4590, 4593, 4596, 4597, 4598, 4600, 4601, 4603, 4604, 4609, 4612, 4615, 4616, 4621, 4625, 4626, 4627, 4628, 4630, 4633, 4638, 4639, 4641, 4643, 4644, 4645, 4647, 4651, 4652, 4657, 4661, 4665, 4667, 4668, 4670, 4671, 4676, 4677, 4685, 4686, 4687, 4688, 4689, 4691, 4692, 4697, 4698, 4699, 4704, 4711, 4712, 4714, 4715, 4718, 4719, 4724, 4725, 4727, 4731, 4737, 4739, 4740, 4741, 4742, 4744, 4745, 4746, 4748, 4749, 4751, 4752, 4753, 4759, 4762, 4767, 4768, 4771, 4778, 4782, 4783, 4791, 4792, 4800, 4802, 4803, 4805, 4806, 4810, 4812, 4814, 4816, 4817, 4819, 4821, 4824, 4829, 4831, 4832, 4834, 4835, 4836, 4841, 4843, 4847, 4849, 4850, 4851, 4852, 4853, 4855, 4858, 4859, 4862, 4863, 4864, 4865, 4867, 4872, 4873, 4874, 4884, 4887, 4888, 4891, 4896, 4898, 4900, 4901, 4902, 4903, 4908, 4912, 4915, 4922, 4923, 4930, 4932, 4933, 4934, 4935, 4936, 4939, 4941, 4944, 4946, 4947, 4949, 4950, 4951, 4953, 4954, 4955, 4958, 4963, 4964, 4967, 4972, 4973, 4974, 4975, 4976, 4978, 4979, 4980, 4982, 4985, 4986, 4988, 4989, 4998, 4999, 5003, 5004, 5005, 5006, 5007, 5008, 5010, 5011, 5014, 5019, 5025, 5030, 5034, 5035, 5043, 5045, 5046, 5049, 5054, 5059, 5060, 5068, 5070, 5071, 5076, 5079, 5081, 5082, 5085, 5087, 5093, 5095, 5097, 5099, 5100, 5102, 5107, 5109, 5114, 5117, 5119, 5123, 5127, 5130, 5132, 5137, 5139, 5140, 5142, 5145, 5147, 5151, 5152, 5153, 5158, 5159, 5161, 5165, 5167, 5168, 5172, 5173, 5175, 5176, 5181, 5187, 5188, 5191, 5194, 5196, 5197, 5198, 5201, 5203, 5204, 5209, 5211, 5212, 5215, 5221, 5226, 5227, 5231, 5232, 5236, 5237, 5240, 5244, 5247, 5254, 5263, 5264, 5271, 5274, 5275, 5279, 5281, 5282, 5284, 5286, 5288, 5290, 5292, 5293, 5294, 5295, 5301, 5305, 5308, 5309, 5313, 5317, 5319, 5321, 5322, 5323, 5324, 5325, 5326, 5329, 5330, 5331, 5332, 5333, 5337, 5338, 5339, 5341, 5342, 5343, 5346, 5349, 5355, 5356, 5357, 5358, 5360, 5362, 5364, 5366, 5370, 5373, 5374, 5375, 5379, 5380, 5383, 5385, 5387, 5392, 5395, 5396, 5398, 5399, 5401, 5403, 5406, 5411, 5412, 5413, 5415, 5416, 5419, 5421, 5422, 5423, 5428, 5429, 5431, 5434, 5438, 5443, 5444, 5447, 5448, 5449, 5451, 5452, 5454, 5456, 5457, 5459, 5461, 5462, 5463, 5467, 5478, 5479, 5480, 5481, 5482, 5485, 5488, 5489, 5490, 5493, 5494, 5495, 5498, 5499, 5505, 5507, 5508, 5512, 5515, 5520, 5521, 5522, 5523, 5524, 5531, 5534, 5535, 5537, 5543, 5546, 5549, 5554, 5556, 5558, 5559, 5560, 5561, 5563, 5565, 5566, 5570, 5573, 5574, 5577, 5579, 5580, 5584, 5585, 5591, 5606, 5611, 5614, 5615, 5623, 5624, 5626, 5628, 5631, 5632, 5633, 5634, 5635, 5636, 5639, 5640, 5645, 5648, 5651, 5654, 5655, 5658, 5660, 5661, 5662, 5663, 5666, 5670, 5671, 5674, 5679, 5681, 5686, 5687, 5690, 5692, 5697, 5698, 5700, 5702, 5703, 5705, 5706, 5710, 5712, 5713, 5714, 5716, 5718, 5720, 5723, 5724, 5726, 5728, 5732, 5733, 5737, 5741, 5742, 5744, 5746, 5747, 5749, 5752, 5755, 5758, 5761, 5771, 5773, 5775, 5776, 5777, 5779, 5780, 5784, 5786, 5787, 5788, 5789, 5790, 5791, 5792, 5797, 5804, 5805, 5807, 5808, 5810, 5811, 5812, 5817, 5820, 5821, 5828, 5829, 5830, 5832, 5835, 5838, 5840, 5842, 5844, 5851, 5852, 5858, 5862, 5865, 5866, 5867, 5868, 5869, 5870, 5873, 5875, 5877, 5879, 5880, 5881, 5882, 5884, 5886, 5892, 5893, 5897, 5899, 5900, 5904, 5906, 5909, 5910, 5912, 5913, 5916, 5917, 5918, 5919, 5926, 5929, 5930, 5931, 5933, 5935, 5936, 5938, 5939, 5946, 5949, 5951, 5956, 5957, 5964, 5968, 5970, 5974, 5976, 5977, 5979, 5987, 5988, 5992, 5994, 6003, 6005, 6006, 6014, 6015, 6016, 6020, 6024, 6031, 6032, 6033, 6034, 6036, 6039, 6041, 6043, 6044, 6045, 6053, 6056, 6060, 6063, 6065, 6066, 6069, 6072, 6073, 6074, 6076, 6078, 6079, 6081, 6086, 6088, 6093, 6094, 6103, 6104, 6107, 6108, 6110, 6112, 6113, 6117, 6119, 6121, 6124, 6126, 6136, 6138, 6140, 6141, 6142, 6143, 6145, 6148, 6149, 6151, 6153, 6159, 6160, 6161, 6163, 6164, 6165, 6176, 6177]\n"
     ]
    }
   ],
   "source": [
    "print(search_engine.main_index[\"This\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_index_decompressed[\"This\"] == search_engine.main_index[\"This\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_index_decompressed[\"model\"] == search_engine.main_index[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the uncompressed index: 3439175 bytes\n",
      "Size of the variable byte encoded index: 1403079 bytes\n",
      "Size of the gamma encoded index: 999760 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def get_size(path):\n",
    "    \"\"\"\n",
    "    Get the size of the given file.\n",
    "    \"\"\"\n",
    "    return os.stat(path).st_size\n",
    "\n",
    "print(f\"Size of the uncompressed index: {get_size('uncompressed_index.json')} bytes\")\n",
    "print(f\"Size of the variable byte encoded index: {get_size('variable_byte_index')} bytes\")\n",
    "print(f\"Size of the gamma encoded index: {get_size('gamma_index')} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompressed_index_vb_encoding = decompress_index_vb_encoding(\"variable_byte_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompressed_index_vb_encoding[\"This\"] == search_engine.main_index[\"This\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decompressed_index_vb_encoding[\"recent\"] == search_engine.main_index[\"recent\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08ac30a6a1fd2e576b33e03f7d61c3a285d7ee0582c2dd23dde6343ef303ebe9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
