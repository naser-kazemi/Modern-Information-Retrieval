[
    {
        "Id": "12508951ba96b7d4c0906ed95542287d3ebdfd95",
        "title": "The Eighth Visual Object Tracking VOT2020 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Joni-Kristian K{\\&quot;a}m{\\&quot;a}r{\\&quot;a}inen",
            "Martin Danelljan",
            "Luka Cehovin Zajc",
            "Alan Luke{\\vz}i{\\vc}",
            "Ondrej Drbohlav",
            "Linbo He",
            "Yushan Zhang",
            "Song Yan",
            "Jinyu Yang",
            "Gustavo Javier Fernandez",
            "Alexander G. Hauptmann",
            "Alireza Memarmoghadam",
            "{\\&#x27;A}lvaro Garc{\\&#x27;i}a-Mart{\\&#x27;i}n",
            "Andreas Robinson",
            "Anton Yuriiovych Varfolomieiev",
            "Awet Haileslassie Gebrehiwot",
            "Bedirhan Uzun",
            "Bin Yan",
            "Bing Li",
            "Chen Qian",
            "Chi-Yi Tsai",
            "Christian Micheloni",
            "Dong Wang",
            "Fei Wang",
            "Fei Xie",
            "Felix J{\\&quot;a}remo Lawin",
            "Fredrik K. Gustafsson",
            "Gian Luca Foresti",
            "Goutam Bhat",
            "Guang-Gui Chen",
            "Haibin Ling",
            "Haitao Zhang",
            "Hakan Cevikalp",
            "Haojie Zhao",
            "Haoran Bai",
            "Hari Chandana Kuchibhotla",
            "Hasan Saribas",
            "Heng Fan",
            "Hossein Ghanei-Yakhdan",
            "Houqiang Li",
            "Houwen Peng",
            "Huchuan Lu",
            "Hui Li",
            "Javad Khaghani",
            "Jes{\\&#x27;u}s Besc{\\&#x27;o}s",
            "Jianhua Li",
            "Jianlong Fu",
            "Jiaqian Yu",
            "Jingtao Xu",
            "Josef Kittler",
            "Jun Yin",
            "Junhyun Lee",
            "Kaicheng Yu",
            "Kaiwen Liu",
            "Kang Yang",
            "Kenan Dai",
            "Li Cheng",
            "Li Zhang",
            "Lijun Wang",
            "Linyuan Wang",
            "Luc Van Gool",
            "Luca Bertinetto",
            "Matteo Dunnhofer",
            "Miao Cheng",
            "Mohana Murali Dasari",
            "Ning Wang",
            "Pengyu Zhang",
            "Philip H. S. Torr",
            "Qiang Wang",
            "Radu Timofte",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Seokeon Choi",
            "Seyed Mojtaba Marvasti-Zadeh",
            "Shaochuan Zhao",
            "Shohreh Kasaei",
            "Shoumeng Qiu",
            "Shuhao Chen",
            "Thomas Bo Sch{\\&quot;o}n",
            "Tianyang Xu",
            "Wei Lu",
            "Weiming Hu",
            "Wen-gang Zhou",
            "Xi Qiu",
            "Xiao Ke",
            "Xiaojun Wu",
            "Xiaolin Zhang",
            "Xiaoyun Yang",
            "Xuefeng Zhu",
            "Yingjie Jiang",
            "Yingming Wang",
            "Yiwei Chen",
            "Yu Ye",
            "Yuezhou Li",
            "Yuncon Yao",
            "Yunsung Lee",
            "Yuzhang Gu",
            "Zezhou Wang",
            "Zhangyong Tang",
            "Zhenhua Feng",
            "Zhijun Mai",
            "Zhipeng Zhang",
            "Zhirong Wu",
            "Ziang Ma"
        ],
        "date": "23 August 2020",
        "abstract": "A significant novelty is introduction of a new VOT short-term tracking evaluation methodology, and introduction of segmentation ground truth in the VOT-ST2020 challenge \u2013 bounding boxes will no longer be used in theVDT challenges. The Visual Object Tracking challenge VOT2020 is the eighth annual tracker benchmarking activity organized by the VOT initiative. Results of 58 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The VOT2020 challenge was composed of five sub-challenges focusing on different tracking domains: (i) VOT-ST2020 challenge focused on short-term tracking in RGB, (ii) VOT-RT2020 challenge focused on \u201creal-time\u201d short-term tracking in RGB, (iii) VOT-LT2020 focused on long-term tracking namely coping with target disappearance and reappearance, (iv) VOT-RGBT2020 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2020 challenge focused on long-term tracking in RGB and depth imagery. Only the VOT-ST2020 datasets were refreshed. A significant novelty is introduction of a new VOT short-term tracking evaluation methodology, and introduction of segmentation ground truth in the VOT-ST2020 challenge \u2013 bounding boxes will no longer be used in the VOT-ST challenges. A The Eighth Visual Object Tracking VOT2020 Challenge Results 3 new VOT Python toolkit that implements all these novelites was introduced. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.",
        "references": [
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "dd45fe910a0200d43aaa77362f658542f6e175ff",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "320d05db95ab42ade69294abe46cd1aca6aca602"
        ],
        "related_topics": [
            "VOT2020",
            "Accurate Tracking By Overlap Maximization",
            "Visual Object Tracking",
            "Short-term Tracking",
            "Alpha-Refine",
            "GlobalTrack",
            "Meta-Updater",
            "DiMP",
            "Reappearance",
            "SuperDiMP"
        ],
        "reference_count": "87",
        "citation_count": "187"
    },
    {
        "Id": "c6db34ade32b3681a92068b22a354903b2953d52",
        "title": "Benign and malignant breast tumors classification based on region growing and CNN segmentation",
        "authors": [
            "Rahimeh Rouhi",
            "Mehdi Jafari",
            "Shohreh Kasaei",
            "Peiman Keshavarzian"
        ],
        "date": "15 February 2015",
        "abstract": "Semantic Scholar extracted view of \"Benign and malignant breast tumors classification based on region growing and CNN segmentation\" by R. Rouhi et al.",
        "references": [
            "3c948ca247c6f55ef994400e713412b5f845dd40",
            "9b5d0a48b0feb156a1270da54d90d0963a3f0404",
            "76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "d0059280e3f69b8fd07ce036e7d2407e3ebcff9e",
            "2dfb1fd3adfa58a3448251e03b1a5a78239958b3",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "342da5d8633aebf27d914a8618e523579a130289",
            "fe83150bc326fd62d352cb2993ac91344f195e10",
            "483f0f12feb8ac1c396349e5526a7552b6b067cd",
            "a9d0b3485f3091e832f87edb469c350c90cabae1"
        ],
        "related_topics": [
            "Cellular Neural Networks",
            "Mammograms",
            "Malignant",
            "Convolutional Neural Network",
            "Mammographic Image Analysis Society",
            "Approximate Nearest Neighbor",
            "Support Vector Machines",
            "Random Forests",
            "Digital Database For Screening Mammography",
            "Classification"
        ],
        "reference_count": "58",
        "citation_count": "352"
    },
    {
        "Id": "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
        "title": "Event Detection and Summarization in Soccer Videos Using Bayesian Network and Copula",
        "authors": [
            "Mostafa Tavassolipour",
            "Mahmood Karimian",
            "Shohreh Kasaei"
        ],
        "date": "1 February 2014",
        "abstract": "A novel Bayesian network-based method that is capable of detecting seven different events in soccer videos; namely, goal, card, goal attempt, corner, foul, offside, and nonhighlights is proposed. Semantic video analysis and automatic concept extraction play an important role in several applications; including content-based search engines, video indexing, and video summarization. As the Bayesian network is a powerful tool for learning complex patterns, a novel Bayesian network-based method is proposed for automatic event detection and summarization in soccer videos. The proposed method includes efficient algorithms for shot boundary detection, shot view classification, mid-level visual feature extraction, and construction of the related Bayesian network. The method contains of three main stages. In the first stage, the shot boundaries are detected. Using the hidden Markov model, the video is segmented into large and meaningful semantic units, called play-break sequences. In the next stage, several features are extracted from each of these units. Finally, in the last stage, in order to achieve high level semantic features (events and concepts), the Bayesian network is used. The basic part of the method is constructing the Bayesian network, for which the structure is estimated using the Chow-Liu tree. The joint distributions of random variables of the network are modeled by applying the Farlie-Gumbel-Morgenstern family of Copulas. The performance of the proposed method is evaluated on a dataset with about 9 h of soccer videos. The method is capable of detecting seven different events in soccer videos; namely, goal, card, goal attempt, corner, foul, offside, and nonhighlights. Experimental results show the effectiveness and robustness of the proposed method on detecting these events.",
        "references": [
            "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a",
            "43eb2d80721daa318cc19e4cce405122934e88f2",
            "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e",
            "0365c1382924394e200cb627e59cb9c21f8e75bd",
            "c9edc10d5c22ae62a700c37a41bb0ea22961a0aa",
            "90fbd777cf57096e9292601dfe0dbab30198d40f",
            "f7b4fcc9dbb97997dc1793d7a366cba274f53134",
            "8e0f60b718fa19c2ed10bd93401796683af79512",
            "c2f8178ce89a6cc1ad0e1dd5db4bf155d5a80620"
        ],
        "related_topics": [
            "Bayesian Networks",
            "Soccer Videos",
            "Event Detection",
            "Goal Attempt",
            "Summarization",
            "Shot Boundary Detection",
            "Corner",
            "Video Summarization",
            "Hidden Markov Models",
            "Semantic Video Analysis"
        ],
        "reference_count": "40",
        "citation_count": "98"
    },
    {
        "Id": "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
        "title": "An efficient PCA-based color transfer method",
        "authors": [
            "Arash Abadpour",
            "Shohreh Kasaei"
        ],
        "date": "1 February 2007",
        "abstract": "Semantic Scholar extracted view of \"An efficient PCA-based color transfer method\" by A. Abadpour et al.",
        "references": [
            "e1dfff8cff33ede3564a35724632bddc5b8619b1",
            "d93e4e77e2baba98f1af7f23d47fbf9b46be4df5",
            "05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1",
            "0c2ced886708cc3aea4705f8765d152cd3f69cd2",
            "a2882b8b0c9635d39d15a28138e3f47907f3177b",
            "61579369e7b97dec9c699c058edaafcde2817d21",
            "4a6c5c9b1fb106f7d82508ae593d30e207c8ea45",
            "ab67b9d0da50e251a4f7e42370540547b891ceb1",
            "d5c6edb53dc41f298f145041cd2c53e40e3acf2b",
            "ed7e166f65bcecc522c6c4bbb29fcf8048010873"
        ],
        "related_topics": [
            "Color Transfer",
            "Principal Component Analysis",
            "Vector Space",
            "Color Transfer Method",
            "Natural Images",
            "Colorizing"
        ],
        "reference_count": "45",
        "citation_count": "67"
    },
    {
        "Id": "1fbb4201af091aef55360f113ba35814063923e4",
        "title": "Deep Learning for Visual Tracking: A Comprehensive Survey",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Li Cheng",
            "Hossein Ghanei-Yakhdan",
            "Shohreh Kasaei"
        ],
        "date": "2 December 2019",
        "abstract": "This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics, and extensively evaluates and analyzes the leading visualtracking methods. Visual target tracking is one of the most sought-after yet challenging research topics in computer vision. Given the ill-posed nature of the problem and its popularity in a broad range of real-world scenarios, a number of large-scale benchmark datasets have been established, on which considerable methods have been developed and demonstrated with significant progress in recent years \u2013 predominantly by recent deep learning (DL)-based methods. This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics. It also extensively evaluates and analyzes the leading visual tracking methods. First, the fundamental characteristics, primary motivations, and contributions of DL-based methods are summarized from nine key aspects of: network architecture, network exploitation, network training for visual tracking, network objective, network output, exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Second, popular visual tracking benchmarks and their respective properties are compared, and their evaluation metrics are summarized. Third, the state-of-the-art DL-based methods are comprehensively examined on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by conducting critical analyses of these state-of-the-art trackers quantitatively and qualitatively, their pros and cons under various common scenarios are investigated. It may serve as a gentle use guide for practitioners to weigh when and under what conditions to choose which method(s). It also facilitates a discussion on ongoing issues and sheds light on promising research directions.",
        "references": [
            "26e2ca763087be09e3799ad294302aa91077942d",
            "021d0c7013da519b508610064f264c76d768fdf1",
            "0a400fd7f0ee28694889baaa4faef150b6912dfa",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "388d29f001411ff80650f80cf197afc440d98b51",
            "f24015a365ea2454391c285cd30b8ae723dbb05e",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "1855818c492d5f42dbe14814e4dd9b5733d54790",
            "e2e34b202363e4a46a14cd35fd4088d88b2e650e"
        ],
        "related_topics": [
            "Large-scale Single Object Tracking",
            "SiamRPN++",
            "DaSiamRPN",
            "Absent Label",
            "TC128",
            "TrackingNet",
            "Accurate Tracking By Overlap Maximization",
            "Deep Learning",
            "OTB-2015",
            "VOT2018"
        ],
        "reference_count": "272",
        "citation_count": "205"
    },
    {
        "Id": "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
        "title": "The Ninth Visual Object Tracking VOT2021 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Jiri Matas",
            "Ale{\\vs} Leonardis",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "J. K{\\&quot;a}m{\\&quot;a}r{\\&quot;a}inen",
            "Hyung Jin Chang",
            "Martin Danelljan",
            "Luka Cehovin Zajc",
            "Alan Luke{\\vz}i{\\vc}",
            "Ondrej Drbohlav",
            "Jani K{\\&quot;a}pyl{\\&quot;a}",
            "Gustav H{\\&quot;a}ger",
            "Song Yan",
            "Jinyu Yang",
            "Zhongqun Zhang",
            "Gustavo Javier Fernandez",
            "Mohamed H. Abdelpakey",
            "Goutam Bhat",
            "Llukman Cerkezi",
            "Hakan \u00c7evikalp",
            "Shengyong Chen",
            "Xin Chen",
            "Miao Cheng",
            "Ziyi Cheng",
            "Yu-Chen Chiu",
            "Ozgun Cirakman",
            "Yutao Cui",
            "Kenan Dai",
            "Mohana Murali Dasari",
            "Qili Deng",
            "Xingping Dong",
            "Daniel K. Du",
            "Matteo Dunnhofer",
            "Zhenhua Feng",
            "Zhiyong Feng",
            "Z. Fu",
            "Shiming Ge",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Yuzhang Gu",
            "Bilge Gunsel",
            "Qing Guo",
            "Filiz Gurkan",
            "Wencheng Han",
            "Yanyan Huang",
            "Felix J{\\&quot;a}remo Lawin",
            "Shang-Jhih Jhang",
            "Rongrong Ji",
            "Cheng Jiang",
            "Yingjie Jiang",
            "Felix Juefei-Xu",
            "Yin Jun",
            "Xiaolong Ke",
            "Fahad Shahbaz Khan",
            "Byeong Hak Kim",
            "Josef Kittler",
            "Xiangyuan Lan",
            "Jun Ha Lee",
            "Bastian Leibe",
            "Hui Li",
            "Jianhua Li",
            "Xianxian Li",
            "Yuezhou Li",
            "Bo Liu",
            "Chang Liu",
            "Jingen Liu",
            "Li Liu",
            "Qingjie Liu",
            "Huchuan Lu",
            "Wei Lu",
            "Jonathon Luiten",
            "Jie Ma",
            "Ziang Ma",
            "Niki Martinel",
            "Christoph Mayer",
            "Alireza Memarmoghadam",
            "Christian Micheloni",
            "Yuzhen Niu",
            "Danda Pani Paudel",
            "Houwen Peng",
            "Shoumeng Qiu",
            "Aravindh Rajiv",
            "Muhammad Abid Rana",
            "Andreas Robinson",
            "Hasan Saribas",
            "Ling Shao",
            "Mohamed S. Shehata",
            "Furao Shen",
            "Jianbing Shen",
            "Kristian Simonato",
            "Xiaoning Song",
            "Zhangyong Tang",
            "Radu Timofte",
            "Philip H. S. Torr",
            "Chi-Yi Tsai",
            "Bedirhan Uzun",
            "Luc Van Gool",
            "Paul Voigtlaender",
            "Dong Wang",
            "Guangting Wang",
            "Liangliang Wang",
            "Lijun Wang",
            "Limin Wang",
            "Linyuan Wang",
            "Yong Wang",
            "Yunhong Wang",
            "Chenyang Wu",
            "Gangshan Wu",
            "Xiaojun Wu",
            "Fei Xie",
            "Tianyang Xu",
            "Xiang Xu",
            "Wanli Xue",
            "Bin Yan",
            "Wankou Yang",
            "Xiaoyun Yang",
            "Yu Ye",
            "J. Yin",
            "Chengwei Zhang",
            "Chunhui Zhang",
            "Haitao Zhang",
            "Kaihua Zhang",
            "Kangkai Zhang",
            "Xiaohan Zhang",
            "Xiaolin Zhang",
            "Xinyu Zhang",
            "Zhibing Zhang",
            "Shaochuan Zhao",
            "Mingmin Zhen",
            "Bineng Zhong",
            "Jiawen Zhu",
            "Xuefeng Zhu"
        ],
        "date": "1 October 2021",
        "abstract": "The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative; results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT-RT2021 challenge focused on \"real-time\" short-term tracking in RGB, (iii) VOT-LT2021 focused on long-term tracking, namely coping with target disappearance and reappearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2021 dataset was refreshed, while VOT-RGBD2021 introduces a training dataset and sequestered dataset for winner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website1.",
        "references": [
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "f202feae9ca7b3766e072b6af657beed2236a93c",
            "0f50914e86b6010586f1772308858de9a418fb9f",
            "be412c7c7128cf91455233b652d6c94a6001a7c8"
        ],
        "related_topics": [
            "VOT2021",
            "Chinese Discourse Treebank",
            "Accurate Tracking By Overlap Maximization",
            "Rgb And Depth",
            "SuperDiMP",
            "Short-term Trackers",
            "Long-Term Tracker",
            "VOT Challenge",
            "TrDiMP",
            "Recursive Graph Bisection"
        ],
        "reference_count": "68",
        "citation_count": "72"
    },
    {
        "Id": "05df852d87566d335bbbf1864d191910205f482b",
        "title": "The First Visual Object Tracking Segmentation VOTS2023 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Jivr&#x27;i Matas",
            "Martin Danelljan",
            "Michael Felsberg",
            "Hyung Jin Chang",
            "Luka \u02c7Cehovin Zajc",
            "Alan Luke\u02c7zi\u02c7c",
            "Ondrej Drbohlav",
            "Zhongqun Zhang",
            "Khanh-Tung Tran",
            "Xuan-Son Vu",
            "Johanna Bj\u00a8orklund",
            "Christoph Mayer",
            "Yushan Zhang",
            "Lei Ke",
            "Jie Zhao",
            "Gustavo Fern\u00b4andez",
            "Noor M. Al-Shakarji",
            "Dong An",
            "Michael Arens",
            "Stefan Becker",
            "Goutam Bhat",
            "Sebastian Bullinger",
            "Antoni B. Chan",
            "Shijie Chang",
            "Hanyuan Chen",
            "Xin Chen",
            "Yan Chen",
            "Zhenyu Chen",
            "Yan-Xiang Cheng",
            "Yutao Cui",
            "Chunyuan Deng",
            "Jiahua Dong",
            "Matteo Dunnhofer",
            "Wei Feng",
            "Jianlong Fu",
            "Jie Gao",
            "Ruize Han",
            "Zeqi Hao",
            "Jun-Yan He",
            "Keji He",
            "Zhenyu He",
            "Xiantao Hu",
            "Kaer Huang",
            "Yuqing Huang",
            "Yi Jiang",
            "Ben Kang",
            "Jinpeng Lan",
            "Hyungjun Lee",
            "Chenyang Li",
            "Jiahao Li",
            "Ning Li",
            "Wangkai Li",
            "Xiaodi Li",
            "Xin Li",
            "Pengyu Liu",
            "Yue Liu",
            "Huchuan Lu",
            "Bin Luo",
            "Ping Luo",
            "Yinchao Ma",
            "Deshui Miao",
            "Christian Micheloni",
            "Kannappan Palaniappan",
            "Hancheol Park",
            "Matthieu Paul",
            "Houwen Peng",
            "Zekun Qian",
            "Gani Rahmon",
            "Norbert Scherer-Negenborn",
            "Pengcheng Shao",
            "Wooksu Shin",
            "Elham Soltani Kazemi",
            "Tian-Ming Song",
            "Rainer Stiefelhagen",
            "Rui Sun",
            "Chuanming Tang",
            "Zhangyong Tang",
            "Imad Eddine Toubal",
            "Jack Valmadre",
            "Joost van de Weijer",
            "Luc Van Gool",
            "Jash Vira",
            "Stephane Vujasinovi\u00b4c",
            "Cheng Wan",
            "Jia Wan",
            "Dong Wang",
            "Fei Wang",
            "Hekun Wang",
            "Limin Wang",
            "Song Wang",
            "Yaowei Wang",
            "Zhepeng Wang",
            "Gangshan Wu",
            "Jiannan Wu",
            "Qiangqiang Wu",
            "Xiaojun Wu",
            "Anqi Xiao",
            "Jinxia Xie",
            "Chen Chen Xu",
            "Min Xu",
            "Tian-hao Xu",
            "Yuanyou Xu",
            "Bin Yan",
            "Dawei Yang",
            "Mingdong Yang",
            "Tianyu Yang",
            "Yi Yang",
            "Zongxin Yang",
            "Xuanwu Yin",
            "Fisher Yu",
            "Hongyuan Yu",
            "Qian Yu",
            "Weichen Yu",
            "YongSheng Yuan",
            "Zehuan Yuan",
            "Jianlin Zhang",
            "Lu Zhang",
            "Tianzhu Zhang",
            "Guodongfang Zhao",
            "Shaochuan Zhao",
            "Ya-Jing Zheng",
            "Bineng Zhong",
            "Jiawen Zhu",
            "Xuefeng Zhu",
            "Yueting Zhuang",
            "Cheng Zong",
            "Kunlong Zuo"
        ],
        "date": "2 October 2023",
        "abstract": "Results of the presented 47 trackers indicate that modern tracking frameworks are well-suited to deal with convergence of short-term and long-term tracking and that multiple and single target tracking can be considered a single problem. The Visual Object Tracking Segmentation VOTS2023 challenge is the eleventh annual tracker benchmarking activity of the VOT initiative. This challenge is the first to merge short-term and long-term as well as single-target and multiple-target tracking with segmentation masks as the only target location specification. A new dataset was created; the ground truth has been withheld to prevent overfitting. New performance measures and evaluation protocols have been created along with a new toolkit and an evaluation server. Results of the presented 47 trackers indicate that modern tracking frameworks are well-suited to deal with convergence of short-term and long-term tracking and that multiple and single target tracking can be considered a single problem. A leaderboard, with participating trackers details, the source code, the datasets, and the evaluation kit are publicly available at the challenge website1.",
        "references": [
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "53329e5c79c1128c7b252a12b182c472a3413bfa",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "9286efaa3dba58837b628f61f4940a09b3eeb85c"
        ],
        "related_topics": [],
        "reference_count": "71",
        "citation_count": "2"
    },
    {
        "Id": "2d4713ce1df60f771b65e900fd02352989df82ef",
        "title": "Long-term Visual Tracking: Review and Experimental Comparison",
        "authors": [
            "Chang Liu",
            "Xiao-Fan Chen",
            "Chunjuan Bo",
            "Dong Wang"
        ],
        "date": "7 November 2022",
        "abstract": "This paper provides a thorough review of long-term tracking, summarizing long- term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results, and discusses the future prospects from multiple perspectives. As a fundamental task in computer vision, visual object tracking has received much attention in recent years. Most studies focus on short-term visual tracking which addresses shorter videos and always-visible targets. However, long-term visual tracking is much closer to practical applications with more complicated challenges. There exists a longer duration such as minute-level or even hour-level in the long-term tracking task, and the task also needs to handle more frequent target disappearance and reappearance. In this paper, we provide a thorough review of long-term tracking, summarizing long-term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results. Then we provide a detailed description of existing benchmarks and corresponding evaluation protocols. Furthermore, we conduct extensive experiments and analyse the performance of trackers on six benchmarks: VOTLT2018, VOTLT2019 (2020/2021), OxUvA, LaSOT, TLP and the long-term subset of VTUAV-V. Finally, we discuss the future prospects from multiple perspectives, including algorithm design and benchmark construction. To our knowledge, this is the first comprehensive survey for long-term visual object tracking. The relevant content is available at https://github.com/wangdong-dut/Long-term-Visual-Tracking.",
        "references": [
            "23f8927f996d56f3b5076d8993a70bcfc70182a1",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "1ae15ff20d54d9ffd2a45a9c124c77ad2b419ae3",
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "894e4376750b83b63649cc518b121f345ca0df83",
            "913cebc279c363fb9476496f096519e27212b3d5"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Long-term Visual Tracking",
            "Large-scale Single Object Tracking",
            "Evaluation Protocol",
            "OxUvA",
            "Computer Vision",
            "Target Disappearances",
            "Time-Lock Puzzles"
        ],
        "reference_count": "100",
        "citation_count": "6"
    },
    {
        "Id": "0fba73b3b1e73db08c68c95384b09659694c1b5d",
        "title": "Learning Spatial Distribution of Long-Term Trackers Scores",
        "authors": [
            "Vincenzo Mariano Scarrica",
            "Antonino Staiano"
        ],
        "date": "2 August 2023",
        "abstract": "This work aims to generalize the fusion concept to an arbitrary number of trackers used as baseline trackers in the pipeline, leveraging a learning phase to better understand how outcomes correlate with each other, even when no target is present. Long-Term tracking is a hot topic in Computer Vision. In this context, competitive models are presented every year, showing a constant growth rate in performances, mainly measured in standardized protocols as Visual Object Tracking (VOT) and Object Tracking Benchmark (OTB). Fusion-trackers strategy has been applied over last few years for overcoming the known re-detection problem, turning out to be an important breakthrough. Following this approach, this work aims to generalize the fusion concept to an arbitrary number of trackers used as baseline trackers in the pipeline, leveraging a learning phase to better understand how outcomes correlate with each other, even when no target is present. A model and data independence conjecture will be evidenced in the manuscript, yielding a recall of 0.738 on LTB-50 dataset when learning from VOT-LT2022, and 0.619 by reversing the two datasets. In both cases, results are strongly competitive with state-of-the-art and recall turns out to be the first on the podium.",
        "references": [
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "009e625561119aa9affb936ad74e611fd1fa36d4",
            "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Object Tracking Benchmark",
            "Baseline Trackers",
            "Long-term Tracking",
            "Long-Term Tracker",
            "Computer Vision"
        ],
        "reference_count": "0",
        "citation_count": "48"
    },
    {
        "Id": "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
        "title": "Switch and Refine: A Long-Term Tracking and Segmentation Framework",
        "authors": [
            "Xiang Xu",
            "Jian Zhao",
            "Jianmin Wu",
            "Furao Shen"
        ],
        "date": "1 March 2023",
        "abstract": "A new long-term VOT framework is proposed that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. In long-term video object tracking (VOT) tasks, most long-term trackers are modified from short-term trackers, which contain more and more machine learning modules to improve their performance. However, we empirically find that more modules do not necessarily lead to better results. In this paper, we make the long-term tracking framework simple by carefully selecting the cutting-edge trackers. Specifically, we propose a new long-term VOT framework that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. Such a framework fully exploits existing advanced works from three complementary perspectives. Experimental results show that by exploiting the capabilities of existing methods instead of designing new neural networks, we can still achieve remarkable results on seven long-term VOT datasets. By introducing a continuous adjustable speed control parameter, our tracker reaches 20+FPS with only a small performance loss. The refine module not only improves the bounding box estimations but also outputs segmentation masks, so that our framework can handle the video object segmentation (VOS) tasks by using only VOT trackers. We obtain a trade-off between time and accuracy on two representative VOS datasets by only using bounding boxes as the initial input.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "f1c9f81ce054619f30b5c27fd97579f7216d7048",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "f6186788541d332af19a96183787e01ef9080fb0",
            "3985382474245388bbc73e2c849e783010901775"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Video Object Segmentation",
            "Global Re-detector",
            "Bounding Box Estimation",
            "Long-term Tracking",
            "Neural Network",
            "Segmentation Masks",
            "Long-term Tracking Framework"
        ],
        "reference_count": "76",
        "citation_count": "4"
    },
    {
        "Id": "d884af3933148cef3b50fd38c810f5a7763d0fc9",
        "title": "Multi-modal visual tracking: Review and experimental comparison",
        "authors": [
            "Pengyu Zhang",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "date": "8 December 2020",
        "abstract": "Different aspects of multi-modal tracking algorithms are summarized under a unified taxonomy, with specific focus on visible-depth (RGB-D) and visible-thermal (RGB-T) tracking. Visual object tracking has been drawing increasing attention in recent years, as a fundamental task in computer vision. To extend the range of tracking applications, researchers have been introducing information from multiple modalities to handle specific scenes, with promising research prospects for emerging methods and benchmarks. To provide a thorough review of multi-modal tracking, different aspects of multi-modal tracking algorithms are summarized under a unified taxonomy, with specific focus on visible-depth (RGB-D) and visible-thermal (RGB-T) tracking. Subsequently, a detailed description of the related benchmarks and challenges is provided. Extensive experiments were conducted to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, various future directions, including model design and dataset construction, are discussed from different perspectives for further research.",
        "references": [
            "1975bee228ac228df235d20777e32331bb21566d",
            "d8d847b085e9af12eeafc0af8df95ff2a1a98fb5",
            "f202feae9ca7b3766e072b6af657beed2236a93c",
            "761a9b5d8750eb63a9717650c4aaca53ce36a364",
            "5f2ff21932fec3882c4c85c474a1be4645bfd92b",
            "487eb86379e979a72ebfef67db6eb8f048d1d258",
            "d10861d377be150b1e03cb942deb8763095de88f",
            "3fbf32a428db505e0bb45177016e8851d9b31e97",
            "b52382b22d25dd63c2a68424304e39024bf6e15f",
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Princeton Tracking Benchmark",
            "GTOT",
            "Computer Vision",
            "RGBT234",
            "Penn Treebank"
        ],
        "reference_count": "130",
        "citation_count": "10"
    },
    {
        "Id": "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
        "title": "CoCoLoT: Combining Complementary Trackers in Long-Term Visual Tracking",
        "authors": [
            "Matteo Dunnhofer",
            "Christian Micheloni"
        ],
        "date": "9 May 2022",
        "abstract": "This paper provides a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance and competes favourably with the state-of-the-art on the most popular long- term visual tracking benchmarks. How to combine the complementary capabilities of an ensemble of different algorithms has been of central interest in visual object tracking. A significant progress on such a problem has been achieved, but considering short-term tracking scenarios. Instead, long-term tracking settings have been substantially ignored by the solutions. In this paper, we explicitly consider long-term tracking scenarios and provide a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance. CoCoLoT perceives whether the trackers are following the target object through an online learned deep verification model, and accordingly activates a decision policy which selects the best performing tracker as well as it corrects the performance of the failing one. The proposed methodology is evaluated extensively and the comparison with several other solutions reveals that it competes favourably with the state-of-the-art on the most popular long-term visual tracking benchmarks.",
        "references": [
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "bd4f219ce6bc5c22f9da71959d5192cf0b0141fe",
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "c734274f43575bc5f4bcf8719f0be55a5e89be5e",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "5b73cd259a3fa72f95e8bac9e520250b950acf3a",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "eb00b8453b23d4f6f142378e2fb0f0a9e6f9c5e2",
            "19d6b9725a59f4b624205829d5f03ac893ca1367"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Long-term Tracking",
            "Long-term Visual Tracking",
            "Visual Trackers",
            "Complementary Trackers",
            "Ensemble"
        ],
        "reference_count": "47",
        "citation_count": "4"
    },
    {
        "Id": "ca97f741f331b5b43d0577a46c05984f0785a8fa",
        "title": "Is First Person Vision Challenging for Object Tracking?",
        "authors": [
            "Matteo Dunnhofer",
            "Antonino Furnari",
            "Giovanni Maria Farinella",
            "Christian Micheloni"
        ],
        "date": "24 November 2020",
        "abstract": "The study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure, and shows that object tracking in FPV is challenging. Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks.",
        "references": [
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "44990f618f46f02da321b1043a64e72d5f7c0486",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "d1e61fa7824709cae37fb59483dd0772e3101c08",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "61394599ed0aabe04b724c7ca3a778825c7e776f",
            "9559f0b77932a3c5f17aeb8564b400430d173ec7"
        ],
        "related_topics": [
            "TREK-150",
            "First-Person Vision",
            "First-person Video",
            "Object Tracking",
            "Trackers",
            "Camera Wearer",
            "State-of-the-art Trackers",
            "Tracking Scenario",
            "Target Object",
            "Benchmark Dataset"
        ],
        "reference_count": "109",
        "citation_count": "16"
    },
    {
        "Id": "a4f25f02ca52e1122aa7494244928b8e4684258b",
        "title": "Effective long-term tracking with contrast optimizer",
        "authors": [
            "Yongbo Han",
            "Yitao Liang"
        ],
        "date": "1 July 2023",
        "abstract": "A contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. The main challenge of long-term tracking includes data uncertainty in long-term observations. Previous methods tackle the long-term tracking task by online update-based trackers. However, sophisticated online update strategies of these trackers are usually with a considerable computational burden. In this work, a contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. Specifically, the optimizer first perceives the similarity between distractors and positive samples through metric learning. Next, the contrastive learning between target anchors and hard negative samples forces the optimizer to notice the difference between targets and distractors. Finally, the optimizer will learn a binary output to assist the tracker updating. The proposed optimizer can be easily integrated into other online trackers with little impact on their running speed. Extensive experimental results show that the method achieves state-of-the-art performance on the VOT2018LT, VOT2019LT, OxUvA, and LaSOT benchmarks while running at real-time speed on GPU.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4"
        ],
        "related_topics": [
            "Long-term Tracking",
            "Hard Negative Samples",
            "VOT2019LT",
            "Metric Learning",
            "LaSOT Benchmark",
            "VOT2018LT",
            "Trackers",
            "Contrastive Learning",
            "Positive Sample",
            "OxUvA"
        ],
        "reference_count": "0",
        "citation_count": "71"
    },
    {
        "Id": "0530cbeb847f5e5002d1183c482759dff5f8c439",
        "title": "Visual Object Tracking With Discriminative Filters and Siamese Networks: A Survey and Outlook",
        "authors": [
            "Sajid Javed",
            "Martin Danelljan",
            "Fahad Shahbaz Khan",
            "Muhammad Haris Khan",
            "Michael Felsberg",
            "Jiri Matas"
        ],
        "date": "6 December 2021",
        "abstract": "This survey presents a systematic and thorough review of more than 90 DCFs and Siamese trackers, based on results in nine tracking benchmarks, and distinguishes and comprehensively review the shared as well as specific open research challenges in both these tracking paradigms. Accurate and robust visual object tracking is one of the most challenging and fundamental computer vision problems. It entails estimating the trajectory of the target in an image sequence, given only its initial location, and segmentation, or its rough approximation in the form of a bounding box. Discriminative Correlation Filters (DCFs) and deep Siamese Networks (SNs) have emerged as dominating tracking paradigms, which have led to significant progress. Following the rapid evolution of visual object tracking in the last decade, this survey presents a systematic and thorough review of more than 90 DCFs and Siamese trackers, based on results in nine tracking benchmarks. First, we present the background theory of both the DCF and Siamese tracking core formulations. Then, we distinguish and comprehensively review the shared as well as specific open research challenges in both these tracking paradigms. Furthermore, we thoroughly analyze the performance of DCF and Siamese trackers on nine benchmarks, covering different experimental aspects of visual tracking: datasets, evaluation metrics, performance, and speed comparisons. We finish the survey by presenting recommendations and suggestions for distinguished open challenges based on our analysis.",
        "references": [
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "738165f33c50b059e87b14d8b4a129230e14eacd",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "2bcf2bd59219d89f335cbc8d1dd4f431076b4c4c",
            "5648597dc65a3e1fdc6d8e0aeccbf9bf6fe82dcb",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "DCFs",
            "Tracking Paradigm",
            "Bounding Box",
            "Discriminative Correlation Filter",
            "Recommendation",
            "Computer Vision",
            "Tracking Benchmarks",
            "Siamese Networks"
        ],
        "reference_count": "181",
        "citation_count": "72"
    },
    {
        "Id": "2415fc06de82ab41ad8b9615162247afb02974af",
        "title": "Classification of benign and malignant breast tumors based on hybrid level set segmentation",
        "authors": [
            "Rahimeh Rouhi",
            "Mehdi Jafari"
        ],
        "date": "15 March 2016",
        "abstract": "Semantic Scholar extracted view of \"Classification of benign and malignant breast tumors based on hybrid level set segmentation\" by R. Rouhi et al.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "3c948ca247c6f55ef994400e713412b5f845dd40",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "5992c015bf844e958bc78f09b46b39e80c903ac0",
            "76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "6d8c60d47e2be4de763bb2f8044e09981016396e",
            "51cf0b75aea4f46dac484a3049f8897ec112a110",
            "9b5d0a48b0feb156a1270da54d90d0963a3f0404",
            "63a83600a73ffbfd6a58e315a247aa4f3da90a9b",
            "474ae46626676d01c7b38328c107b1531b181b46"
        ],
        "related_topics": [
            "Level Sets",
            "Cellular Neural Networks",
            "Computer Aided Detection ( Cade ) And Diagnosis",
            "Approximate Nearest Neighbor",
            "Malignant",
            "K-nearest Neighbors",
            "Convolutional Neural Network",
            "Mammograms",
            "Support Vector Machines",
            "Random Forests"
        ],
        "reference_count": "58",
        "citation_count": "59"
    },
    {
        "Id": "8133e66c9c03095ef605090e6a72b752dc774d92",
        "title": "Benign and malignant breast cancer segmentation using optimized region growing technique",
        "authors": [
            "Stephan Punitha",
            "A. Amuthan",
            "Kanagaraj Suresh Joseph"
        ],
        "date": "1 December 2018",
        "abstract": "Semantic Scholar extracted view of \"Benign and malignant breast cancer segmentation using optimized region growing technique\" by S. Punitha et al.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "19a85922bcc7c932934a79a85c59e15655d88b2e",
            "d83750e6f32d0172cbb6e10d640aa6cdaf03e2b0",
            "4f325d8be07594c94f0d7568c80c91c55bdb3774",
            "7ff70f493df1a7d5dfc0c437e7fce6a5172be933",
            "34c44883a6152c5298f2c452670c1127072400e6",
            "2c5db1c44dddf0d8e6ea667f3f7afb13032ea386",
            "8fd20170a036d6f139a3b031058865b6c02ad65a",
            "54ceeab87535d4048eb262dcc8cdbc46ae735ea4",
            "37257b305a7a5962306505b60f3384fc0d03f184"
        ],
        "related_topics": [
            "Dragon Fly Optimization",
            "Malignant",
            "Radiologist",
            "Texture Feature"
        ],
        "reference_count": "30",
        "citation_count": "83"
    },
    {
        "Id": "708111a6f38d6c6a94be69da4b33947ed1e52c06",
        "title": "Genetic Algorithm Based Feature Selection for the Classification of Breast Masses in Mammograms",
        "authors": [
            "V. Aravindarajan",
            "R. Prasanna"
        ],
        "date": "2023",
        "abstract": "A 3-class classification system is proposed to classify the breast masses in mammogram images into benign, malignant and normal, and it is observed that GA+KNN combination outperforms with respect to accuracy. - In this paper, a 3-class classification system is proposed to classify the breast masses in mammogram images into benign, malignant and normal. The proposed algorithm consists of five modules: pre-processing of images, segmentation of Region of Interest (RoI), feature extraction, feature selection and classification. Pre-processing of an image is done by using Adaptive Histogram Equalization (AHE). Region growing algorithm is used to segment the Region of Interest (ROI) from the pre-processed image. Feature matrix is generated by using Gray Level Co-occurrence Matrix (GLCM) for the segmented ROI. A Genetic algorithm (GA) is used for selecting an optimal set of features from the feature matrix.The performance of GA is compared with a t-test. To evaluate the performance, three classifiers are used and the results are compared. They are multiSVM, KNN, and Naive Bayes classifiers. Totally, six combinations are evaluated. GA+multiSVM, GA+KNN, GA+Naive Bayes, t-test+multiSVM, t-test+KNN, and t-test+Naive Bayes. It is observed that GA+KNN combination outperforms with respect to accuracy. The mammogram images used in this work are collected from the publicly available dataset, Digital Database for Screening Mammography (DDSM ).",
        "references": [
            "9830278ec1efee0fb3c5a84ec8bf28dced81fbff",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "5384355c1dcb8b6325b351e2317cf16eb8a2aba6",
            "241b3094b2cc3682df765ed3ed70f8a616358729",
            "1d22adf2080a7a5e98739e11419a9e837de21432",
            "82b09f56d68eefdb3a3442cfe29433cd72480d64",
            "9f8034a9f7448f59d0ddd4485d6e01dff2b270f9",
            "db06e9623dbe034a8e5d351011148be70bc83dab",
            "cabeb7c149cfc54fac61431ed3f6a2d0e74269f7",
            "2a937dc9c5e24728bfee5e60045ca6d710930aa4"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "25"
    },
    {
        "Id": "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
        "title": "An enhancement of mammogram images for breast cancer classification using artificial neural networks",
        "authors": [
            "Jalpa J. Patel",
            "Sarman K. Hadia"
        ],
        "date": "1 June 2021",
        "abstract": "In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification of mammogram images. Breast cancer is the most driving reason for death in women in both developed and developing nations. For the plan of effective classification of a system, the selection of features method must be used to decrease irregularity part in mammogram images. The proposed approach is used to crop the region of interests (ROIs) manually. Based on that number of features are extracted. In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification. A number of selected features is applied to train the neural network. In this proposed method accessible informational index from the mini\u2013mammographic image analysis society (MIAS) database was used. The classification of this mammogram database involved a neural networks classifier which attained an accuracy of 99.7% with a sensitivity of 99.5%, and specificity of 100% as the area under the curve (AUC) is 0.9975 and matthew\u2019s correlation coefficient (MCC) represents a binary class value which reached the value of 0.9931. It can be useful in a computer-aided diagnosis system (CAD) framework to help the radiologist in analyzing breast cancer. Results achieved with the proposed method are better compared to recent work.",
        "references": [
            "b7cd3daaf49d0d68579015680d123044683d70ee",
            "b06a69cf5663829d4f4f8168d820b2f7baf88918",
            "dd7282d139c163df0243f7cb508c7be979aa9fda",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "4a0c851fbffcfdd4d191cff4b38ac42e1d4d6fc9",
            "95bb0ee471480da79e41ae196bb4da02abe52a27",
            "d592e7ed372adffd2d2d8f5c72565d71e911237e",
            "ca30be76395ba443b24d20d7df5d3b5372df55ff",
            "0fc4a247cce443bc2a3d64818d8981ab8586bb32",
            "8133e66c9c03095ef605090e6a72b752dc774d92"
        ],
        "related_topics": [
            "Classification",
            "Region Of Interest",
            "Neural Network",
            "Radiologist",
            "Cylindrical Algebraic Decomposition",
            "Area Under The ROC Curve"
        ],
        "reference_count": "42",
        "citation_count": "10"
    },
    {
        "Id": "301fee6989cc75f84343836cab5a25691d58dc5c",
        "title": "Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature",
        "authors": [
            "Tariq Sadad",
            "Asim Munir",
            "Tanzila Saba",
            "Ayyaz Hussain"
        ],
        "date": "1 November 2018",
        "abstract": "Semantic Scholar extracted view of \"Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature\" by Tariq Sadad et al.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "d5abfbdef48c20ed124833024e61e4773e944966",
            "82917b65ec0e7fa1bbdc0ba08642fd8c336196a1",
            "d10fa706f2383510f8ecbf69a6c404fc4a5837f8",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "2537fbb22837de4c9f2b167d7905b2810d84e28f",
            "00aefc484dd5684ea999ba310dc152ecbfc7963e",
            "9c204bc24341157f1dbd72fde781a4906d8f17cd",
            "c82fff512c57f12059632bfc02912b494144d984",
            "4fc76c0c9c547cae53da984918cf4893dbca38a4"
        ],
        "related_topics": [
            "Local Phase Quantization",
            "Tumors",
            "Mammographic Image Analysis Society",
            "Mammograms",
            "Classification",
            "Classifier",
            "Radiologist",
            "Decision Trees",
            "DDSM Dataset",
            "Classification Accuracy"
        ],
        "reference_count": "53",
        "citation_count": "83"
    },
    {
        "Id": "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
        "title": "Segmentation and classification of breast cancer using novel deep learning architecture",
        "authors": [
            "Shashank Ramesh",
            "Sasikala Sasikala",
            "S. Gomathi",
            "V. Geetha",
            "V. Anbumani"
        ],
        "date": "25 May 2022",
        "abstract": "A novel deep-learning architecture for tumor segmentation is proposed in this study, and machine learning algorithms are used to categorize benign or malignant tumors. Breast cancer is one of the most frequent cancers in women, and it has a higher mortality rate than other cancers. As a result, early detection is critical. In computer-assisted disease diagnosis, accurate segmentation of the region of interest is a vital concept. The segmentation techniques have been widely used by doctors and physicians to locate the pathology, identify the abnormality, compute the tissue volume, analyze the anatomical structures, and provide treatment. Cancer diagnostic efficiency is based on two aspects: The precision value associated with the segmentation and calculation of the tumor area and the accuracy of the features extracted from the images to categorize the benign or malignant tumors. A novel deep-learning architecture for tumor segmentation is therefore proposed in this study, and machine learning algorithms are used to categorize benign or malignant tumors. The segmentation results improve the decision-making capability of the physicians to identify whether a tumor is malignant or not and normally, the machine learning techniques need expert annotation and pathology reports to identify this. This challenge is overcome in this work with the help of the GoogLeNet architecture used for segmentation. The segmentation results are then offered to the Support Vector Mchine, Decision Tree, Random Forest, and Na\u00efve Bayes classifier to improve their efficiency. Our work has provided better results in terms of accuracy, Jaccard and dice coefficient, sensitivity, and specificity compared to conventional architectures. The proposed model offers an accuracy score of 99.12% which is relatively higher than the other techniques. A 3.78% accuracy improvement is noticed by the proposed model against the AlexNet classifier and the actual increase is 4.61% on average when compared to the existing techniques.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "95cdb70035d13910f2bb7d76ad29d67f619e01d2",
            "18aed16d53bfd0570d40d0be0f3b35338d1c9ead",
            "f7cbb1b2a56d4c74ce4920e353619103098a886e",
            "7c66be06a11210b10afe5dddde51f7c355b98b14",
            "160befefc99d88390d40910b8cd77a5b4d4e310a",
            "ea98b9481ec6943b799a89f0647becafa303066f",
            "b845188382aa1eaaecea4879f41e78f6e0ef03df",
            "542f7bd9c6853d69561e090a8ff82829bf1691f7",
            "3d119f56d43d3d14f4e790da3936b17bac96bab0"
        ],
        "related_topics": [
            "Random Forests",
            "Deep Learning",
            "Tumors",
            "Die Coefficient",
            "Decision Trees",
            "Architecture",
            "Classification"
        ],
        "reference_count": "68",
        "citation_count": "17"
    },
    {
        "Id": "156733fbf757d4e8a333537518c8961163c4fbf7",
        "title": "Classification of Mammograms Using Convolutional Neural Network Based Feature Extraction",
        "authors": [
            "Taye Girma Debelee",
            "Mohammadreza Amirian",
            "Achim Ibenthal",
            "G{\\&quot;u}nther Palm",
            "Friedhelm Schwenker"
        ],
        "date": "25 September 2017",
        "abstract": "The convolutional Neural Networks (CNN) based feature extraction method is proposed and the features dimensionality was reduced using Principal Component Analysis (PCA) and reduced features are given to the K-Nearest Neighbors (KNN) to classify mammograms as normal or abnormal using 10-fold cross-validation. Breast cancer is the most common cause of death among women in the entire world and the second cause of death after lung cancer. The use of automatic breast cancer detection and classification might possibly enhance the survival rate of the patients through starting early treatment. In this paper, the convolutional Neural Networks (CNN) based feature extraction method is proposed. The features dimensionality was reduced using Principal Component Analysis (PCA). The reduced features are given to the K-Nearest Neighbors (KNN) to classify mammograms as normal or abnormal using 10-fold cross-validation. The experimental result of the proposed approach performed on Mammography Image Analysis Society (MIAS) and Digital Database for Screening Mammography (DDSM) datasets were found to be promising compared to previous studies in the area of image processing, artificial intelligence and CNN with an accuracy of 98.75\\(\\%\\) and 98.90\\(\\%\\) on MIAS and DDSM dataset respectively.",
        "references": [
            "8271f755d7cea799600e25662dd3f2ac8d23aeb1",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "95bb0ee471480da79e41ae196bb4da02abe52a27",
            "d38eeb88a928c1a1c6e1fdd18d504669b112ed8b",
            "2c5db1c44dddf0d8e6ea667f3f7afb13032ea386",
            "4592b296b02a97ce7c49e7ef53bfa5e809bb548e",
            "eb8f6f8c48e6b61ea35e7294f26f7cfbbc3dd833",
            "a4360f362168a6107e1014b7fc61bed32038fc70",
            "0003a45ae653fcccb568b90dbb339cf5811a18bf",
            "2415fc06de82ab41ad8b9615162247afb02974af"
        ],
        "related_topics": [
            "Convolutional Neural Network",
            "Digital Database For Screening Mammography",
            "Mammography Image Analysis Society",
            "Classification",
            "Mammograms",
            "Mammographic Image Analysis Society",
            "Artificial Intelligence",
            "K-nearest Neighbors",
            "Principal Component Analysis"
        ],
        "reference_count": "27",
        "citation_count": "27"
    },
    {
        "Id": "6e69f7ea9f63b651ebd676d51d6cca1a483cb2ec",
        "title": "Machine learning based computer aided diagnosis system for classification of breast masses in mammograms",
        "authors": [
            "Harmandeep Singh",
            "Vipul Sharma",
            "Damanpreet Singh"
        ],
        "date": "1 May 2022",
        "abstract": "The experimental results reveal that the proposed improved seeded region growing approach has been proven helpful in improving the classification performance of the proposed CAD system. Breast cancer continues to be the most common cancer in the fastest developing and the developed nations. Early detection by using mammography has been proven as the best prognosis. Computer Aided Diagnosis (CAD) systems are being used as second reader for the analysis and interpretation of mammogram images. In the last two decades, although breast cancer incidence has increased by many folds but unfortunately the progress in this field has almost stagnated. Therefore, the CAD systems need to be improved to be considered useful. In this study, a machine learning based CAD system for segmentation and classification of breast masses have been proposed. The IRMA Version of DDSM dataset has been used for experimentation and evaluation of the proposed system. Exact breast masses were segmented from manually extracted ROIs of 700*700 pixels by employing an improved seeded region growing algorithm. Various geometry and texture features were computed from the segmented mass lesions and corresponding ROIs respectively. The classification performances of nine state-of-the-art classifiers namely K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Gaussian Mixture Model (GMM), Multi-class Support Vector Machine (mSVM), Decision Tree (DT), Discriminate Analysis (DA), Naive Bayes (NB), Random Forest (RF), Ensemble Tree (ET) have been investigated in this study. On evaluating the experimental results for all the classifiers, highest classification accuracy is obtained with SVM classifier. The experimental results reveal that the proposed improved seeded region growing approach has been proven helpful in improving the classification performance of the proposed system.",
        "references": [
            "c0cdec8e8149ab2e7d185893e3f18eb0089a0f2f",
            "76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "662d927da3c17bfde61da5dbc24c037dce30ce25",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "9bf32a68edfea8b8c072bcd3ee0d696687bab403",
            "da4237818523ea3e3d44dbe18c261afc6d6d404f",
            "f0a4d96b5d7f6668d607512f8744e07dbe1c9b27",
            "db480b3f22244151cfeaa4beedc406a5137d1a4e",
            "037587b28682886bb02d87a150028ed931f967a4",
            "0ffefa7d286b24f7c20bedf779cd7bc7d1f64ceb"
        ],
        "related_topics": [],
        "reference_count": "25",
        "citation_count": "One"
    },
    {
        "Id": "4d7ef4f116a535750529e2853c181d5d3b678646",
        "title": "A Hierarchical Classification Method for Breast Tumor Detection",
        "authors": [
            "Mojtaba Mohammadpoor",
            "Afshin Shoeibi",
            "Hoda Zare",
            "H. Shojaee"
        ],
        "date": "1 December 2016",
        "abstract": "A hierarchical classification method for breast cancer detection is developed by including two Adaptive Boosting classifiers, the first classifier is devoted to separate normal and tumorous cases and the second layer is designed to detect tumor type. Introduction Breast cancer is the second cause of mortality among women. Early detection of it can enhance the chance of survival. Screening systems such as mammography cannot perfectly differentiate between patients and healthy individuals. Computer-aided diagnosis can help physicians make a more accurate diagnosis. Materials and Methods Regarding the importance of separating normal and abnormal cases in screening systems, a hierarchical classification system is defined in this paper. The proposed system is including two Adaptive Boosting (AdaBoost) classifiers, the first classifier separates the candidate images into two groups of normal and abnormal. The second classifier is applied on the abnormal group of the previous stage and divides them into benign and malignant categories. The proposed algorithm is evaluated by applying it on publicly available\u00a0 Mammographic Image Analysis Society (MIAS) dataset. 288 images of the database are used, including 208\u00a0 normal and 80 abnormal images. 47 images of the abnormal images showed benign lesion and 33 of them had malignant lesion.\u00a0 Results Applying the proposed algorithm on MIAS database indicates its advantage compared to previous methods. A major improvement occurred in the first classification stage. Specificity, sensitivity, and accuracy of the first classifier are obtained as 100%, 95.83%, and 97.91%, respectively. These values are calculated as 75% in the second stage\u00a0\u00a0 Conclusion A hierarchical classification method for breast cancer detection is developed in this paper. Regarding the importance of separating normal and abnormal cases in screening systems, the first classifier is devoted to separate normal and tumorous cases. Experimental results on available database shown that the performance of this step is adequately high (100% specificity). The second layer is designed to detect tumor type.\u00a0 The accuracy in the second layer is obtained 75%.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "ba6b8c2852be77702ff1765cefde34004da9d4b5",
            "74b5ce5b484a5bd714cab5dd4424a26cbdabbb35",
            "22d4b480e03e7d068356860d8c4485b41a0acfeb",
            "643afbfd187a020cb19bad103176e055821e1bb9",
            "f15a6caf0d6062fc6e100c3ed64e2634e5f9949f",
            "d2c0761e849f656a8933f2c75b8ff8a10713684f",
            "442a7613e992da09758482b035204ac2184ec7da",
            "32b9384ec5653a9910a7eb4c55d064ee13769a5e"
        ],
        "related_topics": [],
        "reference_count": "14",
        "citation_count": "22"
    },
    {
        "Id": "a7f3cd1885a0f1548596b288eb6768d818b8463d",
        "title": "Breast Cancer Detection and Classification from Mammogram Images Using Multi-model Shape Features",
        "authors": [
            "V. R. Gurudas",
            "S. G. Shaila",
            "A. Vadivel"
        ],
        "date": "28 July 2022",
        "abstract": "This paper proposes a framework that automatically classifies the benign and malignant tumors in mammogram images using the support vector machine (SVM) and artificial neural network (ANN) classifiers, which have achieved good results and are promising. Nowadays, breast cancer has become one of the common diseases and is leading in causes of deaths in women. Early detection of breast cancer is very much needed and critical, and mammography is considered as one of the best-suited procedures. The masses are classified as benign or malignant tumors. The size and shape of the masses are characterized by its shapes as per BI-RADS (Breast Imaging-Reporting and Data System), which can discriminate benign and malignant effectively. In this paper, we propose a framework that automatically classifies the benign and malignant tumors in mammogram images. We have considered INBreast and CBIS-DDSM dataset experiments. The histogram-processing multi-level Otsu thresholding on the extracted Region of Interest (ROI) is applied as pre-processing steps for segmenting it. Eighteen features are extracted from the ROI and characterized structure, shape, size, and boundaries of mass present in images belong to both the datasets. The features extracted from the datasets are cross-validated for training and testing using stratified cross-validation techniques. The support vector machine (SVM) and artificial neural network (ANN) classifiers are trained and validated for benign and malignant tumor classification. The experimental results have achieved good results and are promising.",
        "references": [
            "64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
            "04f3e376672318a7c8bfe12059069d033c688951",
            "82b09f56d68eefdb3a3442cfe29433cd72480d64",
            "870d52e025216445bbc51434527f450c7be630fd",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "c4f73e387ad859a54de1d2f9938b8c7dbd1c2a97",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "34b5e888f948e384f434ba3fd1b74e26ba104d5a",
            "f757d4cf2a1a885810bd0c97f9735612750f329c",
            "7ff70f493df1a7d5dfc0c437e7fce6a5172be933"
        ],
        "related_topics": [
            "Support Vector Machines",
            "Classifier",
            "INbreast",
            "Classification",
            "CBIS-DDSM Dataset",
            "Approximate Nearest Neighbor",
            "Region Of Interest",
            "BI-RADS"
        ],
        "reference_count": "42",
        "citation_count": "One"
    },
    {
        "Id": "a1929ada4fee2a87a05a491358d0cda08e8d3fa7",
        "title": "Automatic Soccer Video Key Event Detection and Summarization Based on Hybrid Approach",
        "authors": [
            "Nudrat Nida",
            "Muhammad Khurram Iqbal",
            "Aun Irtaza"
        ],
        "date": "",
        "abstract": "In this study, an automatic method for key-events detection and summarization is presented for soccer videos and the effectiveness and robustness of the proposed method are tested over an extensively huge dataset. : Sports broadcasters generate an enormous amount of video content in cyberspace due to massive viewership all over the world. Analysis and consumption of this huge repository urge the broadcasters to apply video summarization to extract the exciting segments from the entire video to capture the user's interest and reap the storage and transmission benefits. Therefore, in this study, an automatic method for key-events detection and summarization is presented for soccer videos. The proposed framework reaps benefit both from learning and non-learning methods of summarization. SVM classifier is used to classify boundary and non-boundary frames based on extracted features Histogram difference and the average motion vector. Replay detection shot view classification, and play break sequence formation are performed through efficient algorithms. Nonsubjective features such as play break duration ratio, play duration ratio, near goal duration ratio, etc. are used to perform statistical analysis which helps in devising rules for summarization. To address the shortcomings associated with key event detection and summarization algorithms and to get the best out of the merger of learning and non-learning based approaches for summarization, this research problem needed a deep inside look. The effectiveness and robustness of the proposed method are tested over an extensively huge dataset and the results are highly productive and comparative. Besides soccer, the proposed method finds its application in freestyle football and hockey but with minor modifications.",
        "references": [
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "ff739dd45e2b11df8163d450f09020515ae164f9",
            "a1a120c6ea23e024c202ba6b89dec4e6d34bc787",
            "f5917f8507fd498ee55cc4e7af5c6583a745e229",
            "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e",
            "0258f2cfe8a788078e269dc2fe9d23592d5fffc1",
            "2f269fddf637b71e9ee074e13a1c95f709fdf29a",
            "64b52391bc6d58590f3cd97c3f2943684929760a",
            "b8055df39b2d210f8b373ead38d2027bbec8dfb4"
        ],
        "related_topics": [],
        "reference_count": "25",
        "citation_count": "One"
    },
    {
        "Id": "662df45afea0d80922a86a4af431eb80ede87da2",
        "title": "Bayesian System And Copula For Event Detection And Summarization Of Soccer Videos",
        "authors": [
            "Dhanuja S Patil"
        ],
        "date": "2015",
        "abstract": "An efficient structure for analyzing and summarization of soccer videos utilizing object-based features using the t-cherry junction tree to create a compact representation and great approximation intractable model for client\u2019s relationships in an interpersonal organization is proposed. Event detection is a standout amongst the most key parts for distinctive sorts of area applications of video data framework. Recently, it has picked up an extensive interest of experts and in scholastics from different zones. While detecting video event has been the subject of broad study efforts recently, impressively less existing methodology has considered multimodel data and issues related efficiency. Start of soccer matches different doubtful circumstances rise that can't be effectively judged by the referee committee. A framework that checks objectively image arrangements would prevent not right interpretations because of some errors, or high velocity of the events. Bayesian networks give a structure for dealing with this vulnerability using an essential graphical structure likewise the probability analytics. We propose an efficient structure for analyzing and summarization of soccer videos utilizing object-based features. The proposed work utilizes the t-cherry junction tree, an exceptionally recent advancement in probabilistic graphical models, to create a compact representation and great approximation intractable model for client\u2019s relationships in an interpersonal organization. There are various advantages in this approach firstly; the t-cherry gives best approximation by means of junction tree class. Secondly, to construct a t-cherry junction tree can be to a great extent parallelized; and at last inference can be performed utilizing distributed computation. Examination results demonstrates the effectiveness, adequacy, and the strength of the proposed work which is shown over a far reaching information set, comprising more soccer feature, caught at better places. Keywords\u2014Summarization; Detection;Bayesian network; t-cherry tree.",
        "references": [
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "b5a23310cdc5b492175325ba90af69ecda3dc377",
            "718df880b746a72b02c4ff9f29a8aec06d897d88",
            "d6151de801659937574c3efe13c2d207e9e2f2cd",
            "5a7571db7df03cca52c48f89595c4abefeb51e5c",
            "cc85119fdac7f6e9b0afa5e5a87983f6bca2f1c9",
            "d804cdf4f348a3f4ef782f8818064f0dc17c62d7",
            "c7967ff0c51732110e0e1470975fe0a974fa8a2e",
            "63ddcd8ffd48628df02c7cbe5ede83d35af2f0c6",
            "c51d7cbfb95ee370d1eddb4e0ff03290b8bb479a"
        ],
        "related_topics": [],
        "reference_count": "26",
        "citation_count": "3"
    },
    {
        "Id": "0a89745435b38291f0f2f17173f33ccb5d3a826b",
        "title": "Event detection in soccer videos using shot focus identification",
        "authors": [
            "Wei Zhao",
            "Yao Lu",
            "Haohao Jiang",
            "Wei Huang"
        ],
        "date": "1 November 2015",
        "abstract": "An automatic event detection system fusing low and mid level features for soccer videos by employing an improved approach for Shot Boundary Detection with color and the mean-gradient feature and devise a novel method to locate the spatial position of event occurrence accurately using shot focus identification. This paper presents an automatic event detection system fusing low and mid level features for soccer videos. We first employ an improved approach for Shot Boundary Detection with color and our mean-gradient feature. Then we classify the shots into two view types. We also perform a template-based replay detection for each shot. Play-break sequences are then generated using a rule-based method. We devise a novel method to locate the spatial position of event occurrence accurately using shot focus identification and extract other features from the play-break sequences. These descriptors are finally fed into a multi-kernel SVM classifier. Experiments tested on various matches demonstrate the effectiveness of our method.",
        "references": [
            "99fef30f1afa7e8c485e40aac74f7d8923dc892f",
            "3e4643c9c1e8707ab6e2a1db1fc0e2bab2967538",
            "aa7e4bedb68abc0891e1e4d0fc58873e25c34751",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "1e03756ae8f456d0026308347e24712d9702dd40",
            "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e",
            "a322bf3be4118a4d787bd4f9cb77df48e5b6ce3d",
            "85434dcdf128c2e667d0a05a8799fe205d52e742",
            "fa77b359b9ceb014a6e5a528de24d3d725017b36",
            "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a"
        ],
        "related_topics": [
            "Soccer Videos",
            "View Types",
            "Shot Boundary Detection",
            "Event Detection"
        ],
        "reference_count": "15",
        "citation_count": "5"
    },
    {
        "Id": "828f9424abab6fd3e48df5025bef4ff5d56c4295",
        "title": "A survey on event detection based video summarization for cricket",
        "authors": [
            "Khushali R. Raval",
            "Mahesh M. Goyani"
        ],
        "date": "2 April 2022",
        "abstract": "An overview of the automatic cricket video analysis technique and its applications is given and different approaches for shot boundary detection, shot categorization, and event detection are thoroughly examined. Nowadays, sports video analysis is gaining a lot of traction. Cricket is an exciting team sport to watch. Cricket is becoming more popular, but due to the game\u2019s length and intricacy, computerized cricket video analysis is difficult. This motivates us to conduct surveys that will aid in the advancement of cricket research. We reviewed event detection-based cricket video summarizing strategies in this article. We give an overview of the automatic cricket video analysis technique and its applications. Automatic extraction of important events involves low-level analysis for temporal segmentation of video into shots or play-break segments, according to the literature evaluation. To categories cricket video images and extract semantically meaningful events, domain knowledge must be included. Different approaches for shot boundary detection, shot categorization, and event detection are thoroughly examined. Finally, future directions in cricket video analysis are discussed.",
        "references": [
            "43ef35c36e61e7db6b0ef35ef28314bf8fc29827",
            "8177d4a1f289979d6dcca3536b739925a9552b79",
            "fa0fd2f20c36d5668d3044da903d1549a04a8550",
            "9aeb307f1095e7109ecfb51b5489b36f82138d82",
            "fa1594ecf4bd4b6a76d6e4670ea11593171e9a4d",
            "132369a5df9eb8ee57867dd116792a5ad6c3b180",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "084e58343d5783bfb6f0ede987ecb5b3701ff21c",
            "005bc35240c1a306ac897ab42b611675ab565582",
            "b66b45ba5d12490f7101cc314d7c6c066455fe72"
        ],
        "related_topics": [
            "Cricket",
            "Event Detection",
            "Shot Boundary Detection",
            "Sport Video Analysis",
            "Video Summarization"
        ],
        "reference_count": "81",
        "citation_count": "6"
    },
    {
        "Id": "5d8aff02861984abde3ba17033ca3f07e4138b46",
        "title": "Replay and key-events detection for sports video summarization using confined elliptical local ternary patterns and extreme learning machine",
        "authors": [
            "Ali Javed",
            "Aun Irtaza",
            "Yasmeen Khaliq",
            "Hafiz Malik",
            "Muhammad Tariq Mahmood"
        ],
        "date": "12 February 2019",
        "abstract": "A novel framework to summarize sports videos is presented and experimental results indicate the effectiveness of the proposed framework in terms of replays and key-events detection from selected dataset. Sports broadcasters generate enormous amount of video content viewed all over the world. To capture the user interests in the rebroadcasted content, the sports videos are summarized that need the manual inspection and analysis. However, the huge repository and long duration of videos make manual analysis and summarization a laborious and time-consuming job. To overcome this problem, efforts have been made for automatic video summarization. In this paper, a novel framework to summarize sports videos is presented. It has been observed that the replays within a sports video represent key-events and these events can be used for video summarization. It has been noted that replays are usually sandwiched between start and stop of gradual-transitions. A thresholding-based approach is used to identify gradual transition effect (i.e. fade-in, fade-out) in sports video. The Gaussian mixture model (GMM) is then applied to key-event candidates to extract silhouettes and generate motion history image (MHI) for each key-event. The MHIs are processed using Confined Elliptical Local Ternary Patterns (CE-LTPs) for feature extraction. Extreme learning machine (ELM) classifier is used to learn the underlying model for events. A trained ELM-based classifier is then used for key-event detection. The output of classifier is then used for key-event labeling, replay detection, and complete game summarization. Performance of the proposed framework is evaluated on a dataset consisting of 20 videos of four different sports. Experimental results indicate the effectiveness of the proposed framework in terms of replays and key-events detection from selected dataset.",
        "references": [
            "c39184b1cd6e7eeee5f451f1f6b7f199a76ce27c",
            "b41fdd4780e91f23652e7bb80046b960fad3caf4",
            "1c4b043e1e458f3cb6bcd5c47edbef6816383674",
            "5727fdb06696f92d406adb81459dd2e56da1dfeb",
            "0258f2cfe8a788078e269dc2fe9d23592d5fffc1",
            "aa7e4bedb68abc0891e1e4d0fc58873e25c34751",
            "2f269fddf637b71e9ee074e13a1c95f709fdf29a",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "f5917f8507fd498ee55cc4e7af5c6583a745e229",
            "87f19a58a7739e7c11e3638b5b32d1f51632284f"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "b66b45ba5d12490f7101cc314d7c6c066455fe72",
        "title": "Replay and key-events detection for sports video summarization using confined elliptical local ternary patterns and extreme learning machine",
        "authors": [
            "Ali Javed",
            "Aun Irtaza",
            "Yasmeen Khaliq",
            "Hafiz Malik",
            "Muhammad Tariq Mahmood"
        ],
        "date": "12 February 2019",
        "abstract": "A novel framework to summarize sports videos is presented and experimental results indicate the effectiveness of the proposed framework in terms of replays and key-events detection from selected dataset. Sports broadcasters generate enormous amount of video content viewed all over the world. To capture the user interests in the rebroadcasted content, the sports videos are summarized that need the manual inspection and analysis. However, the huge repository and long duration of videos make manual analysis and summarization a laborious and time-consuming job. To overcome this problem, efforts have been made for automatic video summarization. In this paper, a novel framework to summarize sports videos is presented. It has been observed that the replays within a sports video represent key-events and these events can be used for video summarization. It has been noted that replays are usually sandwiched between start and stop of gradual-transitions. A thresholding-based approach is used to identify gradual transition effect (i.e. fade-in, fade-out) in sports video. The Gaussian mixture model (GMM) is then applied to key-event candidates to extract silhouettes and generate motion history image (MHI) for each key-event. The MHIs are processed using Confined Elliptical Local Ternary Patterns (CE-LTPs) for feature extraction. Extreme learning machine (ELM) classifier is used to learn the underlying model for events. A trained ELM-based classifier is then used for key-event detection. The output of classifier is then used for key-event labeling, replay detection, and complete game summarization. Performance of the proposed framework is evaluated on a dataset consisting of 20 videos of four different sports. Experimental results indicate the effectiveness of the proposed framework in terms of replays and key-events detection from selected dataset.",
        "references": [
            "c39184b1cd6e7eeee5f451f1f6b7f199a76ce27c",
            "b41fdd4780e91f23652e7bb80046b960fad3caf4",
            "1c4b043e1e458f3cb6bcd5c47edbef6816383674",
            "5727fdb06696f92d406adb81459dd2e56da1dfeb",
            "0258f2cfe8a788078e269dc2fe9d23592d5fffc1",
            "aa7e4bedb68abc0891e1e4d0fc58873e25c34751",
            "2f269fddf637b71e9ee074e13a1c95f709fdf29a",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "f5917f8507fd498ee55cc4e7af5c6583a745e229",
            "87f19a58a7739e7c11e3638b5b32d1f51632284f"
        ],
        "related_topics": [
            "Classifier",
            "Extreme Learning Machine",
            "Sports Videos",
            "Gradual Transitions",
            "Key-events Detection",
            "Replay Detection",
            "Gaussian Mixture Model",
            "Automatic Video Summarization",
            "Silhouettes",
            "Video Summarization"
        ],
        "reference_count": "43",
        "citation_count": "25"
    },
    {
        "Id": "92b1bf710340ecbf5f40005d88f9005d6882325f",
        "title": "Video Summarization: Survey on Event Detection and Summarization in Soccer Videos",
        "authors": [
            "Yasmin S. Khan",
            "Soudamini Pawar"
        ],
        "date": "2015",
        "abstract": "There is a broad perspective for further research in this field of video summarization, as soccer is the world\u2019s most famous game played and watched and it is taken as a case study. In today's world, the rapid development of digital video and editing technology has led to fast growing of video data, creating the need for effective and advanced techniques for analysis and video retrieval, as multimedia repositories have made browsing, delivery of contents (video) and video retrieval very slow. Hence, video summarization proposes various ways for faster browsing among a large amount of data and also for content indexing. Many people spend their free time to watch or play different sports like soccer, cricket, etc. but it is not possible to watch each and every game due to the longer timing of the game. In such cases, the users may just want to view the summary of the video that is just an abstract of the original video, instead of watching the whole video that provides more information about the occurrence of various incidents in the video. It is preferable to watch just highlights of the game or just review/trailer of a movie. Apparently, summarizing a video is an important process. In this paper, video summarization approaches are discussed, that can generate static or dynamic summaries. We present different techniques for each mode in literature. We have discussed some features used for generating video summaries. As soccer is the world\u2019s most famous game played and watched, it is taken as a case study. Research done in this domain is discussed. We conclude that there is a broad perspective for further research in this field.",
        "references": [
            "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "88acfe06f45d0c623ee0a757894b695c03b1c10c",
            "e8463f44930ab9ab85f2ab7c986c3008e02e12d7",
            "2a5b66c1d18382a2be4035f5af03683d6c0bb83c",
            "dc8177ff75f0f9125c63a04d71e06d37480c457e",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "33ee7e142456166bb2ae29c7ab5125202072482b",
            "33ac99240737c9e2c47647c445978207a9697f32",
            "d50f0e87357c6ee909eb6322f892e4641666fdfb",
            "0cca81f3b2928f6d4005e1a8b617960e2fb14d3e"
        ],
        "related_topics": [
            "Video Summarization",
            "Video Retrieval",
            "Soccer Videos",
            "Cricket",
            "Dynamic Summary",
            "Event Detection"
        ],
        "reference_count": "21",
        "citation_count": "18"
    },
    {
        "Id": "c3c70ec1636e9cbb271cdf84e6b6125894c38cfb",
        "title": "A decision tree framework for shot classification of field sports videos",
        "authors": [
            "Ali Javed",
            "Khalid Mahmood Malik",
            "Aun Irtaza",
            "Hafiz Malik"
        ],
        "date": "16 January 2020",
        "abstract": "An effective decision tree architecture for shot classification of field sports videos to address the aforementioned issues is proposed and achieves an average improvement of 6.9% in precision and 9.1% in recall as compared to contemporary methods under above-mentioned limitations. Automated approaches to analyze sports video content have been heavily explored in the last few decades to develop more informative and effective solutions for replay detection, shot classification, key-events detection, and summarization. Shot transition detection and classification are commonly applied to perform temporal segmentation for video content analysis. Accurate shot classification is an indispensable requirement to precisely detect the key-events and generate more informative summaries of the sports videos. The current state-of-the-art have several limitations, i.e., use of inflexible game-specific rule-based approaches, high computational cost, dependency on editing effects, game structure, and camera variations, etc. In this paper, we propose an effective decision tree architecture for shot classification of field sports videos to address the aforementioned issues. For this purpose, we employ the combination of low-, mid-, and high-level features to develop an interpretable and computationally efficient decision tree framework for shot classification. Rule-based induction is applied to create various rules using the decision tree to classify the video shots into long, medium, close-up, and out-of-field shots. One of the significant contributions of the proposed work is to find the most reliable rules that are least unpredictable for shot classification. The proposed shot classification method is robust to variations in camera, illumination conditions, game structure, video length, sports genre, broadcasters, etc. Performance of our method is evaluated on YouTube dataset of three different genre of sports that is diverse in terms of length, quantity, broadcasters, camera variations, editing effects and illumination conditions. The proposed method provides superior shot classification performance and achieves an average improvement of 6.9% in precision and 9.1% in recall as compared to contemporary methods under above-mentioned limitations.",
        "references": [
            "b66b45ba5d12490f7101cc314d7c6c066455fe72",
            "2f1f9717c3257fb2a264548ee69130c43e61795b",
            "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "31e12ef5fb856bc1ede9afa8750fed4141efac37",
            "b41fdd4780e91f23652e7bb80046b960fad3caf4",
            "5b54b9de58ee8a38489b8bd48843e2da13afa92c",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "2f5bedd7f5a9381c4a59ec73132ca405bf3bb4b9",
            "0b1e432fd1852d797417b4cbd317c44ce01f5a66",
            "084e58343d5783bfb6f0ede987ecb5b3701ff21c"
        ],
        "related_topics": [
            "Shot Classification",
            "Field Sports Video",
            "Replay Detection",
            "Key-events Detection",
            "Computational Cost",
            "Summarization",
            "Decision Trees"
        ],
        "reference_count": "38",
        "citation_count": "14"
    },
    {
        "Id": "0b1e432fd1852d797417b4cbd317c44ce01f5a66",
        "title": "Soccer Video Structure Analysis by Parallel Feature Fusion Network and Hidden-to-Observable Transferring Markov Model",
        "authors": [
            "Mehrnaz Fani",
            "Mehran Yazdi",
            "David A Clausi",
            "Alexander Wong"
        ],
        "date": "2 November 2017",
        "abstract": "The H2O-MM has the accuracy of 98.7% for play\u2013break segmentation, which is an improvement over two existing hidden Markov models and can be used to further improve overall automated event detection. Automated analysis of broadcast soccer game video is a challenging computer vision problem. Prior to performing high-level analysis (such as event detection), accurate classification of shot views and play\u2013break segmentation are required to analyze the structure of soccer video. A novel deep network called parallel feature fusion network (PFF-Net) combines local and full-scene features to produce accurate shot view classification based on camera zoom and out-of-field status. Then, a novel hidden-to-observable Markov model (H2O-MM) is introduced to determine play/break status of the shots. Testing is performed using a variety of professional broadcast soccer videos. Variations of the PFF-Net are considered and compared with four existing methods where the PFF-Net demonstrates superior performance (92.6%). The H2O-MM has the accuracy of 98.7% for play\u2013break segmentation, which is an improvement over two existing hidden Markov models. The new methods provide improved temporal labeling of broadcast soccer videos, which can be used to further improve overall automated event detection.",
        "references": [
            "d16db59f20593fb90e0bc467afddfdb7d71d8dce",
            "d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "d71c84fbaff0d7f2cbcf2f03630e189c58939a4a",
            "f7b4fcc9dbb97997dc1793d7a366cba274f53134",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "d8b96b08758238de7b901737c824c4694186ae9e",
            "c88e9238b8e564b6554667130996a7c6fa8b2564",
            "cf90380f93f2b67128e0c8e2816481f601c01845",
            "96d642aac3a930cc1b37100130a9f1c0ad8c9a01",
            "2e991717e4e0b6fafb19f3e048886fce9d2058c1"
        ],
        "related_topics": [
            "Classification",
            "Deep Network",
            "Broadcast Soccer Video",
            "Hidden Markov Models",
            "Computer Vision",
            "Event Detection"
        ],
        "reference_count": "37",
        "citation_count": "14"
    },
    {
        "Id": "cc810ccea173885c7630e159409be23a2fdd028c",
        "title": "Structural Approach for Event Resolution in Cricket Videos",
        "authors": [
            "S. C. Premaratne",
            "K. L. Jayaratne"
        ],
        "date": "27 December 2017",
        "abstract": "This research focuses on how to effectively extract and classify data from a multimedia data related to sports videos and draw conclusions considering all media present in the content. Classification of multimedia big data today has become a serious issue for organizations. Therefore, new concepts to mine of multimedia big have emerged and people are doing numerous researches on how to effectively handle different types of multimedia data also known as multi model data. In our research, we focus on how to effectively extract and classify data from a multimedia data related to sports videos and draw conclusions considering all media present in the content. We specifically considered the game of cricket in this research to build a multi-model mining approach to identify specific events. We consider low level details such as color variations in videos and pitch in audio to analyze and distinguish different attributes of a given event. The input can be any cricket video and our approach is to identify events such as a four, a six, a dot ball etc. by extracting low level details of its color variations, edges, camera changes and audio frequency variations. This research for video/audio data extraction provides room for addition of further audio data extraction and textual data extraction for classification of a multimodal data set.",
        "references": [
            "8177d4a1f289979d6dcca3536b739925a9552b79",
            "7770f803c514ae706980d505a7b7cb39d914c2ce",
            "e52d518aeb1160a80f4f4a402dbcfe4f7f26d3f4",
            "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "99963f66cc2716ba5d42fab0454fff3dbf6bfef9",
            "85155400c474d70cf781a4ab1f8e16684846d7de",
            "099b13b46bfa87f105680444eb4dc158fe3b6d52",
            "31ee214486b6cff48c98a9ca39a22c23ac69b897",
            "24ef175324d153b5d98420213c5b200ea7a3e0b0",
            "e77f4abb48bde5e48a2a80e69b5b710f0c90b2ee"
        ],
        "related_topics": [
            "Cricket",
            "Classification",
            "Cricket Videos",
            "Multi-Model Data",
            "Sports Videos"
        ],
        "reference_count": "12",
        "citation_count": "6"
    },
    {
        "Id": "24dccf31b0347d138cdca66d71d9cb264e7498b1",
        "title": "Color transfer based on multiscale gradient-aware decomposition and color distribution mapping",
        "authors": [
            "Zhuohan Su",
            "Daiguo Deng",
            "Xue Yang",
            "Xiaonan Luo"
        ],
        "date": "29 October 2012",
        "abstract": "A novel color transfer method, which is based on the gradient-aware decomposition and the color distribution mapping, is proposed, which can achieve a visual satisfied result without post-processing gradient correction. Automatic global color transfer is a challenging problem in image editing. In this paper, we propose a novel color transfer method, which is based on the gradient-aware decomposition and the color distribution mapping. Firstly, a gradient-aware decomposition model is established to separate the target image into the base and detail layers. Then, the colors of each separated base layer are enforced to match those of a given reference image by Pitie's multi-dimensional probability density function transfer method. After that, all mapped base layers are combined with corresponding boosted detail layers to produce the final output. The experiments demonstrate that our method can achieve a visual satisfied result without post-processing gradient correction.",
        "references": [
            "93b26a24b0b6cc4a92232874f04c75bc8f44c84b",
            "7fd89d13508e4bba173eaf08f71ba2fa86e3e5ca",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "7c2517f714a9015cf1673f9f7d2e024cb2be7230",
            "85683b702e59eddacef2d3fd82c9deab0b26ba8f",
            "6fe1789ca63598aec095df3c57c9607ac3b46dc7",
            "fe2dcdb4c39fd519c7a44bb29a7655237475f91f",
            "787874fedd7384be5b04530bf9334cb58e0c1bd1",
            "000b9a90bbea62e4222704c616c0c2ee65609aa7",
            "d68141f4b1cc0e5efa0e8439039704b7decb9841"
        ],
        "related_topics": [
            "Color Transfer",
            "Base Layer",
            "Detail Layer",
            "Image Editing",
            "Reference Image"
        ],
        "reference_count": "17",
        "citation_count": "7"
    },
    {
        "Id": "be6a3fb065281fe3f27bd99d61ef28e41635e575",
        "title": "PRINCIPAL COLOR AND ITS APPLICATION TO COLOR IMAGE SEGMENTATION",
        "authors": [
            "Arash Abadpour",
            "Shohreh Kasaei"
        ],
        "date": "1 April 2008",
        "abstract": "This paper proposes a new color image segmentation method that uses the linear representation of homogeneous blocks of the image, rather than using the noisy individual pixels, which may contain many outliers. Color image segmentation is a primitive operation in many image processing and computer \nvision applications. Accordingly, there exist numerous segmentation approaches in the literature, \nwhich might be misleading for a researcher who is looking for a practical algorithm. While many \nresearchers are still using the tools which belong to the old color space paradigm, there is evidence \nin the research established in the eighties that a proper descriptor of color vectors should act \nlocally in the color domain. In this paper, these results are used to propose a new color image \nsegmentation method. The proposed method searches for the principal colors, de ned as the \nintersections of the cylindrical representations of homogeneous blocks of the given image. As \nsuch, rather than using the noisy individual pixels, which may contain many outliers, the proposed \nmethod uses the linear representation of homogeneous blocks of the image. The paper includes \ncomprehensive mathematical discussion of the proposed method and experimental results to \nshow the e\u000eciency of the proposed algorithm.",
        "references": [
            "2139c25553efc916c57fa98f9204105b56ece75d",
            "a2882b8b0c9635d39d15a28138e3f47907f3177b",
            "0c2ced886708cc3aea4705f8765d152cd3f69cd2",
            "05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1",
            "fa4dce7d484da0d91d872261e0c41006521e732f",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "1e48105dd2b6d4a21be627040fa6e2074a576bef",
            "d7c0b4473110541613c91fed2bc11c0def55b8f1",
            "1186c8e998b2a1c3dd87e55400929d753877bd19",
            "a34c9af1897c779be9aee293ac43e1dca097a33c"
        ],
        "related_topics": [
            "Color Image Segmentation",
            "Descriptor",
            "Computer Vision"
        ],
        "reference_count": "21",
        "citation_count": "6"
    },
    {
        "Id": "44175dc6641b3a389a81bd457d4d4deb7d5d3e08",
        "title": "A Survey on Color Transfer Methods",
        "authors": [
            "Evline J Alappatt",
            "Vince Paul"
        ],
        "date": "2016",
        "abstract": "This report presents a comprehensive overview of these methods for recoloring images by deriving a mapping between that image and another image serving as a reference and offers a classification of current solutions. Color mapping or color transfer methods aim to recolor a given image or video by deriving a mapping between that image and another image serving as a reference. This class of methods has received considerable attention in recent years, both in academic literature and in industrial applications. Methods for recoloring images have often appeared under the labels of color correction, color transfer or color balancing, to name a few, but their goal is always the same: mapping the colors of one image to another. In this report, we present a comprehensive overview of these methods and offer a classification of current solutions. KeywordsColortransfer, edgepreserving smoothing, image manipulation.",
        "references": [
            "93b26a24b0b6cc4a92232874f04c75bc8f44c84b",
            "54e8f504c3fc6b8e8e27c9a9cd7285698272c81d",
            "40d1e1f71c50d20690784be9ebdeff02552adb1a",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "aa7df77672b87debed9683e6c831d83bc757e853",
            "85683b702e59eddacef2d3fd82c9deab0b26ba8f",
            "7d88404adf4cd7be5c80f540b0ffd957e24bea6a",
            "61d0af20ad1d80a453f87dba8d1f791fbff8c10c",
            "fe2dcdb4c39fd519c7a44bb29a7655237475f91f",
            "e0bd9ee4f46589a7c731a4e6bbbecda3f6d21479"
        ],
        "related_topics": [],
        "reference_count": "24",
        "citation_count": "One"
    },
    {
        "Id": "f2ec186b8532ca09442886f30f33e5eb5e20b5dc",
        "title": "A Computationally Efficient Technique for Image Colorization",
        "authors": [
            "Adrian Pipirigeanu",
            "Vladimir Botchko",
            "Jussi Parkkinen"
        ],
        "date": "29 July 2009",
        "abstract": "The method maps the gray level image into the color space by means of parametrical mapping learnt using PCA and principal components regression, and the experiments show the method's feasibility for colorizing the objects, and textures, as well. In this paper, the fast technique for image colorization is considered. The proposed method transfers colors from the color image (source) to the gray level image (target). For the source image, we use the segmented uniformly colored regions (dielectric surfaces) under single color illumination. This method maps the gray level image into the color space by means of parametrical mapping learnt using PCA and principal components regression. The experiments show the method's feasibility for colorizing the objects, and textures, as well.",
        "references": [
            "81010b5f30917ca280a26a54ecf1d4e5cd477b12",
            "f7431ca78a814a2ad405c945e06f8a26937b0cdb",
            "7d22396f94fa2ba4306125c9d2e2ba5ed75d2d3f",
            "ed7e166f65bcecc522c6c4bbb29fcf8048010873",
            "d5c6edb53dc41f298f145041cd2c53e40e3acf2b",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "1e48105dd2b6d4a21be627040fa6e2074a576bef",
            "bad97714e7b7ca4f8e49dd1fefc05e7d9bea6658",
            "2e997a72e4a4001d325edddfec1ff14d281944fa",
            "88b5dbc6da9261b3365e08a47ab0abeeac3dd80b"
        ],
        "related_topics": [
            "Image Colorization",
            "Principal Component Analysis"
        ],
        "reference_count": "0",
        "citation_count": "12"
    },
    {
        "Id": "a53b9a81fc16016355975215fd9d130c08c441ab",
        "title": "Dark Image Enhancement Using Perceptual Color Transfer",
        "authors": [
            "Jonathan Cepeda-Negrete",
            "Ra{\\&#x27;u}l Enrique S{\\&#x27;a}nchez-Y{\\&#x27;a}{\\~n}ez",
            "Fernando E. Correa-Tome",
            "Rocio A. Lizarraga-Morales"
        ],
        "date": "2018",
        "abstract": "The results show that the methodology presented in this paper can be a good alternative to low-light or night vision processing techniques, and has a low computational complexity, property that is important for real time applications or for low-resource systems. In this paper, we introduce an image enhancing approach for transforming dark images into lightened scenes, and we evaluate such method in different perceptual color spaces, in order to find the best-suited for this particular task. Specifically, we use a classical color transfer method where we obtain first-order statistics from a target image and transfer them to a dark input, modifying its hue and brightness. Two aspects are particular to this paper, the application of color transfer on dark imagery and in the search for the best color space for the application. In this regard, the tests performed show an accurate transference of colors when using perceptual color spaces, being RLAB the best color space for the procedure. Our results show that the methodology presented in this paper can be a good alternative to low-light or night vision processing techniques. Besides, the proposed method has a low computational complexity, property that is important for real time applications or for low-resource systems. This method can be used as a preprocessing step in order to improve the recognition and interpretation of dark imagery in a wide range of applications.",
        "references": [
            "17b0873bab6b5fe7e3203c22514c2b418524529d",
            "562255ae115492717da2ce486e913c0a3d24d187",
            "5c054c99a59f9db3f06dc43ff8b77140876af7a2",
            "ab9fae8f69ec117d74218365352025bc82400a9f",
            "5f2586556999e9d7ec3b79a54adc0c41208edf9c",
            "3584d31719ab35db9d9cbe5353f069bea1b9ad9d",
            "7e8f44a0298ebfe8020e385a8f3f5d1a239cffdd",
            "d5986548cac9950f6837e71272da94e9178b9618",
            "6327caf546ef6e47b9b05735cc5ec572733cb756",
            "24295f6dfa2f2d1b32faf3548f49e86539af1d12"
        ],
        "related_topics": [
            "Dark Image Enhancement",
            "Target Image",
            "Low-light",
            "Computational Complexity"
        ],
        "reference_count": "60",
        "citation_count": "16"
    },
    {
        "Id": "5546fd60122e97ba8075a918269a4c523cf74c4b",
        "title": "Color Transfer for Images: A Survey",
        "authors": [
            "Chenlei Lv",
            "Dan Zhang",
            "Shenglin Geng",
            "Zhongke Wu",
            "Hui Huang"
        ],
        "date": "30 November 2023",
        "abstract": "This paper investigates the mainstream methods of color transfer to provide a survey which introduces the related theories and frameworks and discusses the unsolved issues ofcolor transfer and potential solutions in future work. High-quality image generation is an important topic in digital visualization. As a sub-topic of the research, color transfer is to produce high-quality image with ideal color scheme learned from the reference one. In this paper, we investigate the mainstream methods of color transfer to provide a survey which introduces the related theories and frameworks. Such methods can be concluded into three categories: statistical color transfer; semantic-based color transfer; color transfer for special target. For these mainstream technical routes, we discuss the related research background, technical details, and representative methods. We also exhibit some new trends of the topic according to the recent progress. Based on the comparisons, we discuss the unsolved issues of color transfer and potential solutions in future work.",
        "references": [
            "1d30b941448645bc2dc304dcb1aacf9261070e0e",
            "5777bf9a31e2ec7ebb5f591d542b0560652f859c",
            "fec702c4fef109faccd98fb3f0502c7258970d80",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "95069a2d3bbfb885580c2ee08b5dd00c8bfa3539",
            "702a6d1168a6601086817c72e6fb0f2bf2f2bbba",
            "6324e35287e670a1e723a68554cad0ebda971789",
            "1ce0ed147b783778dd596e7e663b112fd3340164",
            "10fb269f1121e958631e402a99d006d9e20b35ca",
            "2bf08736dcbe24b3261cef9cfee5b3de15f3c30b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "57"
    },
    {
        "Id": "c4e047007c9ca9c1755698442bc2fe1f211a2922",
        "title": "Color fusion algorithm for visible and infrared images based on color transfer in YUV color space",
        "authors": [
            "Lingxue Wang",
            "Shiming Shi",
            "Weiqi Jin",
            "Yuanmeng Zhao"
        ],
        "date": "15 November 2007",
        "abstract": "Tests show that this algorithm pops out the hot targets in IR image with intense red color while the background details in visible image present natural color similar to a color day-time image. Color fusion algorithm for visible and infrared(IR) images based on color transfer in YUV color space under trees, lawn, or land background is presented. Considering the red color will alert observers to possible interested target or danger, this paper aims at working on an algorithm that emphasizes hot targets in IR image with intense red, and the background details in visible image present natural color similar to a color day-time image. V component of YUV space represents the difference between red and Y. Properly increasing the V value will obtain intense red color. Therefore, a nonlinear transfer method based on local mean value of the IR image is proposed. A window of size 5x5 is used to locate hot target in IR image. When the local gray mean value in this window is larger than the global mean value, we determine that this pixel is in a hot area. Then its V value is increased by the ratio of the local gray mean value to the global mean value. Tests show that this method pops out the hot targets with intense red color while the background rendered natural color appearance.",
        "references": [
            "746d5ec819b74c0d59a8be42fc8823235e1bcd75",
            "787effd0978356f963ee2c69fbb96860d4ca5c1e",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "f6866aaea2ead5c75df22ecc66a0484732aef2be",
            "b097c6544ab28e5f4b3ec42fd0b8a95d4b0f6ad5",
            "a6f488da1048492128fb2b931e4ff2a8eb68e654",
            "9459e29a00dd8697bbc73d77d6aa37410889783a",
            "27ac80fee3d770f3fdf1088a8a07b67ae711ffdc",
            "66dd57213790aac7bcf6d2f3422f2bd93491025c",
            "4866151c742245ddefe5a5a464cc173d4d50c4cd"
        ],
        "related_topics": [],
        "reference_count": "16",
        "citation_count": "17"
    },
    {
        "Id": "b98e227f6fd632c95806828116378d04782bac24",
        "title": "Optimizing color transfer using color similarity measurement",
        "authors": [
            "Wei-Sung Chen",
            "Ming-Long Huang",
            "Chung-Ming Wang"
        ],
        "date": "1 June 2016",
        "abstract": "This paper presents a simple yet effective generalized color transfer algorithm, taking into consideration the influences contributed from both the source and target images, and introduces the Gaussian membership function as the first color similarity measurement. Color transfer algorithms alter the color appearance in the source image by borrowing colors from the target image. In this paper, we present a simple yet effective generalized color transfer algorithm, taking into consideration the influences contributed from both the source and target images. We introduce the Gaussian membership function as our first color similarity measurement. This function aims at balancing the degree of similarity of color distributions between source and resultant images and that between target and resultant images, respectively. With regard to the second color similarity measurement, we combine color histogram with the statistic concept of the correlation. Referring to different weights, the histogram correlation method derives correlations at each color channel between source and resultant images and those between target and resultant images. Color transfer is optimized by automatically stabilizing correlations before producing the final color transfer resultant image. Experimental results demonstrate that our proposed algorithm automatically takes into consideration the source and target images producing a visually plausible result. The statics comparison shows that our scheme outperforms current state-of-the-art methods.",
        "references": [
            "8de9fd36e1fe69466cc76a82b7a432dc2eb10861",
            "10fb269f1121e958631e402a99d006d9e20b35ca",
            "7e8f44a0298ebfe8020e385a8f3f5d1a239cffdd",
            "eb55a884b2bc117480d2c376111ac70d610d83e1",
            "54e8f504c3fc6b8e8e27c9a9cd7285698272c81d",
            "22c5a5c9031f8f1464b9a88eef99e500f744903e",
            "7071b4af499486757cce6f687d158b0f1260ac4e",
            "c0afe85ed91447dee1811f5a825eeb407919a974",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "6f1d7d584b39b423151488832b98880c6c7391cd"
        ],
        "related_topics": [
            "Target Image",
            "Color Transfer",
            "Color Histograms"
        ],
        "reference_count": "33",
        "citation_count": "2"
    },
    {
        "Id": "e9be22d7ec8756c432729b2c21c1bec911fbdc4c",
        "title": "An optimized region-based color transfer method for night vision application",
        "authors": [
            "Tanish Hemalbhai Zaveri",
            "Mukesh A. Zaveri",
            "Ishit Makwana",
            "Harshit Mehta"
        ],
        "date": "1 December 2010",
        "abstract": "The simulation results show that the color fused image obtained by the proposed region-based natural color mapping method resembles the natural color appearance and will help the observer by making it more recognizable appearance for better scene interpretation. Modern night-vision systems like image intensifiers and thermal cameras enable operations at night and in adverse weather conditions. Modern night vision camera provides false-colored fused image as an output which is unnatural in appearance and it is therefore hard to interpret. In this paper, a region-based natural color mapping method for night vision imagery is presented. The proposed method colorizes the night vision imagery by using a combined framework consisting of hill-climbing algorithm for color-based segmentation, non-linear diffusion, region recognition and fuzzy based image fusion techniques. The proposed method is an optimized region-based approach and allows selective color transfer from the natural target color image. The simulation results show that the color fused image obtained by proposed method resembles the natural color appearance and will help the observer by making it more recognizable appearance for better scene interpretation.",
        "references": [
            "2ad3927e656867ecb47c694b3197806b1143faa7",
            "ff85aa39c86b57e045e045537ac32813522375fd",
            "d5986548cac9950f6837e71272da94e9178b9618",
            "6327caf546ef6e47b9b05735cc5ec572733cb756",
            "45816243ca4f26be099e14d09744a7d906e8b5c9",
            "5deb3a34a773e9a620eee09136a163fd4c253ee3",
            "05f62d07ad886cb9b7beea3da28ea807cb67160f",
            "9b80d7498e1197870e72da035c8079a76d3e58a6",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "d5c6edb53dc41f298f145041cd2c53e40e3acf2b"
        ],
        "related_topics": [
            "Fused Image"
        ],
        "reference_count": "20",
        "citation_count": "8"
    },
    {
        "Id": "8de9fd36e1fe69466cc76a82b7a432dc2eb10861",
        "title": "Selective color transferring via ellipsoid color mixture map",
        "authors": [
            "Shiguang Liu",
            "Hanqiu Sun",
            "Xiang Zhang"
        ],
        "date": "2012",
        "abstract": "Semantic Scholar extracted view of \"Selective color transferring via ellipsoid color mixture map\" by Shiguang Liu et al.",
        "references": [
            "1e7312678ea3c3a2da6e9a1a56f16b5ef60cf6d2",
            "eb55a884b2bc117480d2c376111ac70d610d83e1",
            "77584fe37d971b45cccf4ce5762704921199e004",
            "d5c6edb53dc41f298f145041cd2c53e40e3acf2b",
            "ac351c56cd3a8ca8ef412e86bda268cef9bb078b",
            "bad97714e7b7ca4f8e49dd1fefc05e7d9bea6658",
            "6f1d7d584b39b423151488832b98880c6c7391cd",
            "7e8f44a0298ebfe8020e385a8f3f5d1a239cffdd",
            "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "22c5a5c9031f8f1464b9a88eef99e500f744903e"
        ],
        "related_topics": [
            "Color Transfer",
            "Pixel",
            "Image Segmentation",
            "Output Image"
        ],
        "reference_count": "25",
        "citation_count": "25"
    },
    {
        "Id": "3aaf88163e9e502daf5be57917470c30c63da6a6",
        "title": "Adaptive exploitation of pre-trained deep convolutional neural networks for robust visual tracking",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Hossein Ghanei-Yakhdan",
            "Shohreh Kasaei"
        ],
        "date": "29 August 2020",
        "abstract": "Due to the automatic feature extraction procedure via multi-layer nonlinear transformations, the deep learning-based visual trackers have recently achieved a great success in challenging scenarios for visual tracking purposes. Although many of those trackers utilize the feature maps from pre-trained convolutional neural networks (CNNs), the effects of selecting different models and exploiting various combinations of their feature maps are still not compared completely. To the best of our knowledge, all those methods use a fixed number of convolutional feature maps without considering the scene attributes (e.g., occlusion, deformation, and fast motion) that might occur during tracking. As a pre-requisition, this paper proposes adaptive discriminative correlation filters (DCF) based on the methods that can exploit CNN models with different topologies. First, the paper provides a comprehensive analysis of four commonly used CNN models to determine the best feature maps of each model. Second, with the aid of analysis results as attribute dictionaries, an adaptive exploitation of deep features is proposed to improve the accuracy and robustness of visual trackers regarding video characteristics. Third, the generalization of proposed method is validated on various tracking datasets as well as CNN models with similar architectures. Finally, extensive experimental results demonstrate the effectiveness of proposed adaptive method compared with the state-of-the-art visual tracking methods.",
        "references": [
            "bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "1131c53b9baaa740a4deef4c1282821b23d18687",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "511b6263795b8921e9f980b0ac7be5f6282337f6",
            "4b39e8494cf031b2b87c6bd5c65c2a2dfb02c531",
            "5c8a6874011640981e4103d120957802fa28f004",
            "503bafe063e410050c174fcc741e39b3b1e0eb22",
            "f233c16a87d518bfe9f923ea7af48ed3eb6bb7d5",
            "26e2ca763087be09e3799ad294302aa91077942d",
            "ed84a17bd753d1ba9404131cff5186db4da6edd8"
        ],
        "related_topics": [
            "Convolutional Neural Network",
            "Feature Maps",
            "Tracking Dataset",
            "Attribute Dictionary",
            "Generalization",
            "Fast Motion",
            "Visual Tracking",
            "Visual Trackers",
            "Discriminative Correlation Filter",
            "Pre-trained"
        ],
        "reference_count": "69",
        "citation_count": "3"
    },
    {
        "Id": "45bde350082fe5ee366c8f1b761429d2277fbcca",
        "title": "Siamese Attentional Cascade Keypoints Network for Visual Object Tracking",
        "authors": [
            "Ershen Wang",
            "Donglei Wang",
            "Yufeng Huang",
            "Gang Tong",
            "Song Xu",
            "Tao Pang"
        ],
        "date": "2021",
        "abstract": "A novel Siamese Attentional Cascade Keypoints Tracking Network named SiamACN is proposed to exactly track the object by using keypoints prediction instead of anchors to improve the tracking efficiency. Visual object tracking is urgent yet challenging work since it requires the simultaneous and effective classification and estimation of a target. Thus, research on tracking has been attracting a considerable amount of attention despite the limitations of existing trackers owing to deformation, occlusion and motion. For most current tracking methods, researchers have proposed various ways to adopt a multi-scale search or anchors for estimation, but these methods always need prior knowledge and too many hyperparameters. To address these issues, we proposed a novel Siamese Attentional Cascade Keypoints Tracking Network named SiamACN to exactly track the object by using keypoints prediction instead of anchors. Compared to complex target prediction, the anchor-free method is performed to avoid plaguy hyperparameters, and a simplified hourglass network with global attention is considered the backbone to improve the tracking efficiency. Further, our framework uses keypoints prediction around the target with cascade corner pooling to simplify the model. To certificate the superiority of our framework, extensive tests are conducted on five tracking benchmarks, including OTB-2015, VOT-2016, VOT-2018, LaSOT and UAV123. Our method achieves the leading performance with an accuracy of 61.2% on VOT2016 and favorably runs at 32 FPS against other competing algorithms, which confirms its effectiveness in real-time applications.",
        "references": [
            "fce3655dc22a783b1f82f09190410f070c7bf42c",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "93874b197f48562fc410b3c351edb9f26da8c123",
            "1fbb4201af091aef55360f113ba35814063923e4"
        ],
        "related_topics": [
            "Anchors",
            "Visual Object Tracking",
            "Hyperparameter",
            "VOT2016",
            "Keypoint Prediction",
            "OTB-2015",
            "VOT2018",
            "Cascade Corner Pooling",
            "Multi-scale Search",
            "Hourglass Network"
        ],
        "reference_count": "73",
        "citation_count": "4"
    },
    {
        "Id": "93742493c26652d4a95627de699869795698a554",
        "title": "Improving Object Tracking by Added Noise and Channel Attention",
        "authors": [
            "Mustansar Fiaz",
            "A. Mahmood",
            "Ki Yeol Baek",
            "Sehar Shahzad Farooq",
            "Soon Ki Jung"
        ],
        "date": "1 July 2020",
        "abstract": "This paper proposes an Input-Regularized Channel Attentional Siamese (IRCA-Siam) tracker which exhibits improved generalization compared to the current state-of-the-art trackers and proposes feature fusion from noisy and clean input channels which improves the target localization. CNN-based trackers, especially those based on Siamese networks, have recently attracted considerable attention because of their relatively good performance and low computational cost. For many Siamese trackers, learning a generic object model from a large-scale dataset is still a challenging task. In the current study, we introduce input noise as regularization in the training data to improve generalization of the learned model. We propose an Input-Regularized Channel Attentional Siamese (IRCA-Siam) tracker which exhibits improved generalization compared to the current state-of-the-art trackers. In particular, we exploit offline learning by introducing additive noise for input data augmentation to mitigate the overfitting problem. We propose feature fusion from noisy and clean input channels which improves the target localization. Channel attention integrated with our framework helps finding more useful target features resulting in further performance improvement. Our proposed IRCA-Siam enhances the discrimination of the tracker/background and improves fault tolerance and generalization. An extensive experimental evaluation on six benchmark datasets including OTB2013, OTB2015, TC128, UAV123, VOT2016 and VOT2017 demonstrate superior performance of the proposed IRCA-Siam tracker compared to the 30 existing state-of-the-art trackers.",
        "references": [
            "e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "fdb98f5a7015de0956ef8d4e468257dc3079b5e5",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "5cc27ba9ea61c3240eb249fc9b56dd42b7fb86e3",
            "deaaa383bb9291bc77a70a22b70d2683673ba76f",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "e972436110b4f2c102d938311beff98ece7b6da7",
            "09769e80cdf027db32a1fcb695a1aa0937214763"
        ],
        "related_topics": [
            "Generalization",
            "State-of-the-art Trackers",
            "Channel Attention",
            "OTB-2015",
            "Object Tracking",
            "VOT2017",
            "Computational Cost",
            "Trackers",
            "OTB-2013",
            "Discrimination"
        ],
        "reference_count": "78",
        "citation_count": "5"
    },
    {
        "Id": "7231131741ece10fe59ba59361abcf70414055d9",
        "title": "Comparison of four visual tracking algorithms based on deep learning",
        "authors": [
            "Wei Jin",
            "Hai-Bin Sun",
            "Xin-Yu Du",
            "Hao Li",
            "Lin-run Ye",
            "Kun Liu"
        ],
        "date": "8 December 2020",
        "abstract": "To evaluate the visual tracking algorithm proposed by the research team, this work compares the algorithm with other three visual tracking algorithms, and qualitatively compares the robustness of the four algorithms on the five tracking challenging factors. To evaluate the visual tracking algorithm proposed by our research team, we compare the algorithm with other three visual tracking algorithms. Firstly, the four visual tracking algorithms are introduced. There are SiamFC, SiamRPN++, ATOM and TDLD, which are all based on deep learning. The first three algorithms are the state-of-the-art trackers of different periods. The last algorithm is proposed by ourselves. And then we do some experiments in seven video sequences from OTB-100 dataset. We qualitatively compare the robustness of the four algorithms on the five tracking challenging factors. The average centre location error (ACLE) and average overlap score (AOC) of the four algorithms are calculated to make a quantitative analysis. The SiamRPN++ algorithm gets the best result of ACLE three times, and the TDLD gets twice. Both the SiamRPN++ and the TDLD get the best result of AOC three times respectively. The analysis results show that performance of the TDLD is very close to the state-of-the-art trackers.",
        "references": [
            "7b0f3227182add82828a8065936dbc06ff53c21a",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "1fbb4201af091aef55360f113ba35814063923e4",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "900ab48d25b44c076e31224b7befa503d9550c53",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951"
        ],
        "related_topics": [
            "SiamRPN++",
            "State-of-the-art Trackers",
            "Deep Learning",
            "Adaptive Optimal Control",
            "ARM C Language Extensions",
            "SiamFC",
            "Visual Tracking",
            "Accurate Tracking By Overlap Maximization"
        ],
        "reference_count": "0",
        "citation_count": "15"
    },
    {
        "Id": "90e1220930ecf6dc052632310efd1301d6ab1587",
        "title": "Multi-expert visual tracking using hierarchical convolutional feature fusion via contextual information",
        "authors": [
            "Sathishkumar Moorthy",
            "Young Hoon Joo"
        ],
        "date": "6 February 2021",
        "abstract": "Semantic Scholar extracted view of \"Multi-expert visual tracking using hierarchical convolutional feature fusion via contextual information\" by Sathishkumar Moorthy et al.",
        "references": [
            "a5278fc76eff08668bc1957b01b22eb627fa2c36",
            "f3322661e9e59848837360cbc15e21f488ee313c",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "5c8a6874011640981e4103d120957802fa28f004",
            "552a06c09c49a91956e0bb3a69d7aae688dbcfd0",
            "b1b421b3d803dbaafb981947245875cf546e24ee",
            "a87cc499cf101b3697cacc65094b4b6590e0d061",
            "01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "ae6aa7a5674ae0050f87aca03bb141c6cbfd4b3a",
            "c46b08850b9c458704a3ca69172e6a0d40a6cb7f"
        ],
        "related_topics": [
            "Correlation Filters",
            "OTB-2015",
            "Scale Variations",
            "OTB-2013",
            "Tracking Methods",
            "Background Clutter",
            "UAVDT Dataset"
        ],
        "reference_count": "49",
        "citation_count": "12"
    },
    {
        "Id": "842451bbece5958301283c9398139130643dcb73",
        "title": "COMET: Context-Aware IoU-Guided Network for Small Object Tracking",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Javad Khaghani",
            "Hossein Ghanei-Yakhdan",
            "Shohreh Kasaei",
            "Li Cheng"
        ],
        "date": "4 June 2020",
        "abstract": "A context-aware IoU-guided tracker that exploits a multitask two-stream network and an offline reference proposal generation strategy that outperforms the state-of-the-arts in a range of aerial view datasets that focusing on tracking small objects. We consider the problem of tracking an unknown small target from aerial videos of medium to high altitudes. This is a challenging problem, which is even more pronounced in unavoidable scenarios of drastic camera motion and high density. To address this problem, we introduce a context-aware IoU-guided tracker (COMET) that exploits a multitask two-stream network and an offline reference proposal generation strategy. The proposed network fully exploits target-related information by multi-scale feature learning and attention modules. The proposed strategy introduces an efficient sampling strategy to generalize the network on the target and its parts without imposing extra computational complexity during online tracking. These strategies contribute considerably in handling significant occlusions and viewpoint changes. Empirically, COMET outperforms the state-of-the-arts in a range of aerial view datasets that focusing on tracking small objects. Specifically, COMET outperforms the celebrated ATOM tracker by an average margin of 6.2% (and 7%) in precision (and success) score on challenging benchmarks of UAVDT, VisDrone-2019, and Small-90.",
        "references": [
            "73e7090d7ad8af42add824518772cd99d4048611",
            "4d866d9c1cd7f756c1bda334e9e4342dcd489aef",
            "770a74e86e7a39eec441d8af00e37329e247d2c8",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "0619650ae0f698bcc38244a6858cc270df9dfaad",
            "3852738e4baa0a3bf57fb4e3d6d19435e764000e",
            "27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "9269b52994d8af23dc17dfbc225cd25c0902686c",
            "fdb98f5a7015de0956ef8d4e468257dc3079b5e5"
        ],
        "related_topics": [
            "Visual Tracking Datasets",
            "Characteristic Objects Method",
            "Viewpoint Change",
            "Online Tracking",
            "Small Objects",
            "Computational Complexity",
            "Camera Motion",
            "UAVDT"
        ],
        "reference_count": "81",
        "citation_count": "20"
    },
    {
        "Id": "6e0442456b3a475e1d836d7e345fdce98ef5ad26",
        "title": "CRACT: Cascaded Regression-Align-Classification for Robust Visual Tracking",
        "authors": [
            "Heng Fan",
            "Haibin Ling"
        ],
        "date": "25 November 2020",
        "abstract": "An improved proposal refinement module, Cascaded Regression-Align-Classification (CRAC), which yields new state-of-the-art performances on many benchmarks and introduces an identification-discrimination component for box classification, which leverages offline reliable fine-grained template and online rich background information to distinguish the target from background. High quality object proposals are crucial in visual tracking algorithms that utilize region proposal network (RPN). Refinement of these proposals, typically by box regression and classification in parallel, has been popularly adopted to boost tracking performance. However, it still meets problems when dealing with complex and dynamic background. Thus motivated, in this paper we introduce an improved proposal refinement module, Cascaded Regression-Align-Classification (CRAC), which yields new state-of-the-art performances on many benchmarks. First, having observed that the offsets from box regression can serve as guidance for proposal feature refinement, we design CRAC as a cascade of box regression, feature alignment and box classification. The key is to bridge box regression and classification via an alignment step, which leads to more accurate features for proposal classification with improved robustness. To address the variation in object appearance, we introduce an identification-discrimination component for box classification, which leverages offline reliable fine-grained template and online rich background information to distinguish the target from background. Moreover, we present pyramid RoIAlign that benefits CRAC by exploiting both the local and global cues of proposals. During inference, tracking proceeds by ranking all refined proposals and selecting the best one. In experiments on seven benchmarks including OTB-2015, UAV123, NfS, VOT-2018, TrackingNet, GOT-10k and LaSOT, our CRACT exhibits very promising results in comparison with state-of-the-art competitors and runs in real-time.",
        "references": [
            "738165f33c50b059e87b14d8b4a129230e14eacd",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "f98be9a91dbf00b52a494720bd36be9c73a1210e",
            "2088d93e7f4fa27b8498428d2ed64f144ab8cf3e",
            "84f911432ba8a3356013b3abfbf1947f1145c953",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "059282edacac41b220f295b5ee1d376aa19871d8"
        ],
        "related_topics": [
            "Classification",
            "Box Regression",
            "Region Proposal Networks",
            "OTB-2015",
            "VOT2018",
            "GOT-10k",
            "Large-scale Single Object Tracking",
            "TrackingNet",
            "Need For Speed",
            "Visual Tracking"
        ],
        "reference_count": "60",
        "citation_count": "9"
    },
    {
        "Id": "90a52f17754f775656c6d2e3bb4df1e1af8e8642",
        "title": "Multiple Pedestrians and Vehicles Tracking in Aerial Imagery: A Comprehensive Study",
        "authors": [
            "Seyed Majid Azimi",
            "Maximilian Kraus",
            "Reza Bahmanyar",
            "Peter Reinartz"
        ],
        "date": "19 October 2020",
        "abstract": "A proposed Deep Learning based Multi-Object Tracking method AerialMPTNet is described that fuses appearance, temporal, and graphical information using a Siamese Neural Network, a Long Short-Term Memory, and a Graph Convolutional Neural Network module for a more accurate and stable tracking. In this paper, we address various challenges in multi-pedestrian and vehicle tracking in high-resolution aerial imagery by intensive evaluation of a number of traditional and Deep Learning based Single- and Multi-Object Tracking methods. We also describe our proposed Deep Learning based Multi-Object Tracking method AerialMPTNet that fuses appearance, temporal, and graphical information using a Siamese Neural Network, a Long Short-Term Memory, and a Graph Convolutional Neural Network module for a more accurate and stable tracking. Moreover, we investigate the influence of the Squeeze-and-Excitation layers and Online Hard Example Mining on the performance of AerialMPTNet. To the best of our knowledge, we are the first in using these two for a regression-based Multi-Object Tracking. Additionally, we studied and compared the L1 and Huber loss functions. In our experiments, we extensively evaluate AerialMPTNet on three aerial Multi-Object Tracking datasets, namely AerialMPT and KIT AIS pedestrian and vehicle datasets. Qualitative and quantitative results show that AerialMPTNet outperforms all previous methods for the pedestrian datasets and achieves competitive results for the vehicle dataset. In addition, Long Short-Term Memory and Graph Convolutional Neural Network modules enhance the tracking performance. Moreover, using Squeeze-and-Excitation and Online Hard Example Mining significantly helps for some cases while degrades the results for other cases. In addition, according to the results, L1 yields better results with respect to Huber loss for most of the scenarios. The presented results provide a deep insight into challenges and opportunities of the aerial Multi-Object Tracking domain, paving the way for future research.",
        "references": [
            "7ba993bc50efa96fa36e9704c1e5190d5815b1bb",
            "c70d7dd6c4c9a7a091b76d9d94b5d122a4179e8c",
            "b9fb0b0e3389f8def4756885087290cd0fdb99be",
            "e5f68e7af6fff8b9a73ba58d5258fcf9437fa08a",
            "69b39e307d84890f817ee4ebb775460d162253d4",
            "bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "d3e0a4e2995ce11929e6aa5a68fc5e417efa615b",
            "1fbb4201af091aef55360f113ba35814063923e4",
            "5c8a6874011640981e4103d120957802fa28f004",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3"
        ],
        "related_topics": [],
        "reference_count": "77",
        "citation_count": "3"
    },
    {
        "Id": "171c292989d4d34163b59bd5e5cc1a383db9c0ab",
        "title": "Towards Accurate Pixel-wise Object Tracking by Attention Retrieval",
        "authors": [
            "Zhipeng Zhang",
            "Bing Li",
            "Weiming Hu",
            "Houwen Peng"
        ],
        "date": "6 August 2020",
        "abstract": "An attention retrieval network (ARN) to perform soft spatial constraints on backbone features and introduces a multi-resolution multi-stage segmentation network (MMS) to further weaken the influence of background clutter by reusing the predicted mask to filter backbone features. The encoding of the target in object tracking moves from the coarse bounding-box to fine-grained segmentation map recently. Revisiting de facto real-time approaches that are capable of predicting mask during tracking, we observed that they usually fork a light branch from the backbone network for segmentation. Although efficient, directly fusing backbone features without considering the negative influence of background clutter tends to introduce false-negative predictions, lagging the segmentation accuracy. To mitigate this problem, we propose an attention retrieval network (ARN) to perform soft spatial constraints on backbone features. We first build a look-up-table (LUT) with the ground-truth mask in the starting frame, and then retrieves the LUT to obtain an attention map for spatial constraints. Moreover, we introduce a multi-resolution multi-stage segmentation network (MMS) to further weaken the influence of background clutter by reusing the predicted mask to filter backbone features. Our approach set a new state-of-the-art on recent pixel-wise object tracking benchmark VOT2020 while running at 40 fps. Notably, the proposed model surpasses SiamMask by 11.7/4.2/5.5 points on VOT2020, DAVIS2016, and DAVIS2017, respectively. We will release our code at this https URL.",
        "references": [
            "12fae9a2c1ed867997e1ca70eba271b3c741c42f",
            "842b24b04ef2b142d655c7b50cd6ab0835d89330",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
            "1190e0210430e8b743af24cdc43efdeef407b669",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "ccb9ffa26b28dffc4f7d613821d1a9f0d60ea3f4",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "d710777495f51144c5b9f0a7372d16e3843e1b25"
        ],
        "related_topics": [
            "VOT2020",
            "Background Clutter",
            "Lower Urinary Tract",
            "Bounding Box",
            "Attention Maps",
            "Segmentation Map",
            "Backbone Network",
            "Object Tracking",
            "DAVIS-2017",
            "Frames Per Second"
        ],
        "reference_count": "55",
        "citation_count": "7"
    },
    {
        "Id": "adacccd99a42c3145ec6392a1a6b08878376e38b",
        "title": "High-Performance Long-Term Tracking With Meta-Updater",
        "authors": [
            "Kenan Dai",
            "Yunhua Zhang",
            "Dong Wang",
            "Jianhua Li",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "1 April 2020",
        "abstract": "This work proposes a novel offline-trained Meta-Updater that can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Long-term visual tracking has drawn increasing attention because it is much closer to practical applications than short-term tracking. Most top-ranked long-term trackers adopt the offline-trained Siamese architectures, thus,they cannot benefit from great progress of short-term trackers with online update. However, it is quite risky to straightforwardly introduce online-update-based trackers to solve the long-term problem, due to long-term uncertain and noisy observations. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Our meta-updater learns a binary output to guide the tracker\u2019s update and can be easily embedded into different trackers. This work also introduces a long-term tracking framework consisting of an online local tracker, an online verifier, a SiamRPN-based re-detector, and our meta-updater. Numerous experimental results on the VOT2018LT,VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our tracker performs remarkably better than other competing algorithms. Our project is available on the website: https://github.com/Daikenan/LTMU.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "383e67e0de2fdac787976543ba38bada48d046fc",
            "834baad9db5a1de1bfe993ff4a55a8a957eb9e0a",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "4f445f3e44f2f2ffb431cf1414c59ccba5a0b27d"
        ],
        "related_topics": [
            "Meta-Updater",
            "VOT2019LT",
            "Long-term Tracking",
            "VOT2018LT",
            "Skimming Module",
            "GlobalTrack",
            "Long-term Visual Tracking",
            "Long-Term Tracker",
            "Global Re-detector",
            "SPLT"
        ],
        "reference_count": "57",
        "citation_count": "141"
    },
    {
        "Id": "05df852d87566d335bbbf1864d191910205f482b",
        "title": "The First Visual Object Tracking Segmentation VOTS2023 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Jivr&#x27;i Matas",
            "Martin Danelljan",
            "Michael Felsberg",
            "Hyung Jin Chang",
            "Luka \u02c7Cehovin Zajc",
            "Alan Luke\u02c7zi\u02c7c",
            "Ondrej Drbohlav",
            "Zhongqun Zhang",
            "Khanh-Tung Tran",
            "Xuan-Son Vu",
            "Johanna Bj\u00a8orklund",
            "Christoph Mayer",
            "Yushan Zhang",
            "Lei Ke",
            "Jie Zhao",
            "Gustavo Fern\u00b4andez",
            "Noor M. Al-Shakarji",
            "Dong An",
            "Michael Arens",
            "Stefan Becker",
            "Goutam Bhat",
            "Sebastian Bullinger",
            "Antoni B. Chan",
            "Shijie Chang",
            "Hanyuan Chen",
            "Xin Chen",
            "Yan Chen",
            "Zhenyu Chen",
            "Yan-Xiang Cheng",
            "Yutao Cui",
            "Chunyuan Deng",
            "Jiahua Dong",
            "Matteo Dunnhofer",
            "Wei Feng",
            "Jianlong Fu",
            "Jie Gao",
            "Ruize Han",
            "Zeqi Hao",
            "Jun-Yan He",
            "Keji He",
            "Zhenyu He",
            "Xiantao Hu",
            "Kaer Huang",
            "Yuqing Huang",
            "Yi Jiang",
            "Ben Kang",
            "Jinpeng Lan",
            "Hyungjun Lee",
            "Chenyang Li",
            "Jiahao Li",
            "Ning Li",
            "Wangkai Li",
            "Xiaodi Li",
            "Xin Li",
            "Pengyu Liu",
            "Yue Liu",
            "Huchuan Lu",
            "Bin Luo",
            "Ping Luo",
            "Yinchao Ma",
            "Deshui Miao",
            "Christian Micheloni",
            "Kannappan Palaniappan",
            "Hancheol Park",
            "Matthieu Paul",
            "Houwen Peng",
            "Zekun Qian",
            "Gani Rahmon",
            "Norbert Scherer-Negenborn",
            "Pengcheng Shao",
            "Wooksu Shin",
            "Elham Soltani Kazemi",
            "Tian-Ming Song",
            "Rainer Stiefelhagen",
            "Rui Sun",
            "Chuanming Tang",
            "Zhangyong Tang",
            "Imad Eddine Toubal",
            "Jack Valmadre",
            "Joost van de Weijer",
            "Luc Van Gool",
            "Jash Vira",
            "Stephane Vujasinovi\u00b4c",
            "Cheng Wan",
            "Jia Wan",
            "Dong Wang",
            "Fei Wang",
            "Hekun Wang",
            "Limin Wang",
            "Song Wang",
            "Yaowei Wang",
            "Zhepeng Wang",
            "Gangshan Wu",
            "Jiannan Wu",
            "Qiangqiang Wu",
            "Xiaojun Wu",
            "Anqi Xiao",
            "Jinxia Xie",
            "Chen Chen Xu",
            "Min Xu",
            "Tian-hao Xu",
            "Yuanyou Xu",
            "Bin Yan",
            "Dawei Yang",
            "Mingdong Yang",
            "Tianyu Yang",
            "Yi Yang",
            "Zongxin Yang",
            "Xuanwu Yin",
            "Fisher Yu",
            "Hongyuan Yu",
            "Qian Yu",
            "Weichen Yu",
            "YongSheng Yuan",
            "Zehuan Yuan",
            "Jianlin Zhang",
            "Lu Zhang",
            "Tianzhu Zhang",
            "Guodongfang Zhao",
            "Shaochuan Zhao",
            "Ya-Jing Zheng",
            "Bineng Zhong",
            "Jiawen Zhu",
            "Xuefeng Zhu",
            "Yueting Zhuang",
            "Cheng Zong",
            "Kunlong Zuo"
        ],
        "date": "2 October 2023",
        "abstract": "Results of the presented 47 trackers indicate that modern tracking frameworks are well-suited to deal with convergence of short-term and long-term tracking and that multiple and single target tracking can be considered a single problem. The Visual Object Tracking Segmentation VOTS2023 challenge is the eleventh annual tracker benchmarking activity of the VOT initiative. This challenge is the first to merge short-term and long-term as well as single-target and multiple-target tracking with segmentation masks as the only target location specification. A new dataset was created; the ground truth has been withheld to prevent overfitting. New performance measures and evaluation protocols have been created along with a new toolkit and an evaluation server. Results of the presented 47 trackers indicate that modern tracking frameworks are well-suited to deal with convergence of short-term and long-term tracking and that multiple and single target tracking can be considered a single problem. A leaderboard, with participating trackers details, the source code, the datasets, and the evaluation kit are publicly available at the challenge website1.",
        "references": [
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "53329e5c79c1128c7b252a12b182c472a3413bfa",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "9286efaa3dba58837b628f61f4940a09b3eeb85c"
        ],
        "related_topics": [],
        "reference_count": "71",
        "citation_count": "2"
    },
    {
        "Id": "0fba73b3b1e73db08c68c95384b09659694c1b5d",
        "title": "Learning Spatial Distribution of Long-Term Trackers Scores",
        "authors": [
            "Vincenzo Mariano Scarrica",
            "Antonino Staiano"
        ],
        "date": "2 August 2023",
        "abstract": "This work aims to generalize the fusion concept to an arbitrary number of trackers used as baseline trackers in the pipeline, leveraging a learning phase to better understand how outcomes correlate with each other, even when no target is present. Long-Term tracking is a hot topic in Computer Vision. In this context, competitive models are presented every year, showing a constant growth rate in performances, mainly measured in standardized protocols as Visual Object Tracking (VOT) and Object Tracking Benchmark (OTB). Fusion-trackers strategy has been applied over last few years for overcoming the known re-detection problem, turning out to be an important breakthrough. Following this approach, this work aims to generalize the fusion concept to an arbitrary number of trackers used as baseline trackers in the pipeline, leveraging a learning phase to better understand how outcomes correlate with each other, even when no target is present. A model and data independence conjecture will be evidenced in the manuscript, yielding a recall of 0.738 on LTB-50 dataset when learning from VOT-LT2022, and 0.619 by reversing the two datasets. In both cases, results are strongly competitive with state-of-the-art and recall turns out to be the first on the podium.",
        "references": [
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "009e625561119aa9affb936ad74e611fd1fa36d4",
            "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Object Tracking Benchmark",
            "Baseline Trackers",
            "Long-term Tracking",
            "Long-Term Tracker",
            "Computer Vision"
        ],
        "reference_count": "0",
        "citation_count": "48"
    },
    {
        "Id": "2d4713ce1df60f771b65e900fd02352989df82ef",
        "title": "Long-term Visual Tracking: Review and Experimental Comparison",
        "authors": [
            "Chang Liu",
            "Xiao-Fan Chen",
            "Chunjuan Bo",
            "Dong Wang"
        ],
        "date": "7 November 2022",
        "abstract": "This paper provides a thorough review of long-term tracking, summarizing long- term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results, and discusses the future prospects from multiple perspectives. As a fundamental task in computer vision, visual object tracking has received much attention in recent years. Most studies focus on short-term visual tracking which addresses shorter videos and always-visible targets. However, long-term visual tracking is much closer to practical applications with more complicated challenges. There exists a longer duration such as minute-level or even hour-level in the long-term tracking task, and the task also needs to handle more frequent target disappearance and reappearance. In this paper, we provide a thorough review of long-term tracking, summarizing long-term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results. Then we provide a detailed description of existing benchmarks and corresponding evaluation protocols. Furthermore, we conduct extensive experiments and analyse the performance of trackers on six benchmarks: VOTLT2018, VOTLT2019 (2020/2021), OxUvA, LaSOT, TLP and the long-term subset of VTUAV-V. Finally, we discuss the future prospects from multiple perspectives, including algorithm design and benchmark construction. To our knowledge, this is the first comprehensive survey for long-term visual object tracking. The relevant content is available at https://github.com/wangdong-dut/Long-term-Visual-Tracking.",
        "references": [
            "23f8927f996d56f3b5076d8993a70bcfc70182a1",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "1ae15ff20d54d9ffd2a45a9c124c77ad2b419ae3",
            "786577081e00d69eeac8e9612eaf2dad59765e73",
            "894e4376750b83b63649cc518b121f345ca0df83",
            "913cebc279c363fb9476496f096519e27212b3d5"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Long-term Visual Tracking",
            "Large-scale Single Object Tracking",
            "Evaluation Protocol",
            "OxUvA",
            "Computer Vision",
            "Target Disappearances",
            "Time-Lock Puzzles"
        ],
        "reference_count": "100",
        "citation_count": "6"
    },
    {
        "Id": "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
        "title": "Switch and Refine: A Long-Term Tracking and Segmentation Framework",
        "authors": [
            "Xiang Xu",
            "Jian Zhao",
            "Jianmin Wu",
            "Furao Shen"
        ],
        "date": "1 March 2023",
        "abstract": "A new long-term VOT framework is proposed that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. In long-term video object tracking (VOT) tasks, most long-term trackers are modified from short-term trackers, which contain more and more machine learning modules to improve their performance. However, we empirically find that more modules do not necessarily lead to better results. In this paper, we make the long-term tracking framework simple by carefully selecting the cutting-edge trackers. Specifically, we propose a new long-term VOT framework that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. Such a framework fully exploits existing advanced works from three complementary perspectives. Experimental results show that by exploiting the capabilities of existing methods instead of designing new neural networks, we can still achieve remarkable results on seven long-term VOT datasets. By introducing a continuous adjustable speed control parameter, our tracker reaches 20+FPS with only a small performance loss. The refine module not only improves the bounding box estimations but also outputs segmentation masks, so that our framework can handle the video object segmentation (VOS) tasks by using only VOT trackers. We obtain a trade-off between time and accuracy on two representative VOS datasets by only using bounding boxes as the initial input.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "f1c9f81ce054619f30b5c27fd97579f7216d7048",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "f6186788541d332af19a96183787e01ef9080fb0",
            "3985382474245388bbc73e2c849e783010901775"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Video Object Segmentation",
            "Global Re-detector",
            "Bounding Box Estimation",
            "Long-term Tracking",
            "Neural Network",
            "Segmentation Masks",
            "Long-term Tracking Framework"
        ],
        "reference_count": "76",
        "citation_count": "4"
    },
    {
        "Id": "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
        "title": "CoCoLoT: Combining Complementary Trackers in Long-Term Visual Tracking",
        "authors": [
            "Matteo Dunnhofer",
            "Christian Micheloni"
        ],
        "date": "9 May 2022",
        "abstract": "This paper provides a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance and competes favourably with the state-of-the-art on the most popular long- term visual tracking benchmarks. How to combine the complementary capabilities of an ensemble of different algorithms has been of central interest in visual object tracking. A significant progress on such a problem has been achieved, but considering short-term tracking scenarios. Instead, long-term tracking settings have been substantially ignored by the solutions. In this paper, we explicitly consider long-term tracking scenarios and provide a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance. CoCoLoT perceives whether the trackers are following the target object through an online learned deep verification model, and accordingly activates a decision policy which selects the best performing tracker as well as it corrects the performance of the failing one. The proposed methodology is evaluated extensively and the comparison with several other solutions reveals that it competes favourably with the state-of-the-art on the most popular long-term visual tracking benchmarks.",
        "references": [
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "bd4f219ce6bc5c22f9da71959d5192cf0b0141fe",
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "c734274f43575bc5f4bcf8719f0be55a5e89be5e",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "5b73cd259a3fa72f95e8bac9e520250b950acf3a",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "eb00b8453b23d4f6f142378e2fb0f0a9e6f9c5e2",
            "19d6b9725a59f4b624205829d5f03ac893ca1367"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Long-term Tracking",
            "Long-term Visual Tracking",
            "Visual Trackers",
            "Complementary Trackers",
            "Ensemble"
        ],
        "reference_count": "47",
        "citation_count": "4"
    },
    {
        "Id": "a4f25f02ca52e1122aa7494244928b8e4684258b",
        "title": "Effective long-term tracking with contrast optimizer",
        "authors": [
            "Yongbo Han",
            "Yitao Liang"
        ],
        "date": "1 July 2023",
        "abstract": "A contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. The main challenge of long-term tracking includes data uncertainty in long-term observations. Previous methods tackle the long-term tracking task by online update-based trackers. However, sophisticated online update strategies of these trackers are usually with a considerable computational burden. In this work, a contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. Specifically, the optimizer first perceives the similarity between distractors and positive samples through metric learning. Next, the contrastive learning between target anchors and hard negative samples forces the optimizer to notice the difference between targets and distractors. Finally, the optimizer will learn a binary output to assist the tracker updating. The proposed optimizer can be easily integrated into other online trackers with little impact on their running speed. Extensive experimental results show that the method achieves state-of-the-art performance on the VOT2018LT, VOT2019LT, OxUvA, and LaSOT benchmarks while running at real-time speed on GPU.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4"
        ],
        "related_topics": [
            "Long-term Tracking",
            "Hard Negative Samples",
            "VOT2019LT",
            "Metric Learning",
            "LaSOT Benchmark",
            "VOT2018LT",
            "Trackers",
            "Contrastive Learning",
            "Positive Sample",
            "OxUvA"
        ],
        "reference_count": "0",
        "citation_count": "71"
    },
    {
        "Id": "4bb2df1e5dc09ec7f0e50b5ba304a6e51943c72f",
        "title": "Long-term tracking with transformer and template update",
        "authors": [
            "Hongying Zhang",
            "Xiao-Xiao Peng",
            "Xuyong Wang"
        ],
        "date": "1 December 2022",
        "abstract": "A feature extraction network based on the transformer and adopt a knowledge distillation strategy to improve the effectiveness of the network for global feature extraction and demonstrates the superiority of the method. Aiming at the tracking failure due to the disappearance of the target in the long-term target tracking process, this paper proposes a long-term target tracking network based on the visual transformer and template update. First of all, we construct a feature extraction network based on the transformer and adopt a knowledge distillation strategy to improve the effectiveness of the network for global feature extraction. Secondly, in the modeling transformer, the target features are fully fused with the search area features by using encoder, and the position information in the target query is learned by the decoder. Then, target predictions are performed on the information from the encoder\u2013decoder to obtain tracking results. Meanwhile, we design a score head model to judge the validity of the dynamic template of the current frame before tracking in the next frame. We select the appropriate dynamic template for the tracking of the next frame according to the score result. In this paper, we performed extensive experiments on LaSOT, VOT2021-LT, TrackingNet, TLP, and UAV123 datasets, and the experimental results prove the effectiveness of our method. In particular, it exceeds STARK by 0.8 $$\\%$$ % (F score) on VOT2021-LT, 1.0 $$\\%$$ % (S score) on LaSOT, and TrackingNet exceed STARK by 1.1 $$\\%$$ % (NP score), which also demonstrates the superiority of the method in this paper.",
        "references": [
            "43cae64a7f0b0942b0409fd4ef4009b0a07a8e5f",
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "82a9596957fafd92893195dc9ad4bb2aca86f72c",
            "0357156aef567fb5b709222894ddea1ce5d4e721",
            "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "46fc2e550dd695eaa899a07a01e306a48b73b656",
            "811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "157ff6e216985911cc2f9775155d2a424ba2984b",
            "b6eaec7917439d79ce840fa97bc371552e9b6685",
            "adacccd99a42c3145ec6392a1a6b08878376e38b"
        ],
        "related_topics": [
            "Large-scale Single Object Tracking",
            "Dynamic Template",
            "STARK",
            "Template Update",
            "TrackingNet",
            "Transformer",
            "Visual Transformers",
            "Tracking Results",
            "Long-term Tracking",
            "Tracking Failure"
        ],
        "reference_count": "0",
        "citation_count": "48"
    },
    {
        "Id": "80ddaf09abab8894622204ed6ac637f5f0f4b8ce",
        "title": "Depth-only Object Tracking",
        "authors": [
            "Song Yan",
            "Jinyu Yang",
            "Ale{\\vs} Leonardis",
            "J. K{\\&quot;a}m{\\&quot;a}r{\\&quot;a}inen"
        ],
        "date": "22 October 2021",
        "abstract": "The end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners and generates depth maps for tracking from the scratch with the generated data and fine-tune it with the available small RGBD tracking datasets. Depth (D) indicates occlusion and is less sensitive to illumination changes, which make depth attractive modality for Visual Object Tracking (VOT). Depth is used in RGBD object tracking where the best trackers are deep RGB trackers with additional heuristic using depth maps. There are two potential reasons for the heuristics: 1) the lack of large RGBD tracking datasets to train deep RGBD trackers and 2) the long-term evaluation protocol of VOT RGBD that benefits from heuristics such as depth-based occlusion detection. In this work, we study how far D-only tracking can go if trained with large amounts of depth data. To compensate the lack of depth data, we generate depth maps for tracking. We train a\"Depth-DiMP\"from the scratch with the generated data and fine-tune it with the available small RGBD tracking datasets. The depth-only DiMP achieves good accuracy in depth-only tracking and combined with the original RGB DiMP the end-to-end trained RGBD-DiMP outperforms the recent VOT 2020 RGBD winners.",
        "references": [
            "625aec94369715717158843c3ee288869cbe098f",
            "f202feae9ca7b3766e072b6af657beed2236a93c",
            "965b01ffc25e643acd16e91dd74ed0d1879f99ec",
            "487eb86379e979a72ebfef67db6eb8f048d1d258",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "13aaaf0e34cdb872bab899a12e3dc7d908c4cf60",
            "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "900ab48d25b44c076e31224b7befa503d9550c53",
            "12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "786577081e00d69eeac8e9612eaf2dad59765e73"
        ],
        "related_topics": [
            "Depth Map",
            "Visual Object Tracking",
            "Depth Data",
            "Illumination Change",
            "Fine-tune",
            "Object Tracking",
            "Evaluation Protocol"
        ],
        "reference_count": "22",
        "citation_count": "One"
    },
    {
        "Id": "dc9a66e0f329de8054f4ab845331fb7183987418",
        "title": "RGBD Object Tracking: An In-depth Review",
        "authors": [
            "Jinyu Yang",
            "Zhe Li",
            "Song Yan",
            "Feng Zheng",
            "Alevs Leonardis",
            "Joni-Kristian Kamarainen",
            "Ling Shao"
        ],
        "date": "26 March 2022",
        "abstract": "This paper is the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking, and proposes robustness evaluation against input perturbations. RGBD object tracking is gaining momentum in computer vision research thanks to the development of depth sensors. Although numerous RGBD trackers have been proposed with promising performance, an in-depth review for comprehensive understanding of this area is lacking. In this paper, we firstly review RGBD object trackers from different perspectives, including RGBD fusion, depth usage, and tracking framework. Then, we summarize the existing datasets and the evaluation metrics. We benchmark a representative set of RGBD trackers, and give detailed analyses based on their performances. Particularly, we are the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD tracking videos, we give an analysis of trackers' performance on handling target disappearance. To enable better understanding of RGBD trackers, we propose robustness evaluation against input perturbations. Finally, we summarize the challenges and provide open directions for this community. All resources are publicly available at https://github.com/memoryunreal/RGBD-tracking-review.",
        "references": [
            "f33b4ba5efdef921383bde48ed1ed4edff86edb9",
            "625aec94369715717158843c3ee288869cbe098f",
            "487eb86379e979a72ebfef67db6eb8f048d1d258",
            "6290d7a7e353fbfe77e21e4d1086143f5e66312b",
            "d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "f202feae9ca7b3766e072b6af657beed2236a93c",
            "c06ecdf5b149c322db0381adb6b3fd5ccb31a720",
            "761a9b5d8750eb63a9717650c4aaca53ce36a364",
            "7681f4c80774c6661980c5a76ffab357cca5f5cf",
            "3f02406b9b59d6f966c735953930fede1d751d0d"
        ],
        "related_topics": [
            "Rgb And Depth",
            "RGBD Tracker",
            "Depth Sensor",
            "Robustness Evaluation",
            "Object Tracking",
            "Trackers",
            "Computer Vision",
            "Target Disappearances",
            "Tracking Framework"
        ],
        "reference_count": "75",
        "citation_count": "6"
    },
    {
        "Id": "5edc3b4fe22a7d27bf142a91f924a070bbe567fc",
        "title": "LVOS: A Benchmark for Long-term Video Object Segmentation",
        "authors": [
            "Li Hong",
            "Wen-Chao Chen",
            "Zhongying Liu",
            "Wei Zhang",
            "Pinxue Guo",
            "Zhaoyu Chen",
            "Wenqiang Zhang"
        ],
        "date": "18 November 2022",
        "abstract": "A new benchmark dataset named LVOS is presented, which consists of 220 videos with a total duration of 421 minutes and is the first densely annotated long-term VOS dataset, and a Diverse Dynamic Memory network (DDMemory) that consists of three complementary memory banks to exploit temporal information adequately is proposed. Existing video object segmentation (VOS) benchmarks focus on short-term videos which just last about 3-5 seconds and where objects are visible most of the time. These videos are poorly representative of practical applications, and the absence of long-term datasets restricts further investigation of VOS on the application in realistic scenarios. So, in this paper, we present a new benchmark dataset named LVOS, which consists of 220 videos with a total duration of 421 minutes. To the best of our knowledge, LVOS is the first densely annotated long-term VOS dataset. The videos in our LVOS last 1.59 minutes on average, which is 20 times longer than videos in existing VOS datasets. Each video includes various attributes, especially challenges deriving from the wild, such as long-term reappearing and cross-temporal similar objeccts. Based on LVOS, we assess existing video object segmentation algorithms and propose a Diverse Dynamic Memory network (DDMemory) that consists of three complementary memory banks to exploit temporal information adequately. The experimental results demonstrate the strength and weaknesses of prior methods, pointing promising directions for further study. Data and code are available at https://lingyihongfd.github.io/lvos.github.io/.",
        "references": [
            "05e9e85b5137016c93d042170e82f77bb551a108",
            "10cb854adb764c1e8c6bd73bb706b1ed59d929b7",
            "f054fdd7a36b569eae7627cf12a4d81322dea022",
            "57076b4306819203521d14dfad08693c2f5452f7",
            "bf658a0ffd83b283656a38b25c99a7edd99020cd",
            "2916938cced9c4936fe7b6f9cc65f1ee3fcc4f47",
            "24d70dbe19baa72a0c8481d5006d3632712ba688",
            "7cb3aa0490a944f6c8d16fbc6c760238bde0ca74",
            "f4f34b56ef957981cebc3d901f49ddd638007d8d",
            "1190e0210430e8b743af24cdc43efdeef407b669"
        ],
        "related_topics": [
            "Video Object Segmentation",
            "Memory Bank",
            "VOS Datasets",
            "Video Object Segmentation Algorithms",
            "Benchmark Dataset"
        ],
        "reference_count": "92",
        "citation_count": "6"
    },
    {
        "Id": "7da79c39db11c8b94cb546173ad6b146c8d6e9b3",
        "title": "Exploiting Image-Related Inductive Biases in Single-Branch Visual Tracking",
        "authors": [
            "Chuanming Tang",
            "Kai Wang",
            "Joost van de Weijer",
            "Jianlin Zhang",
            "Yongmei Huang"
        ],
        "date": "30 October 2023",
        "abstract": "This work proposes an Adaptive ViT Model Prediction tracker (AViTMP) to bridge the gap between single-branch network and discriminative models, and presents a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. Despite achieving state-of-the-art performance in visual tracking, recent single-branch trackers tend to overlook the weak prior assumptions associated with the Vision Transformer (ViT) encoder and inference pipeline. Moreover, the effectiveness of discriminative trackers remains constrained due to the adoption of the dual-branch pipeline. To tackle the inferior effectiveness of the vanilla ViT, we propose an Adaptive ViT Model Prediction tracker (AViTMP) to bridge the gap between single-branch network and discriminative models. Specifically, in the proposed encoder AViT-Enc, we introduce an adaptor module and joint target state embedding to enrich the dense embedding paradigm based on ViT. Then, we combine AViT-Enc with a dense-fusion decoder and a discriminative target model to predict accurate location. Further, to mitigate the limitations of conventional inference practice, we present a novel inference pipeline called CycleTrack, which bolsters the tracking robustness in the presence of distractors via bidirectional cycle tracking verification. Lastly, we propose a dual-frame update inference strategy that adeptively handles significant challenges in long-term scenarios. In the experiments, we evaluate AViTMP on ten tracking benchmarks for a comprehensive assessment, including LaSOT, LaSOTExtSub, AVisT, etc. The experimental results unequivocally establish that AViTMP attains state-of-the-art performance, especially on long-time tracking and robustness.",
        "references": [
            "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "5b959cf1e226e19647d2fd2bcb7f65d43e68dbe5",
            "0fced9da4d992771c5575081778ff5a13afbbb51",
            "b6eaec7917439d79ce840fa97bc371552e9b6685",
            "e5274c4bc7d029fce4614d227fb162e0bb48a21e",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "77"
    },
    {
        "Id": "9b006ec78688e2171cb36ce44bbc111dabb412bf",
        "title": "ReIDTracker Sea: the technical report of BoaTrack and SeaDronesSee-MOT challenge at MaCVi of WACV24",
        "authors": [
            "Kaer Huang",
            "Weitu Chong"
        ],
        "date": "12 November 2023",
        "abstract": "The scheme accomplishes instance representation learning by using self-supervision on ImageNet and by cooperating with high-quality detectors, the multi-target tracking task can be completed simply and efficiently. Multi-Object Tracking is one of the most important technologies in maritime computer vision. Our solution tries to explore Multi-Object Tracking in maritime Unmanned Aerial vehicles (UAVs) and Unmanned Surface Vehicles (USVs) usage scenarios. Most of the current Multi-Object Tracking algorithms require complex association strategies and association information (2D location and motion, 3D motion, 3D depth, 2D appearance) to achieve better performance, which makes the entire tracking system extremely complex and heavy. At the same time, most of the current Multi-Object Tracking algorithms still require video annotation data which is costly to obtain for training. Our solution tries to explore Multi-Object Tracking in a completely unsupervised way. The scheme accomplishes instance representation learning by using self-supervision on ImageNet. Then, by cooperating with high-quality detectors, the multi-target tracking task can be completed simply and efficiently. The scheme achieved top 3 performance on both UAV-based Multi-Object Tracking with Reidentification and USV-based Multi-Object Tracking benchmarks and the solution won the championship in many multiple Multi-Object Tracking competitions. such as BDD100K MOT,MOTS, Waymo 2D MOT",
        "references": [
            "7307f99726c54d79e8e5a11b8bd1ce6f3ce74dc4",
            "e5e94778306509eb9c5ec6b083f8bb344d98a856",
            "7d1ff4ac2390759cbe60dd46b2b9bcabd4a90db4",
            "05df852d87566d335bbbf1864d191910205f482b",
            "3c574538e1d37cc5f7428aeda5e106c932a48e12",
            "2a94c84383ee3de5e6211d43d16e7de387f68878",
            "02b2d7e75078ca018bf193afdcf93dc330d4bf22",
            "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "c8b25fab5608c3e033d34b4483ec47e68ba109b7",
            "889c81b4d7b7ed43a3f69f880ea60b0572e02e27"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "15"
    },
    {
        "Id": "6179ac06f1a8fd1ac6b693b02824948dff438d54",
        "title": "The Visual Object Tracking VOT2016 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Luka Cehovin",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Gustav H{\\&quot;a}ger",
            "Alan Luke{\\vz}i{\\vc}",
            "Gustavo Javier Fernandez",
            "Abhinav Kumar Gupta",
            "Alfredo Petrosino",
            "Alireza Memarmoghadam",
            "{\\&#x27;A}lvaro Garc{\\&#x27;i}a-Mart{\\&#x27;i}n",
            "Andr{\\&#x27;e}s Sol{\\&#x27;i}s Montero",
            "Andrea Vedaldi",
            "Andreas Robinson",
            "Andy Jinhua Ma",
            "Anton Yuriiovych Varfolomieiev",
            "A. Aydin Alatan",
            "Aykut Erdem",
            "Bernard Ghanem",
            "Bin Liu",
            "Bohyung Han",
            "Brais Mart{\\&#x27;i}nez",
            "Chang-Ming Chang",
            "Changsheng Xu",
            "Chong Sun",
            "Daijin Kim",
            "Dapeng Chen",
            "Dawei Du",
            "Deepak Mishra",
            "D. Y. Yeung",
            "Erhan Gundogdu",
            "Erkut Erdem",
            "Fahad Shahbaz Khan",
            "Fatih Murat Porikli",
            "Fei Zhao",
            "Filiz Bunyak",
            "Francesco Battistone",
            "Gao Zhu",
            "Giorgio Roffo",
            "Gorthi Rama Krishna Sai Subrahmanyam",
            "Guilherme Sousa Bastos",
            "Guna Seetharaman",
            "Henry Medeiros",
            "Hongdong Li",
            "Honggang Qi",
            "Horst Bischof",
            "Horst Possegger",
            "Huchuan Lu",
            "Hyemin Lee",
            "Hyeonseob Nam",
            "Hyung Jin Chang",
            "Isabela Drummond",
            "Jack Valmadre",
            "Jae-chan Jeong",
            "Jae Il Cho",
            "Jae-Y. Lee",
            "Jianke Zhu",
            "Jiayi Feng",
            "Jin Gao",
            "Jin Young Choi",
            "Jingjing Xiao",
            "Ji-Wan Kim",
            "Jiyeoup Jeong",
            "Jo{\\~a}o F. Henriques",
            "Jochen Lang",
            "Jongwon Choi",
            "Jos{\\&#x27;e} M. Mart{\\&#x27;i}nez",
            "Junliang Xing",
            "Junyu Gao",
            "Kannappan Palaniappan",
            "Karel Lebeda",
            "Ke Gao",
            "Krystian Mikolajczyk",
            "Lei Qin",
            "Lijun Wang",
            "Longyin Wen",
            "Luca Bertinetto",
            "Madan Kumar Rapuru",
            "Mahdieh Poostchi",
            "Mario Edoardo Maresca",
            "Martin Danelljan",
            "Matthias Mueller",
            "Mengdan Zhang",
            "Michael Arens",
            "Michel F. Valstar",
            "Ming Tang",
            "Mooyeol Baek",
            "Muhammad Haris Khan",
            "Naiyan Wang",
            "Nana Fan",
            "Noor M. Al-Shakarji",
            "Ond\u0159ej Mik{\\vs}{\\&#x27;i}k",
            "Osman Akin",
            "Payman Moallem",
            "Pedro Senna",
            "Philip H. S. Torr",
            "Pong Chi Yuen",
            "Qingming Huang",
            "Rafael Mart{\\&#x27;i}n-Nieto",
            "Rengarajan Pelapur",
            "Richard Bowden",
            "Robert Lagani{\\`e}re",
            "R. Stolkin",
            "Ryan Walsh",
            "Sebastian Bernd Krah",
            "Shengkun Li",
            "Shengping Zhang",
            "Shizeng Yao",
            "Simon Hadfield",
            "Simone Melzi",
            "Siwei Lyu",
            "Siyi Li",
            "Stefan Becker",
            "Stuart Golodetz",
            "Sumithra Kakanuru",
            "Sunglok Choi",
            "Tao Hu",
            "Thomas Mauthner",
            "Tianzhu Zhang",
            "Tony P. Pridmore",
            "Vincenzo Santopietro",
            "Weiming Hu",
            "Wenbo Li",
            "Wolfgang H{\\&quot;u}bner",
            "Xiangyuan Lan",
            "Xiaomeng Wang",
            "Xin Li",
            "Yang Li",
            "Y. Demiris",
            "Yifan Wang",
            "Yuankai Qi",
            "Zejian Yuan",
            "Zexiong Cai",
            "Zhan Xu",
            "Zhenyu He",
            "Zhizhen Chi"
        ],
        "date": "8 October 2016",
        "abstract": "The Visual Object Tracking challenge VOT2016 goes beyond its predecessors by introducing a new semi-automatic ground truth bounding box annotation methodology and extending the evaluation system with the no-reset experiment. The Visual Object Tracking challenge VOT2016 aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 70 trackers are presented, with a large number of trackers being published at major computer vision conferences and journals in the recent years. The number of tested state-of-the-art trackers makes the VOT 2016 the largest and most challenging benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the Appendix. The VOT2016 goes beyond its predecessors by (i) introducing a new semi-automatic ground truth bounding box annotation methodology and (ii) extending the evaluation system with the no-reset experiment. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://votchallenge.net).",
        "references": [
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "6767812e114c426d45ea83894b156f7906e525cd",
            "4338f00c11224f1b4056125561927777ab610c9d",
            "91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "5bae9822d703c585a61575dced83fa2f4dea1c6d",
            "6b175816b1f81127f5e2a2fe998df99d62290a1c",
            "f15d5c0a9d2f3678b4c16330da29b3b4511fdef5"
        ],
        "related_topics": [],
        "reference_count": "110",
        "citation_count": "720"
    },
    {
        "Id": "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
        "title": "The Visual Object Tracking VOT2015 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Gustav H{\\&quot;a}ger",
            "Alan Luke{\\vz}i{\\vc}",
            "Abdelrahman Eldesokey",
            "Gustavo Javier Fernandez",
            "{\\&#x27;A}lvaro Garc{\\&#x27;i}a-Mart{\\&#x27;i}n",
            "Andrej Muhic",
            "Alfredo Petrosino",
            "Alireza Memarmoghadam",
            "Andrea Vedaldi",
            "Antoine Manzanera",
            "Antoine Tran",
            "Aydin Alatan",
            "Bogdan Cosmin Mocanu",
            "Boyu Chen",
            "Chang Huang",
            "Changsheng Xu",
            "Chong Sun",
            "Dalong Du",
            "Dafan Zhang",
            "Dawei Du",
            "Deepak Mishra",
            "Erhan Gundogdu",
            "Erik Velasco-Salido",
            "Fahad Shahbaz Khan",
            "Francesco Battistone",
            "Gorthi Rama Krishna Sai Subrahmanyam",
            "Goutam Bhat",
            "Guan Huang",
            "Guilherme Sousa Bastos",
            "Guna Seetharaman",
            "Hongliang Zhang",
            "Houqiang Li",
            "Huchuan Lu",
            "Isabela Drummond",
            "Jack Valmadre",
            "Jae-chan Jeong",
            "Jaeil Cho",
            "Jae-Y. Lee",
            "Jana Noskova",
            "Jianke Zhu",
            "Jin Gao",
            "Jingyu Liu",
            "Ji-Wan Kim",
            "Jo{\\~a}o F. Henriques",
            "Jos{\\&#x27;e} Mar{\\&#x27;i}a Mart{\\&#x27;i}nez Sanchez",
            "Junfei Zhuang",
            "Junliang Xing",
            "Junyu Gao",
            "Kai Chen",
            "Kannappan Palaniappan",
            "Karel Lebeda",
            "Ke Gao",
            "Kris Kitani",
            "Lei Zhang",
            "Lijun Wang",
            "Lingxiao Yang",
            "Longyin Wen",
            "Luca Bertinetto",
            "Mahdieh Poostchi",
            "Martin Danelljan",
            "Matthias Mueller",
            "Mengdan Zhang",
            "Ming-Hsuan Yang",
            "Nianhao Xie",
            "Ning Wang",
            "Ond\u0159ej Mik{\\vs}{\\&#x27;i}k",
            "Payman Moallem",
            "Pallavi M. Venugopal",
            "Pedro Senna",
            "Philip H. S. Torr",
            "Qiang Wang",
            "Qifeng Yu",
            "Qingming Huang",
            "Rafael Martin Nieto",
            "R. Bowden",
            "Risheng Liu",
            "Ruxandra Tapu",
            "Simon Hadfield",
            "Siwei Lyu",
            "Stuart Golodetz",
            "Sunglok Choi",
            "Tianzhu Zhang",
            "Titus B. Zaharia",
            "Vincenzo Santopietro",
            "Wei Zou",
            "Weiming Hu",
            "Wenbing Tao",
            "Wenbo Li",
            "Wen-gang Zhou",
            "Xianguo Yu",
            "Xiao Bian",
            "Yang Li",
            "Yifan Xing",
            "Yingruo Fan",
            "Zhengyu Zhu",
            "Zhipeng Zhang",
            "Zhiqun He"
        ],
        "date": "7 December 2015",
        "abstract": "The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance and presents a new VOT 2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute. The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 predecessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website.",
        "references": [
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "4338f00c11224f1b4056125561927777ab610c9d",
            "4dff84213493bb177dc6bff266a9893538a1f879",
            "84f911432ba8a3356013b3abfbf1947f1145c953",
            "91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "c4e7e62b3a3eb100b441674ad3817d9a24239e2a"
        ],
        "related_topics": [
            "VOT2017",
            "Visual Object Tracking",
            "VOT2014",
            "VOT Toolkit",
            "VOT2017 Challenge",
            "VOT2013",
            "Online Tracking Benchmark",
            "VOT2016",
            "Spatially Regularized DCF",
            "Equivalent Filter Operations"
        ],
        "reference_count": "94",
        "citation_count": "991"
    },
    {
        "Id": "53329e5c79c1128c7b252a12b182c472a3413bfa",
        "title": "The Visual Object Tracking VOT2017 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Luka Cehovin",
            "Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Gustav H{\\&quot;a}ger",
            "Alan Luke{\\vz}i{\\vc}",
            "Abdelrahman Eldesokey",
            "Gustavo Javier Fernandez",
            "Andrej Muhic",
            "Alfredo Petrosino",
            "Alireza Memarmoghadam",
            "Andrea",
            "Vedaldi",
            "Antoine Manzanera",
            "Antoine Tran",
            "A. Aydin Alatan",
            "Bogdan Cosmin Mocanu",
            "Boyu Chen",
            "Chang Huang",
            "Changsheng Xu",
            "Chong Sun",
            "Dalong Du",
            "David Zhang",
            "Dawei Du",
            "Deepak",
            "Mishra",
            "Erhan Gundogdu",
            "Erik Velasco-Salido",
            "Fahad Shahbaz Khan",
            "Francesco Battistone",
            "Gorthi Rama Krishna Sai Subrahmanyam",
            "Goutam Bhat",
            "Guan Huang",
            "Guilherme Sousa Bastos",
            "Guna",
            "Seetharaman",
            "Hongliang Zhang",
            "Houqiang Li",
            "Huchuan Lu",
            "Isabela Drummond",
            "Jack",
            "Valmadre",
            "Jae-chan Jeong",
            "Jae Il Cho",
            "Jae-Y. Lee",
            "Jana Noskova",
            "Jianke Zhu",
            "Jin Gao",
            "Jingyu Liu",
            "Ji-Wan Kim",
            "Jo{\\~a}o F. Henriques",
            "Junfei Zhuang",
            "Junliang Xing",
            "Junyu Gao",
            "Kai Chen",
            "Kannappan Palaniappan",
            "Karel Lebeda",
            "Ke Gao",
            "Kris Kitani",
            "Lei",
            "Zhang",
            "Lijun Wang",
            "Lingxiao Yang",
            "Longyin Wen",
            "Luca Bertinetto",
            "Mahdieh Poostchi",
            "Martin Danelljan",
            "Matthias Mueller",
            "Mengdan Zhang",
            "Ming-Hsuan Yang",
            "Nianhao Xie",
            "Ning",
            "Wang",
            "Ond\u0159ej Mik{\\vs}{\\&#x27;i}k",
            "Payman Moallem",
            "M PallaviVenugopal",
            "Pedro Senna",
            "Philip H. S. Torr",
            "Qiang Wang",
            "Qifeng Yu",
            "Qingming Huang",
            "Rafael Mart{\\&#x27;i}n-Nieto",
            "R. Bowden",
            "Ri-sheng",
            "Liu",
            "Ruxandra Tapu",
            "Simon Hadfield",
            "Siwei Lyu",
            "Stuart Golodetz",
            "Sunglok Choi",
            "Tianzhu",
            "Titus B. Zaharia",
            "Vincenzo Santopietro",
            "Wei Zou",
            "Weiming Hu",
            "Wenbing Tao",
            "Wenbo",
            "Li",
            "Wen-gang Zhou",
            "Xianguo Yu",
            "Xiao Bian",
            "Yang Li",
            "Yifan Xing",
            "Yingruo Fan",
            "Zheng",
            "Zhu",
            "Zhipeng Zhang",
            "Zhiqun He"
        ],
        "date": "1 October 2017",
        "abstract": "The Visual Object Tracking challenge VOT2017 is the fifth annual tracker benchmarking activity organized by the VOT initiative. Results of 51 trackers are presented; many are state-of-the-art published at major computer vision conferences or journals in recent years. The evaluation included the standard VOT and other popular methodologies and a new \"real-time\" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The VOT2017 goes beyond its predecessors by (i) improving the VOT public dataset and introducing a separate VOT2017 sequestered dataset, (ii) introducing a realtime tracking experiment and (iii) releasing a redesigned toolkit that supports complex experiments. The dataset, the evaluation kit and the results are publicly available at the challenge website1.",
        "references": [
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "4338f00c11224f1b4056125561927777ab610c9d",
            "dd45fe910a0200d43aaa77362f658542f6e175ff",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "5648597dc65a3e1fdc6d8e0aeccbf9bf6fe82dcb",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "91f2b2aeb7e65d0b673ed7e782488b3365027979"
        ],
        "related_topics": [],
        "reference_count": "81",
        "citation_count": "120"
    },
    {
        "Id": "3c74b636c0f74c1a0cbbd6e165c2760264044971",
        "title": "The Visual Object Tracking VOT2014 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Juan E. Sala Matas",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Luka Cehovin",
            "Georg Nebehay",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Gustavo Javier Fernandez",
            "Alan Luke{\\vz}i{\\vc}",
            "Aleksandar Dimitriev",
            "Alfredo Petrosino",
            "Amir Saffari",
            "Bo Li",
            "Bohyung Han",
            "Cherkeng Heng",
            "Christophe Garcia",
            "Dominik Pangersic",
            "Gustav H{\\&quot;a}ger",
            "Fahad Shahbaz Khan",
            "Franc Oven",
            "Horst Possegger",
            "Horst Bischof",
            "Hyeonseob Nam",
            "Jianke Zhu",
            "Jijia Li",
            "Jin Young Choi",
            "Jinwoo Choi",
            "Jo{\\~a}o F. Henriques",
            "Joost van de Weijer",
            "Jorge Batista",
            "Karel Lebeda",
            "Kristoffer {\\&quot;O}fj{\\&quot;a}ll",
            "Kwang Moo Yi",
            "Lei Qin",
            "Longyin Wen",
            "Mario Edoardo Maresca",
            "Martin Danelljan",
            "Michael Felsberg",
            "Ming-Ming Cheng",
            "Philip H. S. Torr",
            "Qingming Huang",
            "R. Bowden",
            "Sam Hare",
            "Samantha YueYing Lim",
            "Seunghoon Hong",
            "Shengcai Liao",
            "Simon Hadfield",
            "S. Li",
            "Stefan Duffner",
            "Stuart Golodetz",
            "Thomas Mauthner",
            "Vibhav Vineet",
            "Weiyao Lin",
            "Yang Li",
            "Yuankai Qi",
            "Zhen Lei",
            "Zhi Heng Niu"
        ],
        "date": "6 September 2014",
        "abstract": "The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance and introduces a new dataset with full annotation of targets by rotated bounding boxes and per-frame attribute. The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are presented. The number of tested trackers makes VOT 2014 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2014 challenge that go beyond its VOT2013 predecessor are introduced: (i) a new VOT2014 dataset with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2013 evaluation methodology, (iii) a new unit for tracking speed assessment less dependent on the hardware and (iv) the VOT2014 evaluation toolkit that significantly speeds up execution of experiments. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://votchallenge.net).",
        "references": [
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "4dff84213493bb177dc6bff266a9893538a1f879",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9",
            "5648597dc65a3e1fdc6d8e0aeccbf9bf6fe82dcb",
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "616b246e332573af1f4859aa91440280774c183a",
            "505f48d8236eb25f871da272c2ac2fe4b41ea289",
            "caa0fd34e50bb417fae3ee32f667e78fe5b198bc",
            "61394599ed0aabe04b724c7ca3a778825c7e776f"
        ],
        "related_topics": [
            "VOT2014",
            "VOT2013",
            "Visual Object Tracking",
            "VOT2014 Challenge",
            "MATRIOSKA",
            "ETISEO",
            "VOT2013 Benchmark",
            "HMMTxD",
            "PixelTrack",
            "MatFlow"
        ],
        "reference_count": "78",
        "citation_count": "518"
    },
    {
        "Id": "4b1a47709d0546e5bc614bf9a521c550e6881d04",
        "title": "The Visual Object Tracking VOT2013 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Roman P. Pflugfelder",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Fatih Murat Porikli",
            "Luka Cehovin",
            "Georg Nebehay",
            "Gustavo Javier Fernandez",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Adam Gatt",
            "Ahmad Khajenezhad",
            "Ahmed Salah El-Din",
            "Ali Soltani-Farani",
            "Ali Zarezade",
            "Alfredo Petrosino",
            "Anthony Milton",
            "Behzad Bozorgtabar",
            "Bo Li",
            "Chee Seng Chan",
            "Cherkeng Heng",
            "Dale A. Ward",
            "David A. Kearney",
            "Dorothy Ndedi Monekosso",
            "Hakki Can Karaimer",
            "H.R. Rabiee",
            "Jianke Zhu",
            "Jin Gao",
            "Jingjing Xiao",
            "Junge Zhang",
            "Junliang Xing",
            "Kaiqi Huang",
            "Karel Lebeda",
            "Lijun Cao",
            "Mario Edoardo Maresca",
            "Mei Kuan Lim",
            "Mohamed ElHelw",
            "Michael Felsberg",
            "Paolo Remagnino",
            "Richard Bowden",
            "Roland G{\\&quot;o}cke",
            "Rustam Stolkin",
            "Samantha YueYing Lim",
            "Sara Maher",
            "S{\\&#x27;e}bastien Poullot",
            "Sebastien C. Wong",
            "Shin&#x27;ichi Satoh",
            "Weihua Chen",
            "Weiming Hu",
            "Xiaoqin Zhang",
            "Yang Li",
            "Zhi Heng Niu"
        ],
        "date": "2 December 2013",
        "abstract": "The evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset are presented, offering a more systematic comparison of the trackers. Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in conjunction with ICCV2013. Researchers from academia as well as industry were invited to participate in the first VOT2013 challenge which aimed at single-object visual trackers that do not apply pre-learned models of object appearance (model-free). Presented here is the VOT2013 benchmark dataset for evaluation of single-object visual trackers as well as the results obtained by the trackers competing in the challenge. In contrast to related attempts in tracker benchmarking, the dataset is labeled per-frame by visual attributes that indicate occlusion, illumination change, motion change, size change and camera motion, offering a more systematic comparison of the trackers. Furthermore, we have designed an automated system for performing and evaluating the experiments. We present the evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset. The dataset, the evaluation tools and the tracker rankings are publicly available from the challenge website (http://votchallenge.net).",
        "references": [
            "2822a883d149956934a20614d6934c6ddaac6857",
            "882c5e862f2256e10bb7dd74d5bbc984b01489fe",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9",
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "505f48d8236eb25f871da272c2ac2fe4b41ea289",
            "caa0fd34e50bb417fae3ee32f667e78fe5b198bc",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "bf5e48bcaddc8d8bfb2c5b138efdb90e94f8258f",
            "9d57723b4908397654fb1846d37db403d8b2b56a",
            "c63a34ac6a4e049118070e707ca7679fbb132d33"
        ],
        "related_topics": [],
        "reference_count": "55",
        "citation_count": "63"
    },
    {
        "Id": "786577081e00d69eeac8e9612eaf2dad59765e73",
        "title": "The Seventh Visual Object Tracking VOT2019 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Jiri Matas",
            "Ale{\\vs} Leonardis",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Joni-Kristian",
            "K{\\&quot;a}m{\\&quot;a}r{\\&quot;a}inen",
            "Luka Cehovin Zajc",
            "Ondrej Drbohlav",
            "Alan Luke{\\vz}i{\\vc}",
            "Amanda Berg",
            "Abdelrahman",
            "Eldesokey",
            "Jani K{\\&quot;a}pyl{\\&quot;a}",
            "Gustavo Javier Fernandez",
            "Abel Gonzalez-Garcia",
            "Alireza",
            "Memarmoghadam",
            "Andong Lu",
            "Anfeng He",
            "Anton Yuriiovych Varfolomieiev",
            "Antoni B. Chan",
            "Ardhendu Shekhar",
            "Tripathi",
            "Arnold W. M. Smeulders",
            "Bala Suraj Pedasingu",
            "Bao Xin Chen",
            "Baopeng Zhang",
            "Baoyuan Wu",
            "Bi",
            "Li",
            "Bin He",
            "Bin Yan",
            "Bing Bai",
            "Bing Li",
            "Bo Li",
            "Byeong Hak Kim",
            "Chao Ma",
            "Chen Fang",
            "Chen",
            "Qian",
            "Cheng Chen",
            "Chenglong Li",
            "Chengquan Zhang",
            "Chi-Yi Tsai",
            "Chong Luo",
            "Christian",
            "Micheloni",
            "Chunhui Zhang",
            "Dacheng Tao",
            "Deepak Gupta",
            "Dejia Song",
            "Dong Wang",
            "Efstratios",
            "Gavves",
            "Eunu Yi",
            "Fahad Shahbaz Khan",
            "Fangyi Zhang",
            "Fei Wang",
            "Fei Zhao",
            "George De",
            "Ath",
            "Goutam Bhat",
            "Guang-Gui Chen",
            "Guangting Wang",
            "Guoxuan Li",
            "Hakan \u00c7evikalp",
            "Hao Du",
            "Haojie",
            "Zhao",
            "Hasan Saribas",
            "Ho Min Jung",
            "Hongliang Bai",
            "Hongyuan Yu",
            "Houwen Peng",
            "Huchuan",
            "L\u01d4",
            "Hui Li",
            "Jia-Ke Li",
            "Jianhua Li",
            "Jianlong Fu",
            "Jie Chen",
            "Jie Gao",
            "Jie Zhao",
            "Jin Tang",
            "Jing",
            "Jingjing Wu",
            "Jingtuo Liu",
            "Jinqiao Wang",
            "Jinqing Qi",
            "Jinyue Zhang",
            "John Tsotsos",
            "Jong Hyuk Jong Hyuk",
            "Lee",
            "Joost van de Weijer",
            "Josef Kittler",
            "Jun Ha Lee",
            "Junfei Zhuang",
            "Kangkai Zhang",
            "Kangkang",
            "Wang",
            "Kenan Dai",
            "Lei Chen",
            "Lei Liu",
            "Leida Guo",
            "Li Zhang",
            "Liang Wang",
            "Liang Wang",
            "Lichao",
            "Zhang",
            "Lijun Wang",
            "Lijun Zhou",
            "Linyu Zheng",
            "Litu Rout",
            "Luc Van Gool",
            "Luca Bertinetto",
            "Martin",
            "Danelljan",
            "Matteo Dunnhofer",
            "Meng Ni",
            "Min Young Kim",
            "Ming Tang",
            "Ming-Hsuan Yang",
            "Naveen",
            "Paluru",
            "Niki Martinel",
            "Pengfei Xu",
            "Pengfei Zhang",
            "Pengkun Zheng",
            "Pengyu Zhang",
            "S. PhilipH.",
            "Torr",
            "Qi Zhang Qiang Wang",
            "Qing Guo",
            "Radu Timofte",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Richard",
            "Everson",
            "Ruize Han",
            "Ruohan Zhang",
            "Shan You",
            "Shaochuan Zhao",
            "Shengwei Zhao",
            "Shihu",
            "Shikun Li",
            "Shiming Ge",
            "Shuai Bai",
            "Shuosen Guan",
            "Tengfei Xing",
            "Tianyang Xu",
            "Tianyu",
            "Yang",
            "Ting Zhang",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Wei Feng",
            "Wei Hu",
            "Weizhao Wang",
            "Wenjie Tang",
            "Wenjun",
            "Zeng",
            "Wenyu Liu",
            "Xi Chen",
            "Xi Qiu",
            "Xiang Bai",
            "Xiaojun Wu",
            "Xiaoyun Yang",
            "Xier",
            "Xin Li",
            "Xingyuan Sun",
            "Xingyu Chen",
            "Xinmei Tian",
            "Xuwen Tang",
            "Xuefeng Zhu",
            "Yan-ping Huang",
            "Yanan",
            "Yanchao Lian",
            "Yang Gu",
            "Yang Ming Liu",
            "Yanjie Chen",
            "Yi Zhang",
            "Yinda Xu",
            "Yingming",
            "Yingping Li",
            "Yu Zhou",
            "Yuan Dong",
            "Yufei Xu",
            "Yunhua Zhang",
            "Yunkun Li",
            "Zeyu Zhao",
            "Luo",
            "Zhaoliang Zhang",
            "Zhenhua Feng",
            "Zhenyu He",
            "Zhichao Song",
            "Zhihao Chen",
            "Zhipeng",
            "Zhirong Wu",
            "Zhiwei Xiong",
            "Zhongjian Huang",
            "Zhu Teng",
            "Zihan Ni"
        ],
        "date": "1 October 2019",
        "abstract": "The Visual Object Tracking challenge VOT2019 is the seventh annual tracker benchmarking activity organized by the VOT initiative; results of 81 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The Visual Object Tracking challenge VOT2019 is the seventh annual tracker benchmarking activity organized by the VOT initiative. Results of 81 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis as well as the standard VOT methodology for long-term tracking analysis. The VOT2019 challenge was composed of five challenges focusing on different tracking domains: (i) VOTST2019 challenge focused on short-term tracking in RGB, (ii) VOT-RT2019 challenge focused on \"real-time\" shortterm tracking in RGB, (iii) VOT-LT2019 focused on longterm tracking namely coping with target disappearance and reappearance. Two new challenges have been introduced: (iv) VOT-RGBT2019 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2019 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2019, VOT-RT2019 and VOT-LT2019 datasets were refreshed while new datasets were introduced for VOT-RGBT2019 and VOT-RGBD2019. The VOT toolkit has been updated to support both standard shortterm, long-term tracking and tracking with multi-channel imagery. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.",
        "references": [
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "53329e5c79c1128c7b252a12b182c472a3413bfa",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "4338f00c11224f1b4056125561927777ab610c9d",
            "6767812e114c426d45ea83894b156f7906e525cd",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "23f8927f996d56f3b5076d8993a70bcfc70182a1",
            "dd45fe910a0200d43aaa77362f658542f6e175ff",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7"
        ],
        "related_topics": [
            "VOT2019",
            "VOT-RGBT2019",
            "Visual Object Tracking",
            "ATOM Tracker",
            "FuCoLoT",
            "DCFST",
            "VOT-RGBT2019 Dataset",
            "A3CTD",
            "RGB Tracking",
            "SPM-Tracker"
        ],
        "reference_count": "114",
        "citation_count": "366"
    },
    {
        "Id": "2574b5fb5a3a60caa44ecfecf7df5b2e315c39cf",
        "title": "Multiple Planar Object Tracking",
        "authors": [
            "Zhicheng Zhang",
            "Sheng-tong Liu",
            "Jufeng Yang"
        ],
        "date": "1 October 2023",
        "abstract": "This work presents a dual-branch network to track the visible part of planar objects, including vertexes and mask, and develops an occlusion area localization strategy to infer the invisible part of planar objects, i.e., the occluded region. Tracking both location and pose of multiple planar objects (MPOT) is of great significance to numerous real-world applications. The greater degree-of-freedom of planar objects compared with common objects makes MPOT far more challenging than well-studied object tracking, especially when occlusion occurs. To address this challenging task, we are inspired by amodal perception that humans jointly track visible and invisible parts of the target, and propose a tracking framework that unifies appearance perception and occlusion reasoning. Specifically, we present a dual-branch network to track the visible part of planar objects, including vertexes and mask. Then, we develop an occlusion area localization strategy to infer the invisible part, i.e., the occluded region, followed by a two-stream attention network finally refining the prediction. To alleviate the lack of data in this field, we build the first large-scale benchmark dataset, namely MPOT-3K. It consists of 3,717 planar objects from 356 videos and contains 148,896 frames together with 687,417 annotations. The collected planar objects have 9 motion patterns and the videos are shot in 6 types of indoor and outdoor scenes. Extensive experiments demonstrate the superiority of our proposed method on the newly developed MPOT-3K as well as other two popular single planar object tracking datasets. The code and MPOT-3K dataset are released on https://zzcheng.top/MPOT.",
        "references": [
            "e33bc1739e0ae16cff53aedda71dae2f43230f93",
            "e651d124ec8b6c3ab73f554b97954b05166b7cfb",
            "4049492a9c07e3cd162bd859399bb5df2b3e9644",
            "a62e40be6c2078ace369ce58801b3d5d1dd1a351",
            "2427b656de022430ae58b065c7bf8c26d1a0e7e0",
            "ebf7a1f10bfb7112487976f414efb28af6e6a08c",
            "1f0cb95449984a4346e2a3deca0370cc74cba470",
            "324a228b070c0f31976c0b0131b6a1929f5d58c0",
            "36a774834c0a19e3467338b73ab0fc1c78fb45b0",
            "7d1ff4ac2390759cbe60dd46b2b9bcabd4a90db4"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "104"
    },
    {
        "Id": "55d093ac4d40359e64b0208bc7a910858511e1b3",
        "title": "AR Long-Term Tracking Combining Multi-Attention and Template Updating",
        "authors": [
            "Mengru Guo",
            "Qiang Chen"
        ],
        "date": "17 April 2023",
        "abstract": "An attention-based feature fusion network effectively fuses the template and search area features through a combination of dual self-attention and cross attention, and the anchor frameless mechanism is adopted in the classification and regression network, resulting in a significant reduction in the number of parameters. Aiming at the problem that the augmented reality system is susceptible to complex scenes and easily leads to the failure of tracking registration, a long-term augmented reality tracking algorithm combining multi-attention and template updating is proposed. Firstly, we improved the ResNet-50 network to extract richer semantic features instead of AlexNet. Secondly, the attention-based feature fusion network effectively fuses the template and search area features through a combination of dual self-attention and cross attention. Dual self-attention effectively enhances the information in the context, whereas cross attention adaptively enhanced the features of both self-attention branches. Thirdly, the ORB feature-matching algorithm is utilized to match the template and search image features, with the template updated if more than 150 matching feature points are found. Lastly, the anchor frameless mechanism is adopted in the classification and regression network, resulting in a significant reduction in the number of parameters. The results of experiments conducted on various public datasets demonstrate the algorithm\u2019s high success rate and accuracy, as well as its robustness in complex environments.",
        "references": [
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "2822a883d149956934a20614d6934c6ddaac6857",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "e284bc13c2b76d0d0c7ad61d976f8a9d3eef8461",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "8c04f169203f9e55056a6f7f956695babe622a38",
            "490020c0d4fa1eb85fe353add5713e49f08c628d"
        ],
        "related_topics": [],
        "reference_count": "13",
        "citation_count": "One"
    },
    {
        "Id": "ebdb59f41d6cf5146fcc6bbf7784e0595338ddc7",
        "title": "SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation",
        "authors": [
            "L. Yao",
            "Haobo Zuo",
            "Guang-Zheng Zheng",
            "Changhong Fu",
            "Jia-Yu Pan"
        ],
        "date": "3 July 2023",
        "abstract": "This work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA, which can achieve better performance with fewer raw nighttime images, and is designed to determine enormous high-quality target domain training samples from every single raw nighttime image. Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample. This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking. Recently, the notable Segment Anything Model (SAM) has achieved remarkable zero-shot generalization ability to discover abundant potential objects due to its huge data-driven training approach. To solve the aforementioned issue, this work proposes a novel SAM-powered DA framework for real-time nighttime UAV tracking, i.e., SAM-DA. Specifically, an innovative SAM-powered target domain training sample swelling is designed to determine enormous high-quality target domain training samples from every single raw nighttime image. This novel one-to-many method significantly expands the high-quality target domain training sample for DA. Comprehensive experiments on extensive nighttime UAV videos prove the robustness and domain adaptability of SAM-DA for nighttime UAV tracking. Especially, compared to the SOTA DA, SAM-DA can achieve better performance with fewer raw nighttime images, i.e., the fewer-better training. This economized training approach facilitates the quick validation and deployment of algorithms for UAVs. The code is available at https://github.com/vision4robotics/SAM-DA.",
        "references": [
            "9e5d4094cbe46de2879ce588748083e1b52aa3be",
            "f3d7eb617179db9f9621fa2c978dfb9f2c39341f",
            "689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "32db2d409384575aeae453acc45220b51fe96301",
            "a0e544f6db3b6659a5f77c419908238f91b188bc",
            "7d0a977179bfe592a29d6aac84d309d3e532e2a9",
            "c846a8b1ffea73b8b1eee20d269e10c0a38aa1e8",
            "827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "27850781e39df9f750e05409b8072261124068e8"
        ],
        "related_topics": [
            "Unmanned Air Vehicles",
            "Segment Anything Model",
            "Self-attention Mechanism",
            "Self-organizing Tree Algorithm",
            "Domain Adaptation",
            "Transfer Learning"
        ],
        "reference_count": "49",
        "citation_count": "4"
    },
    {
        "Id": "d99e537321a88f1bc3800ff15b4438f52ef94f7b",
        "title": "Visual object tracking: Progress, challenge, and future",
        "authors": [
            "Libo Zhang",
            "Heng Fan"
        ],
        "date": "1 February 2023",
        "abstract": "Semantic Scholar extracted view of \"Visual object tracking: Progress, challenge, and future\" by Libo Zhang et al.",
        "references": [
            "1fbb4201af091aef55360f113ba35814063923e4",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "26e2ca763087be09e3799ad294302aa91077942d",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "204e3073870fae3d05bcbc2f6a8e263d9b72e776"
        ],
        "related_topics": [],
        "reference_count": "5",
        "citation_count": "2"
    },
    {
        "Id": "1f0ea37fa9fcdccb4b573b45a6be820f35e07516",
        "title": "Extracting high-precision full-field displacement from videos via pixel matching and optical flow",
        "authors": [
            "Lele Luan",
            "Yang Liu",
            "Haoqin Sun"
        ],
        "date": "1 October 2023",
        "abstract": "Semantic Scholar extracted view of \"Extracting high-precision full-field displacement from videos via pixel matching and optical flow\" by Lele Luan et al.",
        "references": [
            "b46e11a3d7d28a9db6434afa20a5e875bf3ef2d4",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "82cd378e57b29f5e954a3497deabd1db69b59abe",
            "c2a9272676bd5c42276d8446b85432d24ff034f6",
            "4fb316628a3cf5977e033721036118439c05e616",
            "7fd0814703f75dc6e58f55e7a4db241dba68ae12",
            "a7af2d9660bace3578e9b7c045b52e8b291d4584",
            "61a4b990d3fd2669338a98942acb1fe5d0350b55",
            "f2d1276cbfbfe1454f83b6630734c2faee34b99b",
            "c999e256703eb9f448a223ec3e9feb3879b6e087"
        ],
        "related_topics": [],
        "reference_count": "52",
        "citation_count": "2"
    },
    {
        "Id": "8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
        "title": "Hierarchical memory-guided long-term tracking with meta transformer inquiry network",
        "authors": [
            "Xingmei Wang",
            "Guohao Nie",
            "Boquan Li",
            "Yilin Zhao",
            "Minyang Kang",
            "Bo Liu"
        ],
        "date": "1 March 2023",
        "abstract": "Semantic Scholar extracted view of \"Hierarchical memory-guided long-term tracking with meta transformer inquiry network\" by Xingmei Wang et al.",
        "references": [
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "48fe9bbdc61ab6a7c9922273f527514bcecc0e40",
            "0f29ee600eea84e3826d090322b93b16674450bb",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "27849a90109b93ec80d190d570041722fb2b0576"
        ],
        "related_topics": [
            "Hierarchical Memories",
            "Long-term Tracking"
        ],
        "reference_count": "38",
        "citation_count": "One"
    },
    {
        "Id": "23f8927f996d56f3b5076d8993a70bcfc70182a1",
        "title": "Performance Evaluation Methodology for Long-Term Visual Object Tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "19 June 2019",
        "abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed and it is shown that these measures generalize the short-term performance measures, thus linking the two tracking problems. A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term tackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various re-detection strategies as well as influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate future development of long-term trackers.",
        "references": [
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d"
        ],
        "related_topics": [
            "Visual Object Tracking",
            "Sequence",
            "Trackers",
            "VOT Toolkit",
            "Target Disappearances"
        ],
        "reference_count": "50",
        "citation_count": "6"
    },
    {
        "Id": "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
        "title": "Now you see me: evaluating performance in long-term visual tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "19 April 2018",
        "abstract": "An extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluatinglong-term tracking - tracking precision, recall and F-score shows that a good model update strategy and the capability of image-wide re-detection are critical for long- term tracking performance. We propose a new long-term tracking performance evaluation methodology and present a new challenging dataset of carefully selected sequences with many target disappearances. We perform an extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluating long-term tracking - tracking precision, recall and F-score. The evaluation shows that a good model update strategy and the capability of image-wide re-detection are critical for long-term tracking performance. We integrated the methodology in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the development of long-term trackers.",
        "references": [
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "cdafc80c2d68c727756c9a5b528c86389f67b10b",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "a6ee08763d994b1687c594bb9367f8d5cf419113",
            "d3d36c3caa255053877a7e3250d47d906eec81d2"
        ],
        "related_topics": [
            "Target Disappearances",
            "Tracking Recall",
            "Long-Term Tracker",
            "LTB35",
            "FCLT",
            "Short-term Trackers",
            "VOT Toolkit",
            "Omni Directional Videos",
            "Re-Detection",
            "TLP Dataset"
        ],
        "reference_count": "36",
        "citation_count": "57"
    },
    {
        "Id": "19d6b9725a59f4b624205829d5f03ac893ca1367",
        "title": "Long-Term Visual Object Tracking Benchmark",
        "authors": [
            "A. Moudgil",
            "Vineet Gandhi"
        ],
        "date": "4 December 2017",
        "abstract": "Existing short sequence benchmarks fail to bring out the inherent differences in tracking algorithms which widen up while tracking on long sequences and the accuracy of trackers abruptly drops on challenging long sequences, suggesting the potential need of research efforts in the direction of long-term tracking. We propose a new long video dataset (called Track Long and Prosper - TLP) and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking. The proposed dataset paves a way to suitably assess long term tracking performance and train better deep learning architectures (avoiding/reducing augmentation, which may not reflect real world behaviour). We benchmark the dataset on 17 state of the art trackers and rank them according to tracking accuracy and run time speeds. We further present thorough qualitative and quantitative evaluation highlighting the importance of long term aspect of tracking. Our most interesting observations are (a) existing short sequence benchmarks fail to bring out the inherent differences in tracking algorithms which widen up while tracking on long sequences and (b) the accuracy of trackers abruptly drops on challenging long sequences, suggesting the potential need of research efforts in the direction of long-term tracking.",
        "references": [
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "1c721511e4c0e21bd264ca71c0d909528511b7ad",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "cdafc80c2d68c727756c9a5b528c86389f67b10b",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"
        ],
        "related_topics": [
            "TLP Dataset",
            "Omni Directional Videos",
            "Sequence",
            "Visual Object Tracking",
            "Long-term Tracking",
            "Augmentations",
            "State-of-the-art Trackers"
        ],
        "reference_count": "47",
        "citation_count": "77"
    },
    {
        "Id": "219e9a4527110baf1feb3df20db12064eeafdfb7",
        "title": "The Sixth Visual Object Tracking VOT2018 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Michael Felsberg",
            "Roman P. Pflugfelder",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Goutam Bhat",
            "Alan Luke{\\vz}i{\\vc}",
            "Abdelrahman Eldesokey",
            "Gustavo Javier Fernandez",
            "{\\&#x27;A}lvaro Garc{\\&#x27;i}a-Mart{\\&#x27;i}n",
            "{\\&#x27;A}lvaro Iglesias-Arias",
            "Aydin Alatan",
            "Abel Gonzalez-Garcia",
            "Alfredo Petrosino",
            "Alireza Memarmoghadam",
            "Andrea Vedaldi",
            "Andrej Muhic",
            "Anfeng He",
            "Arnold W. M. Smeulders",
            "Asanka G. Perera",
            "Bo Li",
            "Boyu Chen",
            "Changick Kim",
            "Changsheng Xu",
            "Changzhen Xiong",
            "Cheng Tian",
            "Chong Luo",
            "Chong Sun",
            "Cong Hao",
            "Daijin Kim",
            "Deepak Mishra",
            "Deming Chen",
            "Dong Wang",
            "Dongyoon Wee",
            "Efstratios Gavves",
            "Erhan Gundogdu",
            "Erik Velasco-Salido",
            "Fahad Shahbaz Khan",
            "Fan Yang",
            "Fei Zhao",
            "Feng Li",
            "Francesco Battistone",
            "George De Ath",
            "Gorthi Rama Krishna Sai Subrahmanyam",
            "Guilherme Sousa Bastos",
            "Haibin Ling",
            "Hamed Kiani Galoogahi",
            "Hankyeol Lee",
            "Haojie Li",
            "Haojie Zhao",
            "Heng Fan",
            "Honggang Zhang",
            "Horst Possegger",
            "Houqiang Li",
            "Huchuan Lu",
            "Hui Zhi",
            "Huiyun Li",
            "Hyemin Lee",
            "Hyung Jin Chang",
            "Isabela Drummond",
            "Jack Valmadre",
            "Jaime Spencer Martin",
            "Javaan Singh Chahl",
            "Jin Young Choi",
            "Jing Li",
            "Jinqiao Wang",
            "Jinqing Qi",
            "Jinyoung Sung",
            "Joakim Johnander",
            "Jo{\\~a}o F. Henriques",
            "Jongwon Choi",
            "Joost van de Weijer",
            "Jorge Rodr{\\&#x27;i}guez Herranz",
            "Jos{\\&#x27;e} Mar{\\&#x27;i}a Mart{\\&#x27;i}nez Sanchez",
            "Josef Kittler",
            "Junfei Zhuang",
            "Junyu Gao",
            "Klemen Grm",
            "Lichao Zhang",
            "Lijun Wang",
            "Lingxiao Yang",
            "Litu Rout",
            "Liu Si",
            "Luca Bertinetto",
            "Lutao Chu",
            "Manqiang Che",
            "Mario Edoardo Maresca",
            "Martin Danelljan",
            "Ming-Hsuan Yang",
            "Mohamed H. Abdelpakey",
            "Mohamed S. Shehata",
            "Myung Gu Kang",
            "Namhoon Lee",
            "Ning Wang",
            "Ond\u0159ej Mik{\\vs}{\\&#x27;i}k",
            "Payman Moallem",
            "Pablo Vicente-Mo{\\~n}ivar",
            "Pedro Senna",
            "Peixia Li",
            "Philip H. S. Torr",
            "Priya Mariam Raju",
            "Ruihe Qian",
            "Qiang Wang",
            "Qin Zhou",
            "Qing Guo",
            "Rafael Martin Nieto",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Ran Tao",
            "R. Bowden",
            "Richard M. Everson",
            "Runling Wang",
            "Sangdoo Yun",
            "Seokeon Choi",
            "Sergio Vivas",
            "Shuai Bai",
            "Shuangping Huang",
            "Sihang Wu",
            "Simon Hadfield",
            "Siwen Wang",
            "Stuart Golodetz",
            "Ming Tang",
            "Tianyang Xu",
            "Tianzhu Zhang",
            "Tobias Fischer",
            "Vincenzo Santopietro",
            "Vitomir {\\vS}truc",
            "Wei Wang",
            "Wangmeng Zuo",
            "Wei Feng",
            "Wei Wu",
            "Wei Zou",
            "Weiming Hu",
            "Wen-gang Zhou",
            "Wen Jun Zeng",
            "Xiaofan Zhang",
            "Xiaohe Wu",
            "Xiaojun Wu",
            "Xinmei Tian",
            "Yan Li",
            "Yan Lu",
            "Yee Wei Law",
            "Yi Wu",
            "Y. Demiris",
            "Yicai Yang",
            "Yifan Jiao",
            "Yuhong Li",
            "Yunhua Zhang",
            "Yuxuan Sun",
            "Zheng Zhang",
            "Zhengyu Zhu",
            "Zhenhua Feng",
            "Zhihui Wang",
            "Zhiqun He"
        ],
        "date": "8 September 2018",
        "abstract": "The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative; results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a \u201creal-time\u201d experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new long-term tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website (http://votchallenge.net).",
        "references": [
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "4338f00c11224f1b4056125561927777ab610c9d",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9"
        ],
        "related_topics": [
            "VOT2018",
            "VOT2018 Challenge",
            "LADCF",
            "Visual Object Tracking",
            "Long-term Tracking",
            "Spatially Regularized DCF",
            "Target Disappearances",
            "Siamese Region Proposal Network",
            "Expected Average Overlap",
            "LTB35 Dataset"
        ],
        "reference_count": "100",
        "citation_count": "656"
    },
    {
        "Id": "4b1a47709d0546e5bc614bf9a521c550e6881d04",
        "title": "The Visual Object Tracking VOT2013 Challenge Results",
        "authors": [
            "Matej Kristan",
            "Roman P. Pflugfelder",
            "Ale{\\vs} Leonardis",
            "Jiri Matas",
            "Fatih Murat Porikli",
            "Luka Cehovin",
            "Georg Nebehay",
            "Gustavo Javier Fernandez",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Adam Gatt",
            "Ahmad Khajenezhad",
            "Ahmed Salah El-Din",
            "Ali Soltani-Farani",
            "Ali Zarezade",
            "Alfredo Petrosino",
            "Anthony Milton",
            "Behzad Bozorgtabar",
            "Bo Li",
            "Chee Seng Chan",
            "Cherkeng Heng",
            "Dale A. Ward",
            "David A. Kearney",
            "Dorothy Ndedi Monekosso",
            "Hakki Can Karaimer",
            "H.R. Rabiee",
            "Jianke Zhu",
            "Jin Gao",
            "Jingjing Xiao",
            "Junge Zhang",
            "Junliang Xing",
            "Kaiqi Huang",
            "Karel Lebeda",
            "Lijun Cao",
            "Mario Edoardo Maresca",
            "Mei Kuan Lim",
            "Mohamed ElHelw",
            "Michael Felsberg",
            "Paolo Remagnino",
            "Richard Bowden",
            "Roland G{\\&quot;o}cke",
            "Rustam Stolkin",
            "Samantha YueYing Lim",
            "Sara Maher",
            "S{\\&#x27;e}bastien Poullot",
            "Sebastien C. Wong",
            "Shin&#x27;ichi Satoh",
            "Weihua Chen",
            "Weiming Hu",
            "Xiaoqin Zhang",
            "Yang Li",
            "Zhi Heng Niu"
        ],
        "date": "2 December 2013",
        "abstract": "The evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset are presented, offering a more systematic comparison of the trackers. Visual tracking has attracted a significant attention in the last few decades. The recent surge in the number of publications on tracking-related problems have made it almost impossible to follow the developments in the field. One of the reasons is that there is a lack of commonly accepted annotated data-sets and standardized evaluation protocols that would allow objective comparison of different tracking methods. To address this issue, the Visual Object Tracking (VOT) workshop was organized in conjunction with ICCV2013. Researchers from academia as well as industry were invited to participate in the first VOT2013 challenge which aimed at single-object visual trackers that do not apply pre-learned models of object appearance (model-free). Presented here is the VOT2013 benchmark dataset for evaluation of single-object visual trackers as well as the results obtained by the trackers competing in the challenge. In contrast to related attempts in tracker benchmarking, the dataset is labeled per-frame by visual attributes that indicate occlusion, illumination change, motion change, size change and camera motion, offering a more systematic comparison of the trackers. Furthermore, we have designed an automated system for performing and evaluating the experiments. We present the evaluation protocol of the VOT2013 challenge and the results of a comparison of 27 trackers on the benchmark dataset. The dataset, the evaluation tools and the tracker rankings are publicly available from the challenge website (http://votchallenge.net).",
        "references": [
            "2822a883d149956934a20614d6934c6ddaac6857",
            "882c5e862f2256e10bb7dd74d5bbc984b01489fe",
            "9926020dda21874dc7a5ef1511bae6c4cef5ecb9",
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "505f48d8236eb25f871da272c2ac2fe4b41ea289",
            "caa0fd34e50bb417fae3ee32f667e78fe5b198bc",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "bf5e48bcaddc8d8bfb2c5b138efdb90e94f8258f",
            "9d57723b4908397654fb1846d37db403d8b2b56a",
            "c63a34ac6a4e049118070e707ca7679fbb132d33"
        ],
        "related_topics": [],
        "reference_count": "55",
        "citation_count": "63"
    },
    {
        "Id": "fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
        "title": "Learning Target Candidate Association to Keep Track of What Not to Track",
        "authors": [
            "Christoph Mayer",
            "Martin Danelljan",
            "Danda Pani Paudel",
            "Luc Van Gool"
        ],
        "date": "30 March 2021",
        "abstract": "This work proposes a training strategy that combines partial annotations with self-supervision to keep track of distractor objects in order to continue tracking the target, and introduces a learned association network to propagate the identities of all target candidates from frame-to-frame. The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to eventual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.We propose to keep track of distractor objects in order to continue tracking the target. To this end, we introduce a learned association network, allowing us to propagate the identities of all target candidates from frame-to-frame. To tackle the problem of lacking ground-truth correspondences between distractor objects in visual tracking, we propose a training strategy that combines partial annotations with self-supervision. We conduct comprehensive experimental validation and analysis of our approach on several challenging datasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving an AUC score of 67.1% on LaSOT [21] and a +5.8% absolute gain on the OxUvA long-term dataset [41]. The code and trained models are available at https://github.com/visionml/pytracking",
        "references": [
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "d1e61fa7824709cae37fb59483dd0772e3101c08",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "ba9975c8cc84a0d27ecaf23de81c76d37d50420a",
            "811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "58fc267bf29e8982107a5212e02f35ba4f461310",
            "be412c7c7128cf91455233b652d6c94a6001a7c8"
        ],
        "related_topics": [
            "Large-scale Single Object Tracking",
            "SuperDiMP",
            "Normalized Precision Score",
            "Full Occlusion",
            "PrDiMP",
            "TrDiMP",
            "UAV123",
            "Need For Speed",
            "DiMP",
            "Long-term Tracking"
        ],
        "reference_count": "74",
        "citation_count": "126"
    },
    {
        "Id": "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
        "title": "Performance Evaluation Methodology for Long-Term Single-Object Tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "2 April 2020",
        "abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed and it is shown that these measures generalize the short-term performance measures, thus linking the two tracking problems. A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term trackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various redetection strategies as well as the influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the future development of long-term trackers.",
        "references": [],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
        "title": "Learning Spatio-Temporal Transformer for Visual Tracking",
        "authors": [
            "Bin Yan",
            "Houwen Peng",
            "Jianlong Fu",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "date": "31 March 2021",
        "abstract": "A new tracking architecture with an encoder-decoder transformer as the key component, which models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. In this paper, we present a new tracking architecture with an encoder-decoder transformer as the key component. The encoder models the global spatio-temporal feature dependencies between target objects and search regions, while the decoder learns a query embedding to predict the spatial positions of the target objects. Our method casts object tracking as a direct bounding box prediction problem, without using any proposals or predefined anchors. With the encoder-decoder transformer, the prediction of objects just uses a simple fully-convolutional network, which estimates the corners of objects directly. The whole method is end-to-end, does not need any postprocessing steps such as cosine window and bounding box smoothing, thus largely simplifying existing tracking pipelines. The proposed tracker achieves state-of-the-art performance on multiple challenging short-term and long-term benchmarks, while running at real-time speed, being 6\u00d7 faster than Siam R-CNN [54]. Code and models are open-sourced at https://github.com/researchmm/Stark.",
        "references": [
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "738165f33c50b059e87b14d8b4a129230e14eacd",
            "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "0357156aef567fb5b709222894ddea1ce5d4e721",
            "47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "26e2ca763087be09e3799ad294302aa91077942d",
            "069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9"
        ],
        "related_topics": [
            "Large-scale Single Object Tracking",
            "Search Region",
            "Corner Head",
            "Transformer Tracking",
            "Dynamic Template",
            "Search Region Features",
            "STARK",
            "TrackingNet",
            "GOT-10K",
            "Naive Correlation"
        ],
        "reference_count": "68",
        "citation_count": "379"
    },
    {
        "Id": "009e625561119aa9affb936ad74e611fd1fa36d4",
        "title": "A Simple Baseline for Multi-Object Tracking",
        "authors": [
            "Yifu Zhang",
            "Chunyu Wang",
            "Xinggang Wang",
            "Wenjun Zeng",
            "Wenyu Liu"
        ],
        "date": "4 April 2020",
        "abstract": "This work studies the essential reasons behind the failure of object detection and re-identification, and presents a simple baseline that remarkably outperforms the state-of-the-arts on the public datasets at $30$ fps. There has been remarkable progress on object detection and re-identification in recent years which are the core components for multi-object tracking. However, little attention has been focused on accomplishing the two tasks in a single network to improve the inference speed. The initial attempts along this path ended up with degraded results mainly because the re-identification branch is not appropriately learned. In this work, we study the essential reasons behind the failure, and accordingly present a simple baseline to addresses the problem. It remarkably outperforms the state-of-the-arts on the public datasets at $30$ fps. We hope this baseline could inspire and help evaluate new ideas in this field. The code and the pre-trained models are available at \\url{this https URL}.",
        "references": [
            "5bae9822d703c585a61575dced83fa2f4dea1c6d",
            "ddb80e2c3e1c2ba012ff33bafaef86f02b7275b0",
            "a62e40be6c2078ace369ce58801b3d5d1dd1a351",
            "894252730324f233b474bae1d6fe0b77d988ae83",
            "fbdc18d9e6bfcbac30b8bb330c71b6f2be48395e",
            "07f4ba45b771ed123b08261d88acda19406a7987",
            "d92827d0c62ce499e199caadb83e5ba457bc8869",
            "fb3948152788cfaf8a829aab2f02a6ec7de7c7d1",
            "ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4",
            "85aefde69e916523d9587b6abd01419420039474"
        ],
        "related_topics": [
            "Multi-object Tracking",
            "Re Identification",
            "Frames Per Second",
            "Pre-trained Models",
            "Object Detection",
            "Inference Speeds"
        ],
        "reference_count": "64",
        "citation_count": "103"
    },
    {
        "Id": "2c8315ae713b3e27c6e9f291a158134d9c516166",
        "title": "Learning Discriminative Model Prediction for Tracking",
        "authors": [
            "Goutam Bhat",
            "Martin Danelljan",
            "Luc Van Gool",
            "Radu Timofte"
        ],
        "date": "15 April 2019",
        "abstract": "An end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction, derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking.",
        "references": [
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "a3f3e1fca8f173b4cb47ad5488053a8829cbffbd",
            "09769e80cdf027db32a1fcb695a1aa0937214763",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "000178cd12c8a6e5da8215b6365fae03c20fd18d"
        ],
        "related_topics": [
            "Discriminative Model Prediction",
            "Discriminative Learning Loss",
            "VOT2018",
            "LaSOT Dataset",
            "SiamRPN++",
            "Need For Speed",
            "Background Appearance Information",
            "UAV123",
            "Siamese Trackers",
            "Large-scale Single Object Tracking"
        ],
        "reference_count": "50",
        "citation_count": "761"
    },
    {
        "Id": "be412c7c7128cf91455233b652d6c94a6001a7c8",
        "title": "SiamFC++: Towards Robust and Accurate Visual Tracking with Target Estimation Guidelines",
        "authors": [
            "Yinda Xu",
            "Zeyu Wang",
            "Zuoxin Li",
            "Yuan Ye",
            "Gang Yu"
        ],
        "date": "14 November 2019",
        "abstract": "This work proposes a set of practical guidelines of target state estimation for high-performance generic object tracker design and designs the Fully Convolutional Siamese tracker++ (SiamFC++), which achieves state-of-the-art performance on five challenging benchmarks, which proves both the tracking and generalization ability of the tracker. Visual tracking problem demands to efficiently perform robust classification and accurate target state estimation over a given target at the same time. Former methods have proposed various ways of target state estimation, yet few of them took the particularity of the visual tracking problem itself into consideration. Based on a careful analysis, we propose a set of practical guidelines of target state estimation for high-performance generic object tracker design. Following these guidelines, we design our Fully Convolutional Siamese tracker++ (SiamFC++) by introducing both classification and target state estimation branch (G1), classification score without ambiguity (G2), tracking without prior knowledge (G3), and estimation quality score (G4). Extensive analysis and ablation studies demonstrate the effectiveness of our proposed guidelines. Without bells and whistles, our SiamFC++ tracker achieves state-of-the-art performance on five challenging benchmarks(OTB2015, VOT2018, LaSOT, GOT-10k, TrackingNet), which proves both the tracking and generalization ability of the tracker. Particularly, on the large-scale TrackingNet dataset, SiamFC++ achieves a previously unseen AUC score of 75.4 while running at over 90 FPS, which is far above the real-time requirement.",
        "references": [
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "900ab48d25b44c076e31224b7befa503d9550c53",
            "a87cc499cf101b3697cacc65094b4b6590e0d061",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "70c3c9b9a40ca55264e454586dca2a6cf416f6e0"
        ],
        "related_topics": [
            "SiamFC++",
            "GOT-10k",
            "Large-scale Single Object Tracking",
            "TrackingNet",
            "VOT2018",
            "Cross Correlation Operation",
            "Template Branch",
            "Regression Branch",
            "TrackingNet Dataset",
            "LaSOT Benchmark"
        ],
        "reference_count": "37",
        "citation_count": "559"
    },
    {
        "Id": "c4c45661501c16064eead6e5d37dcb80d41c7a78",
        "title": "Object Tracking Benchmark",
        "authors": [
            "Yi Wu",
            "Jongwoo Lim",
            "Ming-Hsuan Yang"
        ],
        "date": "1 September 2015",
        "abstract": "An extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria is carried out to identify effective approaches for robust tracking and provide potential future research directions in this field. Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.",
        "references": [
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "a6ee08763d994b1687c594bb9367f8d5cf419113",
            "16e58cff042f6d556779431c5dc9dafcf092cbf9",
            "bf5e48bcaddc8d8bfb2c5b138efdb90e94f8258f",
            "739d084e486702dbdad01d668f77b431228bae9d",
            "4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "caa0fd34e50bb417fae3ee32f667e78fe5b198bc",
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "e13fc55a4dfbf933665e4555dafba558a17f9fa7",
            "eaf10795a2a34ba6638fd79815d4b81e20eb5955"
        ],
        "related_topics": [],
        "reference_count": "107",
        "citation_count": "2,805"
    },
    {
        "Id": "a4f25f02ca52e1122aa7494244928b8e4684258b",
        "title": "Effective long-term tracking with contrast optimizer",
        "authors": [
            "Yongbo Han",
            "Yitao Liang"
        ],
        "date": "1 July 2023",
        "abstract": "A contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. The main challenge of long-term tracking includes data uncertainty in long-term observations. Previous methods tackle the long-term tracking task by online update-based trackers. However, sophisticated online update strategies of these trackers are usually with a considerable computational burden. In this work, a contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. Specifically, the optimizer first perceives the similarity between distractors and positive samples through metric learning. Next, the contrastive learning between target anchors and hard negative samples forces the optimizer to notice the difference between targets and distractors. Finally, the optimizer will learn a binary output to assist the tracker updating. The proposed optimizer can be easily integrated into other online trackers with little impact on their running speed. Extensive experimental results show that the method achieves state-of-the-art performance on the VOT2018LT, VOT2019LT, OxUvA, and LaSOT benchmarks while running at real-time speed on GPU.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4"
        ],
        "related_topics": [
            "Long-term Tracking",
            "Hard Negative Samples",
            "VOT2019LT",
            "Metric Learning",
            "LaSOT Benchmark",
            "VOT2018LT",
            "Trackers",
            "Contrastive Learning",
            "Positive Sample",
            "OxUvA"
        ],
        "reference_count": "0",
        "citation_count": "71"
    },
    {
        "Id": "86b50babbdee70b8339a1e2f3c3428964ebd8ef9",
        "title": "Device-Free Tracking through Self-Attention Mechanism and Unscented Kalman Filter with Commodity Wi-Fi",
        "authors": [
            "Kabo Poloko Nkabiti",
            "Yueyun Chen"
        ],
        "date": "1 June 2023",
        "abstract": "The proposed approach leverages CSI data collected from commodity Wi-Fi devices and incorporates a combination of the UKF and a sole self-attention mechanism to provide instantaneous and precise estimates of the target\u2019s position while considering factors such as acceleration and network information. Recent advancements in target tracking using Wi-Fi signals and channel state information (CSI) have significantly improved the accuracy and efficiency of tracking mobile targets. However, there remains a gap in developing a comprehensive approach that combines CSI, an unscented Kalman filter (UKF), and a sole self-attention mechanism to accurately estimate the position, velocity, and acceleration of targets in real-time. Furthermore, optimizing the computational efficiency of such approaches is necessary for their applicability in resource-constrained environments. To bridge this gap, this research study proposes a novel approach that addresses these challenges. The approach leverages CSI data collected from commodity Wi-Fi devices and incorporates a combination of the UKF and a sole self-attention mechanism. By fusing these elements, the proposed model provides instantaneous and precise estimates of the target\u2019s position while considering factors such as acceleration and network information. The effectiveness of the proposed approach is demonstrated through extensive experiments conducted in a controlled test bed environment. The results exhibit a remarkable tracking accuracy level of 97%, affirming the model\u2019s ability to successfully track mobile targets. The achieved accuracy showcases the potential of the proposed approach for applications in human-computer interactions, surveillance, and security.",
        "references": [
            "e5365cec7540d84b425aff278c66c6b8946ba809",
            "7eff45a5d62f6abdf939fbb66ef4d1e03c556cee",
            "5c9f448bfb87e9a3db4f2c73a93be271cca0c932",
            "88db67af6974fac3e1ebb9b9e340ba2f6c37b63d",
            "22a70a8228c1872b991825772ec9149739c2a773",
            "58b637c7af3388d6c1095046b94f0c88714d339c",
            "918cd1ced1b146d7bf568a0dc6885981854c5e45",
            "866daf4b59467add6a3318c6bcd5304d4a25a49d",
            "e5cb80b401d4026211fd402eafe1bc21b0cffa52",
            "761ae97017e471bedc63cfdf051ab858a6d56e24"
        ],
        "related_topics": [
            "Unscented Kalman Filter",
            "Channel-state Information",
            "Self-attention Mechanism",
            "Device-Free Tracking",
            "Wi-Fi Signals",
            "Commodity Wi-Fi Devices"
        ],
        "reference_count": "0",
        "citation_count": "66"
    },
    {
        "Id": "31bb6e1f9a91cc6d5980caaf85b1560ada29b499",
        "title": "Object drift determination network based on dual-template joint decision-making in long-term visual tracking",
        "authors": [
            "Zhiqiang Hou",
            "Jiaxin Zhao",
            "Zhuo Wang",
            "Sugang Ma",
            "Wangsheng Yu",
            "JiuLun Fan"
        ],
        "date": "1 December 2023",
        "abstract": "Semantic Scholar extracted view of \"Object drift determination network based on dual-template joint decision-making in long-term visual tracking\" by Zhiqiang Hou et al.",
        "references": [
            "dc03a79ce543fef5cbbedf85c928dde1e92ad711",
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "72cd6429c016e9ae7681247ee4b3ac64e9ce9654",
            "3252a98f639adff305b2524673547d9b8eaf7f6d",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "77944e1564b54d907178d89ccd10211430b44e76",
            "27849a90109b93ec80d190d570041722fb2b0576",
            "3353495c5d2bab6a6a434190ff3b28ad9536a14c",
            "f6186788541d332af19a96183787e01ef9080fb0",
            "253671e7db6b37d7a18c6dd4bb9cc9fb3142dc3f"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "25"
    },
    {
        "Id": "8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
        "title": "Hierarchical memory-guided long-term tracking with meta transformer inquiry network",
        "authors": [
            "Xingmei Wang",
            "Guohao Nie",
            "Boquan Li",
            "Yilin Zhao",
            "Minyang Kang",
            "Bo Liu"
        ],
        "date": "1 March 2023",
        "abstract": "Semantic Scholar extracted view of \"Hierarchical memory-guided long-term tracking with meta transformer inquiry network\" by Xingmei Wang et al.",
        "references": [
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "48fe9bbdc61ab6a7c9922273f527514bcecc0e40",
            "0f29ee600eea84e3826d090322b93b16674450bb",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "27849a90109b93ec80d190d570041722fb2b0576"
        ],
        "related_topics": [
            "Hierarchical Memories",
            "Long-term Tracking"
        ],
        "reference_count": "38",
        "citation_count": "One"
    },
    {
        "Id": "adacccd99a42c3145ec6392a1a6b08878376e38b",
        "title": "High-Performance Long-Term Tracking With Meta-Updater",
        "authors": [
            "Kenan Dai",
            "Yunhua Zhang",
            "Dong Wang",
            "Jianhua Li",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "1 April 2020",
        "abstract": "This work proposes a novel offline-trained Meta-Updater that can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Long-term visual tracking has drawn increasing attention because it is much closer to practical applications than short-term tracking. Most top-ranked long-term trackers adopt the offline-trained Siamese architectures, thus,they cannot benefit from great progress of short-term trackers with online update. However, it is quite risky to straightforwardly introduce online-update-based trackers to solve the long-term problem, due to long-term uncertain and noisy observations. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Our meta-updater learns a binary output to guide the tracker\u2019s update and can be easily embedded into different trackers. This work also introduces a long-term tracking framework consisting of an online local tracker, an online verifier, a SiamRPN-based re-detector, and our meta-updater. Numerous experimental results on the VOT2018LT,VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our tracker performs remarkably better than other competing algorithms. Our project is available on the website: https://github.com/Daikenan/LTMU.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "383e67e0de2fdac787976543ba38bada48d046fc",
            "834baad9db5a1de1bfe993ff4a55a8a957eb9e0a",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "4f445f3e44f2f2ffb431cf1414c59ccba5a0b27d"
        ],
        "related_topics": [
            "Meta-Updater",
            "VOT2019LT",
            "Long-term Tracking",
            "VOT2018LT",
            "Skimming Module",
            "GlobalTrack",
            "Long-term Visual Tracking",
            "Long-Term Tracker",
            "Global Re-detector",
            "SPLT"
        ],
        "reference_count": "57",
        "citation_count": "141"
    },
    {
        "Id": "19d6b9725a59f4b624205829d5f03ac893ca1367",
        "title": "Long-Term Visual Object Tracking Benchmark",
        "authors": [
            "A. Moudgil",
            "Vineet Gandhi"
        ],
        "date": "4 December 2017",
        "abstract": "Existing short sequence benchmarks fail to bring out the inherent differences in tracking algorithms which widen up while tracking on long sequences and the accuracy of trackers abruptly drops on challenging long sequences, suggesting the potential need of research efforts in the direction of long-term tracking. We propose a new long video dataset (called Track Long and Prosper - TLP) and benchmark for single object tracking. The dataset consists of 50 HD videos from real world scenarios, encompassing a duration of over 400 minutes (676K frames), making it more than 20 folds larger in average duration per sequence and more than 8 folds larger in terms of total covered duration, as compared to existing generic datasets for visual tracking. The proposed dataset paves a way to suitably assess long term tracking performance and train better deep learning architectures (avoiding/reducing augmentation, which may not reflect real world behaviour). We benchmark the dataset on 17 state of the art trackers and rank them according to tracking accuracy and run time speeds. We further present thorough qualitative and quantitative evaluation highlighting the importance of long term aspect of tracking. Our most interesting observations are (a) existing short sequence benchmarks fail to bring out the inherent differences in tracking algorithms which widen up while tracking on long sequences and (b) the accuracy of trackers abruptly drops on challenging long sequences, suggesting the potential need of research efforts in the direction of long-term tracking.",
        "references": [
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "1c721511e4c0e21bd264ca71c0d909528511b7ad",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "cdafc80c2d68c727756c9a5b528c86389f67b10b",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"
        ],
        "related_topics": [
            "TLP Dataset",
            "Omni Directional Videos",
            "Sequence",
            "Visual Object Tracking",
            "Long-term Tracking",
            "Augmentations",
            "State-of-the-art Trackers"
        ],
        "reference_count": "47",
        "citation_count": "77"
    },
    {
        "Id": "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
        "title": "Performance Evaluation Methodology for Long-Term Single-Object Tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "2 April 2020",
        "abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed and it is shown that these measures generalize the short-term performance measures, thus linking the two tracking problems. A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term trackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various redetection strategies as well as the influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the future development of long-term trackers.",
        "references": [],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "320d05db95ab42ade69294abe46cd1aca6aca602",
        "title": "High Performance Visual Tracking with Siamese Region Proposal Network",
        "authors": [
            "Bo Li",
            "Junjie Yan",
            "Wei Wu",
            "Zheng Zhu",
            "Xiaolin Hu"
        ],
        "date": "1 June 2018",
        "abstract": "The Siamese region proposal network (Siamese-RPN) is proposed which is end-to-end trained off-line with large-scale image pairs for visual object tracking and consists of SiAMESe subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.",
        "references": [
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "1131c53b9baaa740a4deef4c1282821b23d18687",
            "7ccbb845829234548bfa9b24c61297b4f0cd678e",
            "5404718135548b01516a668e0c022c5cb22b422e",
            "c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "4f445f3e44f2f2ffb431cf1414c59ccba5a0b27d",
            "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "dda27eb7ddc4510f94cac0e5134b5d56aa77b075",
            "5f0850ec47a17f22ba2611a5cb67a30cb02cf306"
        ],
        "related_topics": [
            "Siamese Region Proposal Network",
            "Siamese-RPN",
            "Siamese Subnetwork",
            "Template Branch",
            "Region Proposal Subnetwork",
            "VOT2017",
            "VOT2016",
            "Local One-shot Detection Task",
            "Youtube-BB",
            "Template Patch"
        ],
        "reference_count": "40",
        "citation_count": "1,808"
    },
    {
        "Id": "f1c9f81ce054619f30b5c27fd97579f7216d7048",
        "title": "Robust Long-Term Object Tracking via Improved Discriminative Model Prediction",
        "authors": [
            "Seokeon Choi",
            "Junhyun Lee",
            "Yunsung Lee",
            "Alexander Hauptmann"
        ],
        "date": "11 August 2020",
        "abstract": "The proposed tracker RLT-DiMP improves SuperDiMP in the following three aspects: uncertainty reduction using random erasing, random search with spatio-temporal constraints, and background augmentation for more discriminative feature learning. We propose an improved discriminative model prediction method for robust long-term tracking based on a pre-trained short-term tracker. The baseline pre-trained short-term tracker is SuperDiMP which combines the bounding-box regressor of PrDiMP with the standard DiMP classifier. Our tracker RLT-DiMP improves SuperDiMP in the following three aspects: (1) Uncertainty reduction using random erasing: To make our model robust, we exploit an agreement from multiple images after erasing random small rectangular areas as a certainty. And then, we correct the tracking state of our model accordingly. (2) Random search with spatio-temporal constraints: we propose a robust random search method with a score penalty applied to prevent the problem of sudden detection at a distance. (3) Background augmentation for more discriminative feature learning: We augment various backgrounds that are not included in the search area to train a more robust model in the background clutter. In experiments on the VOT-LT2020 benchmark dataset, the proposed method achieves comparable performance to the state-of-the-art long-term trackers. The source code is available at: this https URL.",
        "references": [
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "834baad9db5a1de1bfe993ff4a55a8a957eb9e0a",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9",
            "6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0"
        ],
        "related_topics": [
            "SuperDiMP",
            "Random Search",
            "Random Erasing",
            "Background Clutter",
            "Long-term Tracking",
            "Discriminative Model Prediction",
            "Background Augmentation",
            "Bounding-box Regressors",
            "Discriminative Feature Learning",
            "PrDiMP"
        ],
        "reference_count": "34",
        "citation_count": "10"
    },
    {
        "Id": "45512d44f1205bc92775f2e880858b3f23c9f5fd",
        "title": "D3S \u2013 A Discriminative Single Shot Segmentation Tracker",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "20 November 2019",
        "abstract": "Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-artTrackers on the TrackingNet. Template-based discriminative trackers are currently the dominant tracking paradigm due to their robustness, but are restricted to bounding box tracking and a limited range of transformation models, which reduces their localization accuracy. We propose a discriminative single-shot segmentation tracker - D3S, which narrows the gap between visual object tracking and video object segmentation. A single-shot network applies two target models with complementary geometric properties, one invariant to a broad range of transformations, including non-rigid deformations, the other assuming a rigid object to simultaneously achieve high robustness and online target segmentation. Without per-dataset finetuning and trained only for segmentation as the primary output, D3S outperforms all trackers on VOT2016, VOT2018 and GOT-10k benchmarks and performs close to the state-of-the-art trackers on the TrackingNet. D3S outperforms the leading segmentation tracker SiamMask on video segmentation benchmark and performs on par with top video object segmentation algorithms, while running an order of magnitude faster, close to real-time.",
        "references": [
            "12fae9a2c1ed867997e1ca70eba271b3c741c42f",
            "d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "f5c5c5a2ae127e3e21c1ea94ccad4c17fd02b914",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "b3249763ac9ecc4df6ef96721c8c7410e0f0468a",
            "8b74008565b575f9ab7a0962ca5f6955d64db045",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "4a70c20ad66e5f3bb12fccd84c63ba619053c811"
        ],
        "related_topics": [
            "SiamMask",
            "Segmentation Tracker",
            "VOT2018",
            "TrackingNet",
            "Video Object Segmentation Benchmarks",
            "Sota Trackers",
            "Segmentation-based Tracker",
            "GOT-10k",
            "Expected Average Overlap",
            "Average Overlap"
        ],
        "reference_count": "52",
        "citation_count": "176"
    },
    {
        "Id": "dc9a66e0f329de8054f4ab845331fb7183987418",
        "title": "RGBD Object Tracking: An In-depth Review",
        "authors": [
            "Jinyu Yang",
            "Zhe Li",
            "Song Yan",
            "Feng Zheng",
            "Alevs Leonardis",
            "Joni-Kristian Kamarainen",
            "Ling Shao"
        ],
        "date": "26 March 2022",
        "abstract": "This paper is the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking, and proposes robustness evaluation against input perturbations. RGBD object tracking is gaining momentum in computer vision research thanks to the development of depth sensors. Although numerous RGBD trackers have been proposed with promising performance, an in-depth review for comprehensive understanding of this area is lacking. In this paper, we firstly review RGBD object trackers from different perspectives, including RGBD fusion, depth usage, and tracking framework. Then, we summarize the existing datasets and the evaluation metrics. We benchmark a representative set of RGBD trackers, and give detailed analyses based on their performances. Particularly, we are the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD tracking videos, we give an analysis of trackers' performance on handling target disappearance. To enable better understanding of RGBD trackers, we propose robustness evaluation against input perturbations. Finally, we summarize the challenges and provide open directions for this community. All resources are publicly available at https://github.com/memoryunreal/RGBD-tracking-review.",
        "references": [
            "f33b4ba5efdef921383bde48ed1ed4edff86edb9",
            "625aec94369715717158843c3ee288869cbe098f",
            "487eb86379e979a72ebfef67db6eb8f048d1d258",
            "6290d7a7e353fbfe77e21e4d1086143f5e66312b",
            "d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "f202feae9ca7b3766e072b6af657beed2236a93c",
            "c06ecdf5b149c322db0381adb6b3fd5ccb31a720",
            "761a9b5d8750eb63a9717650c4aaca53ce36a364",
            "7681f4c80774c6661980c5a76ffab357cca5f5cf",
            "3f02406b9b59d6f966c735953930fede1d751d0d"
        ],
        "related_topics": [
            "Rgb And Depth",
            "RGBD Tracker",
            "Depth Sensor",
            "Robustness Evaluation",
            "Object Tracking",
            "Trackers",
            "Computer Vision",
            "Target Disappearances",
            "Tracking Framework"
        ],
        "reference_count": "75",
        "citation_count": "6"
    },
    {
        "Id": "e80a02ee86f78ed5e0adfcb7f78a13c28cbedf31",
        "title": "Visible-Thermal UAV Tracking: A Large-Scale Benchmark and New Baseline",
        "authors": [
            "Pengyu Zhang",
            "Jie Zhao",
            "D. Wang",
            "Huchuan Lu",
            "Xiang Ruan"
        ],
        "date": "8 April 2022",
        "abstract": "A large-scale benchmark with high diversity for visible-thermal UAV tracking (VTUAV), including 500 sequences with 1.7 million high-resolution (1920* 1080 pixels) frame pairs is constructed, and a coarse-to-fine attribute annotation is provided, where frame-level attributes are provided to exploit the potential of challenge-specific trackers. With the popularity of multi-modal sensors, visible-thermal (RGB-T) object tracking is to achieve robust performance and wider application scenarios with the guidance of objects' temperature information. However, the lack of paired training samples is the main bottleneck for unlocking the power of RGB-T tracking. Since it is laborious to collect high-quality RGB-T sequences, recent benchmarks only provide test sequences. In this paper, we construct a large-scale benchmark with high diversity for visible-thermal UAV tracking (VTUAV), including 500 sequences with 1.7 million high-resolution (1920* 1080 pixels) frame pairs. In addition, comprehensive applications (short-term tracking, long-term tracking and segmentation mask prediction) with diverse categories and scenes are considered for exhaustive evaluation. Moreover, we provide a coarse-to-fine attribute annotation, where frame-level attributes are provided to exploit the potential of challenge-specific trackers. In addition, we design a new RGB-T baseline, named Hierarchical Multi-modal Fusion Tracker (HMFT), which fuses RGB-T data in various levels. Numerous experiments on several datasets are conducted to reveal the effectiveness of HMFT and the complement of different fusion types. The project is available at here.",
        "references": [
            "1975bee228ac228df235d20777e32331bb21566d",
            "d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "27850781e39df9f750e05409b8072261124068e8",
            "ace913cb0f6548324196a649e83dc72ae86997f3",
            "6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "3e4384fa3b599d833bc3e9e2a7815df3236f45b5",
            "e0e50ae9508690ae3a2faf434173f2b382c93320",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "7772b2b1715a3c7d2b726136207776fdff7797ad"
        ],
        "related_topics": [
            "RGB-T Tracking",
            "LasHeR",
            "RGBT234",
            "GTOT",
            "RGB Tracking",
            "RGB-T Trackers",
            "RGB-T",
            "Visible Thermal",
            "UAV Tracking",
            "Categories"
        ],
        "reference_count": "59",
        "citation_count": "41"
    },
    {
        "Id": "6e5fedfa1c232b5821086da936c97955fba0b36a",
        "title": "Deep Triply Attention Network for RGBT Tracking",
        "authors": [
            "Rui Yang",
            "Xiao Wang",
            "Yabin Zhu",
            "Jin Tang"
        ],
        "date": "7 June 2023",
        "abstract": "An effective triply attentive network for robust RGBT tracking is proposed, which consists of a local attention module, a cross-modality co-attention module, and a global attention module based on multi-modal information to compute high-quality global proposals. RGB-Thermal (RGBT) tracking has gained significant attention in the field of computer vision due to its wide range of applications in video surveillance, autonomous driving, and human-computer interaction. This paper focuses on achieving a robust fusion of different modalities for RGBT tracking through attention modeling. We propose an effective triply attentive network for robust RGBT tracking, which consists of a local attention module, a cross-modality co-attention module, and a global attention module. The local attention module enables the tracker to focus on target regions while considering background interference, generated through backpropagation of the score map with respect to the RGB and thermal image pair. To enhance the interaction of different modalities in feature learning, we introduce a co-attention module that selects more discriminative features for both the visible (RGB) and thermal modalities simultaneously. To compensate for the limitations of local sampling, we incorporate a global attention module based on multi-modal information to compute high-quality global proposals. This module not only complements the local search strategy but also re-tracks lost targets when they come back into view. Extensive experiments conducted on three RGBT tracking datasets demonstrate that our proposed method outperforms other RGBT trackers, achieving more competitive results. Specifically, on the LasHeR dataset, the precision rate, normalized precision rate, and success rate reach 57.5%, 51.6%, and 41.0%, respectively. The above state-of-the-art experimental results confirm the effectiveness of our method in exploring the complementary advantages between modalities and achieving robust visual tracking.",
        "references": [
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0",
            "7e06006deb6d0a4b79f319713b7acacc2b7ca3a1",
            "281388c93fcdd7eaf6cb98015b28da09ee2cc071",
            "11841185405c5d17e92110e5067b24e410d5c8de",
            "4bcec82f1fedc9882056b135f842ffed3abf2dc5",
            "e0e50ae9508690ae3a2faf434173f2b382c93320",
            "6460b74428bf4834ff1fffb9a437821c26382310",
            "6c917ba8d703aab8f3bb606fbb04c37b7056fbbb",
            "e7e4cad3a2359c92667d89c8892cc78fbf0c0008",
            "f9d1d33ced795a95a1d23df62c49c93eab46f6de"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "61"
    },
    {
        "Id": "3a5556e4343b4cc21975f089807ca2fc2feafc77",
        "title": "Single-Model and Any-Modality for Video Object Tracking",
        "authors": [
            "Zongwei Wu",
            "Jilai Zheng",
            "Xiangxuan Ren",
            "Florin-Alexandru Vasluianu",
            "Chao Ma",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Radu Timofte"
        ],
        "date": "27 November 2023",
        "abstract": "This work introduces Un-Track, a \\underline{Un}ified Tracker of a single set of parameters for any modality, which surpasses both SOTA unified trackers and modality-specific finetuned counterparts, validating the effectiveness and practicality of this method. In the realm of video object tracking, auxiliary modalities such as depth, thermal, or event data have emerged as valuable assets to complement the RGB trackers. In practice, most existing RGB trackers learn a single set of parameters to use them across datasets and applications. However, a similar single-model unification for multi-modality tracking presents several challenges. These challenges stem from the inherent heterogeneity of inputs -- each with modality-specific representations, the scarcity of multi-modal datasets, and the absence of all the modalities at all times. In this work, we introduce Un-Track, a \\underline{Un}ified Tracker of a single set of parameters for any modality. To handle any modality, our method learns their common latent space through low-rank factorization and reconstruction techniques. More importantly, we use only the RGB-X pairs to learn the common latent space. This unique shared representation seamlessly binds all modalities together, enabling effective unification and accommodating any missing modality, all within a single transformer-based architecture and without the need for modality-specific fine-tuning. Our Un-Track achieves +8.1 absolute F-score gain, on the DepthTrack dataset, by introducing only +2.14 (over 21.50) GFLOPs with +6.6M (over 93M) parameters, through a simple yet efficient prompting strategy. Extensive comparisons on five benchmark datasets with different modalities show that Un-Track surpasses both SOTA unified trackers and modality-specific finetuned counterparts, validating our effectiveness and practicality.",
        "references": [
            "2147cfb8ef36bb938fbae4c9b7c9536ecadac424",
            "7e06006deb6d0a4b79f319713b7acacc2b7ca3a1",
            "0eeefe629de269ed532902aad2c5edd9b5902bf1",
            "7772b2b1715a3c7d2b726136207776fdff7797ad",
            "d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "a0eb66d7b46df875f3e62129a87788351163e0f1",
            "1975bee228ac228df235d20777e32331bb21566d",
            "3b15935df4ddfcb8235ed71cc4d5f2233c390e2c",
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0",
            "bc257bb35ce41286d0d177fa6253cc5ce773b7ef"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "96"
    },
    {
        "Id": "35d238b9a170d7456422f32796ff41cc26f72a57",
        "title": "MFGNet: Dynamic Modality-Aware Filter Generation for RGB-T Tracking",
        "authors": [
            "Xiao Wang",
            "Xiu Shu",
            "Shiliang Zhang",
            "Bo Jiang",
            "Yaowei Wang",
            "Yonghong Tian",
            "Feng Wu"
        ],
        "date": "22 July 2021",
        "abstract": "A new dynamic modality-aware filter generation module (named MFGNet) is proposed to boost the message communication between visible and thermal data by adaptively adjusting the convolutional kernels for various input images in practical tracking. Many RGB-T trackers attempt to attain robust feature representation by utilizing an adaptive weighting scheme (or attention mechanism). Different from these works, we propose a new dynamic modality-aware filter generation module (named MFGNet) to boost the message communication between visible and thermal data by adaptively adjusting the convolutional kernels for various input images in practical tracking. Given the image pairs as input, we first encode their features with the backbone network. Then, we concatenate these feature maps and generate dynamic modality-aware filters with two independent networks. The visible and thermal filters will be used to conduct a dynamic convolutional operation on their corresponding input feature maps respectively. Inspired by residual connection, both the generated visible and thermal feature maps will be summarized with input feature maps. The augmented feature maps will be fed into the RoI align module to generate instance-level features for subsequent classification. To address issues caused by heavy occlusion, fast motion and out-of-view, we propose to conduct a joint local and global search by exploiting a new direction-aware target driven attention mechanism. The spatial and temporal recurrent neural network is used to capture the direction-aware context for accurate global attention prediction. Extensive experiments on three large-scale RGB-T tracking benchmark datasets validated the effectiveness of our proposed algorithm.",
        "references": [
            "7e06006deb6d0a4b79f319713b7acacc2b7ca3a1",
            "11841185405c5d17e92110e5067b24e410d5c8de",
            "f9d1d33ced795a95a1d23df62c49c93eab46f6de",
            "281388c93fcdd7eaf6cb98015b28da09ee2cc071",
            "fc1f0056de711b997d3e942660800e576c08892c",
            "20e503cefe30a8ac9f996f6eebbfa5df3d87112a",
            "4a8ad8a812022fb716bd4a5fd02f4919fb697c45",
            "7772b2b1715a3c7d2b726136207776fdff7797ad",
            "e0e50ae9508690ae3a2faf434173f2b382c93320",
            "50003685fe0d72bd77eb675029da922f55b423bc"
        ],
        "related_topics": [
            "Input Feature-maps",
            "RGB-T Tracking",
            "Instance-level Features",
            "Fast Motion",
            "Backbone Network",
            "Classification",
            "Input Images",
            "Out-of-view",
            "Heavy Occlusion",
            "Residual Connections"
        ],
        "reference_count": "103",
        "citation_count": "17"
    },
    {
        "Id": "73607a8e90897fdb767f3de4d247e56e3cbcff0b",
        "title": "Object Fusion Tracking for RGB-T Images via Channel Swapping and Modal Mutual Attention",
        "authors": [
            "Tian Luan",
            "Hui Zhang",
            "Jiafeng Li",
            "Jing Zhang",
            "Li Zhuo"
        ],
        "date": "1 October 2023",
        "abstract": "A fusion structure based on modal mutual attention is designed, which achieves effective enhancement of RGB-T fusion feature representation by integrating modal self-attention and cross-modal attention. RGB-thermal (RGB-T) dual-modal imaging significantly broadens the observation dimensions of the vision system. However, effectively harnessing the inherent advantages of different spectral bands and establishing fusion solutions tightly coupled with end tasks remains highly challenging. This article proposes a modality fusion approach that combines channel switching and cross-modal attention for RGB-T tracking. We explore the hierarchical fusion method adapted to the deep features of different abstraction levels. For low-level features, cross-modal information is introduced to increase the diversity of unimodal data by swapping feature channels with low computational costs. To exploit the semantic representation of high-level deep features and heterogeneous information in multimodal data, a fusion structure based on modal mutual attention is designed, which achieves effective enhancement of RGB-T fusion feature representation by integrating modal self-attention and cross-modal attention. Experimental results on public datasets show that the proposed algorithm is effective and computationally efficient to obtain the state-of-the-art tracking performance and real-time processing.",
        "references": [
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0",
            "7e06006deb6d0a4b79f319713b7acacc2b7ca3a1",
            "11841185405c5d17e92110e5067b24e410d5c8de",
            "281388c93fcdd7eaf6cb98015b28da09ee2cc071",
            "4bcec82f1fedc9882056b135f842ffed3abf2dc5",
            "52353d3cfe62079bf23738b92be6e34329389aab",
            "6c917ba8d703aab8f3bb606fbb04c37b7056fbbb",
            "e0e50ae9508690ae3a2faf434173f2b382c93320",
            "4a8ad8a812022fb716bd4a5fd02f4919fb697c45",
            "faa161430affd30ccd00be78f60dba32863e5667"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "63"
    },
    {
        "Id": "0b296b4067eaddd872cd2e0a60f4c3d6e95d151f",
        "title": "Dynamic Tracking Aggregation with Transformers for RGB-T Tracking",
        "authors": [
            "Xiaohu Liu",
            "Zhiyong Lei"
        ],
        "date": "2023",
        "abstract": "This study proposes dynamic tracking aggregation (DTA) as a unified framework to perform object detection and data association and obtains fused features based a transformer model and an L1-norm strategy. RGB-thermal (RGB-T) tracking using unmanned aerial vehicles (UAVs) involves challenges with regards to the similarity of objects, occlusion, fast motion, and motion blur, among other issues. In this study, we propose dynamic tracking aggregation (DTA) as a unified framework to perform object detection and data association. The proposed approach obtains fused features based a transformer model and an L1-norm strategy. To link the current frame with recent information, a dynamically updated embedding called dynamic tracking identification (DTID) is used to model the iterative tracking process. For object association, we designed a long short-term tracking aggregation module for dynamic feature propagation to match spatial and temporal embeddings. DTA achieved a highly competitive performance in an experimental evaluation on public benchmark datasets.",
        "references": [
            "e80a02ee86f78ed5e0adfcb7f78a13c28cbedf31",
            "0357156aef567fb5b709222894ddea1ce5d4e721",
            "d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "d49ba5146ab759be3b257228d7095649b3d48b57",
            "e0e50ae9508690ae3a2faf434173f2b382c93320",
            "3e4384fa3b599d833bc3e9e2a7815df3236f45b5",
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0",
            "75a98b6ce3279848c806fc1ad18100bf666b4933",
            "91751fd7f4a16adcf102a72ef6984b6704f8e6ea",
            "ccd97f5fe180b39adccae96f9d3cef39845eaefe"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "26"
    },
    {
        "Id": "cb0dc168bdb8381421ca7324a80781e817e0dd54",
        "title": "RGBT Tracking via Progressive Fusion Transformer with Dynamically Guided Learning",
        "authors": [
            "Yabin Zhu",
            "Chenglong Li",
            "Xiao Wang",
            "Jin Tang",
            "Zhixiang Huang"
        ],
        "date": "26 March 2023",
        "abstract": "A novel Progressive Fusion Transformer called ProFormer is proposed, which progressively integrates single-modality information into the multimodal representation for robust RGBT tracking and sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR, and VTUAV datasets. Existing Transformer-based RGBT tracking methods either use cross-attention to fuse the two modalities, or use self-attention and cross-attention to model both modality-specific and modality-sharing information. However, the significant appearance gap between modalities limits the feature representation ability of certain modalities during the fusion process. To address this problem, we propose a novel Progressive Fusion Transformer called ProFormer, which progressively integrates single-modality information into the multimodal representation for robust RGBT tracking. In particular, ProFormer first uses a self-attention module to collaboratively extract the multimodal representation, and then uses two cross-attention modules to interact it with the features of the dual modalities respectively. In this way, the modality-specific information can well be activated in the multimodal representation. Finally, a feed-forward network is used to fuse two interacted multimodal representations for the further enhancement of the final multimodal representation. In addition, existing learning methods of RGBT trackers either fuse multimodal features into one for final classification, or exploit the relationship between unimodal branches and fused branch through a competitive learning strategy. However, they either ignore the learning of single-modality branches or result in one branch failing to be well optimized. To solve these problems, we propose a dynamically guided learning algorithm that adaptively uses well-performing branches to guide the learning of other branches, for enhancing the representation ability of each branch. Extensive experiments demonstrate that our proposed ProFormer sets a new state-of-the-art performance on RGBT210, RGBT234, LasHeR, and VTUAV datasets.",
        "references": [
            "4bcec82f1fedc9882056b135f842ffed3abf2dc5",
            "f9d1d33ced795a95a1d23df62c49c93eab46f6de",
            "c2f2ed50ec721ca53bd7463cb2c825f277e9d369",
            "ebc0ad0f13d92210bbd2d567e1ab0e0900abe5d0",
            "e7e4cad3a2359c92667d89c8892cc78fbf0c0008",
            "6460b74428bf4834ff1fffb9a437821c26382310",
            "35d238b9a170d7456422f32796ff41cc26f72a57",
            "65b05457a3e9cd963f3e4c6a7f4ff89c03f1abac",
            "0e3d6cf9fe4b1d7770cd4ce409eb7e98b896621b",
            "96b0d333ea77ca85f74015f21f8a89380b092e5a"
        ],
        "related_topics": [
            "Multimodal Representation",
            "ProFormer",
            "Rgb And Thermal",
            "Cross-attention",
            "Transformer",
            "LasHeR",
            "Self-attention",
            "RGBT210",
            "Feed-forward Network",
            "RGBT Tracking"
        ],
        "reference_count": "0",
        "citation_count": "61"
    },
    {
        "Id": "841bd808a62a9d7da7b5075a8da0266297502785",
        "title": "Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking",
        "authors": [
            "Jingxian Sun",
            "Lichao Zhang",
            "Yufei Zha",
            "Abel Gonzalez-Garcia",
            "Peng Zhang",
            "Wei Huang",
            "Yanning Zhang"
        ],
        "date": "31 July 2021",
        "abstract": "The proposed cross-modal distillation method effectively learns TIR-specific target representations transferred from the RGB modality with Cross-Modal Distillation (CMD) on a large amount of unlabeled paired RGB-TIR data. The target representation learned by convolutional neural networks plays an important role in Thermal Infrared (TIR) tracking. Currently, most of the top-performing TIR trackers are still employing representations learned by the model trained on the RGB data. However, this representation does not take into account the information in the TIR modality itself, limiting the performance of TIR tracking. To solve this problem, we propose to distill representations of the TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a large amount of unlabeled paired RGB-TIR data. We take advantage of the two-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal distillation working on two components of the tracker. Specifically, we use one branch as a teacher module to distill the representation learned by the model into the other branch. Benefiting from the powerful model in the RGB modality, the cross-modal distillation can learn the TIR-specific representation for promoting TIR tracking. The proposed approach can be incorporated into different baseline trackers conveniently as a generic and independent component. Furthermore, the semantic coherence of paired RGB and TIR images is utilized as a supervised signal in the distillation loss for cross-modal knowledge transfer. In practice, three different approaches are explored to generate paired RGB-TIR patches with the same semantics for training in an unsupervised way. It is easy to extend to an even larger scale of unlabeled training data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR dataset demonstrate that our proposed cross-modal distillation method effectively learns TIR-specific target representations transferred from the RGB modality. Our tracker outperforms the baseline tracker by achieving absolute gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision respectively. Code and models are available at https://github.com/zhanglichao/cmdTIRtracking.",
        "references": [
            "abfb590702cbe0bcb14b896278faa1a254c79726",
            "ab48606313cb46884a2ab6fd22ea96a6ebb88108",
            "494bd3c8a04b7cc77c92fe0a1a00bd5380a1a088",
            "7a402bf164c0e961bde8c9d0358a64a1ede2810f",
            "3ad57901042546e8b9f21c3c4fb78a984a8a0a25",
            "4c954e814ad6e8624fed1e8b2747a631307813f8",
            "0b8708a9d4a3a72e20386d6647afd9ef57711614",
            "281388c93fcdd7eaf6cb98015b28da09ee2cc071",
            "5a3ada2d268f1b955fa4e3706d53474ded4b6c20",
            "503bafe063e410050c174fcc741e39b3b1e0eb22"
        ],
        "related_topics": [
            "Target Impulse Response",
            "Cross Modal Distillation",
            "TIR Modalities",
            "Normalized Precision",
            "Trackers",
            "Semantic Coherence",
            "DiMP",
            "Thermal Infrared Tracking"
        ],
        "reference_count": "63",
        "citation_count": "13"
    },
    {
        "Id": "d8b96bd1d192dda324250a6fde11be382a2455ff",
        "title": "Thermal Infrared Target Tracking: A Comprehensive Review",
        "authors": [
            "Di Yuan",
            "Haiping Zhang",
            "Xiu Shu",
            "Qiao Liu",
            "Xiaojun Chang",
            "Zhenyu He",
            "Guangming Shi"
        ],
        "date": "2024",
        "abstract": "Thermal infrared (TIR) target tracking task is not affected by illumination changes and can be tracked at night, on rainy days, foggy days, and other extreme weather; so it is widely used in auxiliary driving, unmanned aerial vehicle reconnaissance, video surveillance, and other scenes. However, the TIR target tracking task also presents some challenges, such as intensity change, occlusion, deformation, similarity interference, and so on. These challenges significantly affect the performance of the TIR target tracking methods. To resolve these challenges in the TIR target tracking scenarios, numerous tracking methods have appeared in recent years. The purpose of this article is to give a comprehensive review and summary of the research status of the TIR target tracking methods. We first classify the TIR target tracking methods according to their frameworks and briefly summarize the advantages and disadvantages of different tracking methods, which can better understand the current research progress of the TIR target tracking methods. Next, the public datasets/benchmarks for performance testing of the TIR target tracking methods are introduced. Subsequently, we demonstrate the tracking results of several representative tracking methods on some datasets/benchmarks to more intuitively show the progress of the TIR target tracking methods made in the current research. Finally, we discussed the future research direction in an attempt to promote the better development of the TIR target-tracking tasks.",
        "references": [
            "c643ef83e1a9fb8420b26faadebdc122c500268d",
            "fb129cbc2c753b40999e8faffcae1867d3ee1e35",
            "c97a989e21e453db0c0fbcff46a37225f2b9996c",
            "4415ab6adf726075615c34864309c1f1efc5dcd4",
            "6ecd14681e7894a2adb590f4ce201e45436053d3",
            "9a7c20e18c1489811e29e75499f2953a2114be3e",
            "7c78f89fb80449c862ed28d6253d791675319f9b",
            "e3db2ef428d7b8bf60a4f4a323ce3cfb7c1966c4",
            "9c01e5fc559da8123b4a9e57d5a3eea5e9fbc734",
            "201d116761d9d300193df370107f26d7d475023b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "191"
    },
    {
        "Id": "a4f25f02ca52e1122aa7494244928b8e4684258b",
        "title": "Effective long-term tracking with contrast optimizer",
        "authors": [
            "Yongbo Han",
            "Yitao Liang"
        ],
        "date": "1 July 2023",
        "abstract": "A contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. The main challenge of long-term tracking includes data uncertainty in long-term observations. Previous methods tackle the long-term tracking task by online update-based trackers. However, sophisticated online update strategies of these trackers are usually with a considerable computational burden. In this work, a contrastive learning-based online optimizer-assisted long-term tracking framework (named LTCO) is proposed to guide the online tracker to make more accurate update decisions while reducing the impact of online updates on tracking speed. Specifically, the optimizer first perceives the similarity between distractors and positive samples through metric learning. Next, the contrastive learning between target anchors and hard negative samples forces the optimizer to notice the difference between targets and distractors. Finally, the optimizer will learn a binary output to assist the tracker updating. The proposed optimizer can be easily integrated into other online trackers with little impact on their running speed. Extensive experimental results show that the method achieves state-of-the-art performance on the VOT2018LT, VOT2019LT, OxUvA, and LaSOT benchmarks while running at real-time speed on GPU.",
        "references": [
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4"
        ],
        "related_topics": [
            "Long-term Tracking",
            "Hard Negative Samples",
            "VOT2019LT",
            "Metric Learning",
            "LaSOT Benchmark",
            "VOT2018LT",
            "Trackers",
            "Contrastive Learning",
            "Positive Sample",
            "OxUvA"
        ],
        "reference_count": "0",
        "citation_count": "71"
    },
    {
        "Id": "45e2b30cc94d2f6c7642d9c183a6d8a827fc99f4",
        "title": "Visualizing Skiers' Trajectories in Monocular Videos",
        "authors": [
            "Matteo Dunnhofer",
            "Luca Sordi",
            "Christian Micheloni"
        ],
        "date": "6 April 2023",
        "abstract": "SkiTraVis is proposed, an algorithm to visualize the sequence of points traversed by a skier during its performance and demonstrates the potential of the solution for broadcasting media enhancement and coach assistance. Trajectories are fundamental to winning in alpine skiing. Tools enabling the analysis of such curves can enhance the training activity and enrich broadcasting content. In this paper, we propose SkiTraVis, an algorithm to visualize the sequence of points traversed by a skier during its performance. SkiTraVis works on monocular videos and constitutes a pipeline of a visual tracker to model the skier's motion and of a frame correspondence module to estimate the camera's motion. The separation of the two motions enables the visualization of the trajectory according to the moving camera's perspective. We performed experiments on videos of real-world professional competitions to quantify the visualization error, the computational efficiency, as well as the applicability. Overall, the results achieved demonstrate the potential of our solution for broadcasting media enhancement and coach assistance.",
        "references": [
            "4832a96f8c59d85c98175b84594ae24b7b9ed0ca",
            "4ea19217a73ca7fb5998198e752583947ba447d8",
            "19caa2d6ad3e335954441dca9ff27fb66b50afc2",
            "32b07bcca0f86dc1f9319666fd574f9b85a68250",
            "f13da4ae645a0b5c90ca20272d3513f96e7a136a",
            "c86d81f5ed30c93780e9b0f5dcfffdfa2e1f1cda",
            "ac3844634ca870eff5e3ffd8cdd6bf4f07626dac",
            "a77bf0816b596461bd02bb3a94a14c79698dea40",
            "8342aa4993988f6e4fe030f2b09ff1c9f6196e1a",
            "fbf1a2bbb464071d8bdc5458ffaaa9efdca05c2e"
        ],
        "related_topics": [
            "Monocular Videos",
            "Visual Trackers",
            "Alpine Skiing"
        ],
        "reference_count": "0",
        "citation_count": "70"
    },
    {
        "Id": "8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
        "title": "Hierarchical memory-guided long-term tracking with meta transformer inquiry network",
        "authors": [
            "Xingmei Wang",
            "Guohao Nie",
            "Boquan Li",
            "Yilin Zhao",
            "Minyang Kang",
            "Bo Liu"
        ],
        "date": "1 March 2023",
        "abstract": "Semantic Scholar extracted view of \"Hierarchical memory-guided long-term tracking with meta transformer inquiry network\" by Xingmei Wang et al.",
        "references": [
            "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "2d4713ce1df60f771b65e900fd02352989df82ef",
            "dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
            "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "48fe9bbdc61ab6a7c9922273f527514bcecc0e40",
            "0f29ee600eea84e3826d090322b93b16674450bb",
            "b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "27849a90109b93ec80d190d570041722fb2b0576"
        ],
        "related_topics": [
            "Hierarchical Memories",
            "Long-term Tracking"
        ],
        "reference_count": "38",
        "citation_count": "One"
    },
    {
        "Id": "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
        "title": "Performance Evaluation Methodology for Long-Term Single-Object Tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "2 April 2020",
        "abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed and it is shown that these measures generalize the short-term performance measures, thus linking the two tracking problems. A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term trackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various redetection strategies as well as the influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the future development of long-term trackers.",
        "references": [],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "adacccd99a42c3145ec6392a1a6b08878376e38b",
        "title": "High-Performance Long-Term Tracking With Meta-Updater",
        "authors": [
            "Kenan Dai",
            "Yunhua Zhang",
            "Dong Wang",
            "Jianhua Li",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "1 April 2020",
        "abstract": "This work proposes a novel offline-trained Meta-Updater that can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Long-term visual tracking has drawn increasing attention because it is much closer to practical applications than short-term tracking. Most top-ranked long-term trackers adopt the offline-trained Siamese architectures, thus,they cannot benefit from great progress of short-term trackers with online update. However, it is quite risky to straightforwardly introduce online-update-based trackers to solve the long-term problem, due to long-term uncertain and noisy observations. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Our meta-updater learns a binary output to guide the tracker\u2019s update and can be easily embedded into different trackers. This work also introduces a long-term tracking framework consisting of an online local tracker, an online verifier, a SiamRPN-based re-detector, and our meta-updater. Numerous experimental results on the VOT2018LT,VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our tracker performs remarkably better than other competing algorithms. Our project is available on the website: https://github.com/Daikenan/LTMU.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "383e67e0de2fdac787976543ba38bada48d046fc",
            "834baad9db5a1de1bfe993ff4a55a8a957eb9e0a",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "4f445f3e44f2f2ffb431cf1414c59ccba5a0b27d"
        ],
        "related_topics": [
            "Meta-Updater",
            "VOT2019LT",
            "Long-term Tracking",
            "VOT2018LT",
            "Skimming Module",
            "GlobalTrack",
            "Long-term Visual Tracking",
            "Long-Term Tracker",
            "Global Re-detector",
            "SPLT"
        ],
        "reference_count": "57",
        "citation_count": "141"
    },
    {
        "Id": "bd4f219ce6bc5c22f9da71959d5192cf0b0141fe",
        "title": "Tracking-by-Trackers with a Distilled and Reinforced Model",
        "authors": [
            "Matteo Dunnhofer",
            "Niki Martinel",
            "Christian Micheloni"
        ],
        "date": "8 July 2020",
        "abstract": "This paper proposes a novel tracking methodology that takes advantage of other visual trackers, offline and online, and extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers. Visual object tracking was generally tackled by reasoning independently on fast processing algorithms, accurate online adaptation methods, and fusion of trackers. In this paper, we unify such goals by proposing a novel tracking methodology that takes advantage of other visual trackers, offline and online. A compact student model is trained via the marriage of knowledge distillation and reinforcement learning. The first allows to transfer and compress tracking knowledge of other trackers. The second enables the learning of evaluation measures which are then exploited online. After learning, the student can be ultimately used to build (i) a very fast single-shot tracker, (ii) a tracker with a simple and effective online adaptation mechanism, (iii) a tracker that performs fusion of other trackers. Extensive validation shows that the proposed algorithms compete with real-time state-of-the-art trackers.",
        "references": [
            "eb00b8453b23d4f6f142378e2fb0f0a9e6f9c5e2",
            "1c604a8c6466d40911dc36d2522c315d6bab0f68",
            "4cbe61862bb95fc99293c24d6e02afcb50a05461",
            "f342285b29a207f6918f49b12fd49aa7d9eb0d38",
            "96dc41d3b004fd4c7f96b71b4e174beb3088b2bb",
            "d50cc385d093d84343b8eec4a01612561fa5ee09",
            "61394599ed0aabe04b724c7ca3a778825c7e776f",
            "a73bc57fb0aa429ba5f7f12b6d02e2c6274cabdd",
            "f2aa4dc725821980f39e27dfc23d5a0fbd4be6cf",
            "2ce63d77eecc35faef85a3b752a314c93a077ac9"
        ],
        "related_topics": [
            "Trackers",
            "Visual Object Tracking",
            "Student Model",
            "Knowledge Distillation",
            "Reinforcement Learning",
            "Online Adaptation Mechanism"
        ],
        "reference_count": "85",
        "citation_count": "21"
    },
    {
        "Id": "ca97f741f331b5b43d0577a46c05984f0785a8fa",
        "title": "Is First Person Vision Challenging for Object Tracking?",
        "authors": [
            "Matteo Dunnhofer",
            "Antonino Furnari",
            "Giovanni Maria Farinella",
            "Christian Micheloni"
        ],
        "date": "24 November 2020",
        "abstract": "The study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure, and shows that object tracking in FPV is challenging. Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks.",
        "references": [
            "c4c45661501c16064eead6e5d37dcb80d41c7a78",
            "91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "44990f618f46f02da321b1043a64e72d5f7c0486",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "d1e61fa7824709cae37fb59483dd0772e3101c08",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "61394599ed0aabe04b724c7ca3a778825c7e776f",
            "9559f0b77932a3c5f17aeb8564b400430d173ec7"
        ],
        "related_topics": [
            "TREK-150",
            "First-Person Vision",
            "First-person Video",
            "Object Tracking",
            "Trackers",
            "Camera Wearer",
            "State-of-the-art Trackers",
            "Tracking Scenario",
            "Target Object",
            "Benchmark Dataset"
        ],
        "reference_count": "109",
        "citation_count": "16"
    },
    {
        "Id": "c734274f43575bc5f4bcf8719f0be55a5e89be5e",
        "title": "A Method of Stable Long-Term Single Object Tracking",
        "authors": [
            "Zitong Yi",
            "Zhihang Tong",
            "Yanyun Zhao",
            "Zhicheng Zhao",
            "Fei Su"
        ],
        "date": "5 July 2021",
        "abstract": "An FP-verifier based on feature pools to reevaluate the candidate bounding boxes given by the local tracker and global tracker to ensure that online learning local tracker can be updated stably is proposed. We propose a stable long-term tracking method to deal with visual tracking in multiple complex scenarios to solve the problem of frequent disappearance and reappearance of targets in long-term tracking. In our method, we do not blindly start the global tracker once the target disappears, but use it only when necessary and in reasonable scope with the assistance of localization module. In addition, we designed an FP-verifier based on feature pools to reevaluate the candidate bounding boxes given by our local tracker and global tracker to ensure that online learning local tracker can be updated stably. Our method outperforms the state-of-the-art results on the VOT2020LT challenge. In addition, the control experiments show that our FP-verifier is more effective than the RT-MDNet verifier used by the top three winners of VOT2020LT challenge.",
        "references": [
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "6f7b23893368bd3660086c502f540256c0372ee2",
            "900ab48d25b44c076e31224b7befa503d9550c53"
        ],
        "related_topics": [
            "Local Tracker",
            "Long-term Tracking",
            "State-of-the-art Results",
            "Visual Tracking",
            "Single Object Tracking"
        ],
        "reference_count": "18",
        "citation_count": "2"
    },
    {
        "Id": "09b734072ad4f610478847c9cdc59a4a0c309b37",
        "title": "\u2018Skimming-Perusal\u2019 Tracking: A Framework for Real-Time and Robust Long-Term Tracking",
        "authors": [
            "B. Yan",
            "Haojie Zhao",
            "Dong Wang",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "4 September 2019",
        "abstract": "This work presents a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules, designed to efficiently choose the most possible regions from a large number of sliding windows for image-wide global search. Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "d20d7d3490fd970992b3631048c75a8c5fe2e4e3",
            "e73590fdfd6dab391111bb734053ae24207e2c71",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad"
        ],
        "related_topics": [
            "Perusal Modules",
            "Long-term Tracking",
            "Skimming Module",
            "Skimming-Perusal",
            "SPLT",
            "MaxGM",
            "Global Re-detection",
            "Short-term Tracking",
            "VOT2018LT",
            "Global Re-detector"
        ],
        "reference_count": "43",
        "citation_count": "130"
    },
    {
        "Id": "3a7b1e19efee0ddee65493a9516801ebd201c77d",
        "title": "Predictive Visual Tracking: A New Benchmark and Baseline Approach",
        "authors": [
            "Bowen Li",
            "Yiming Li",
            "Junjie Ye",
            "Changhong Fu",
            "Hang Zhao"
        ],
        "date": "8 March 2021",
        "abstract": "A new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation and can provide a more realistic evaluation of the trackers for the robotic applications. As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers offline and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-of-the-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach.",
        "references": [
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "27850781e39df9f750e05409b8072261124068e8",
            "c29199b0cd3c9b60288a0b726939fa829d6c2a34",
            "6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "0619650ae0f698bcc38244a6858cc270df9dfaad",
            "716bbe9b2643febe903b6fb8b87aab5739dd4617",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d"
        ],
        "related_topics": [],
        "reference_count": "38",
        "citation_count": "5"
    },
    {
        "Id": "0a8f2a341b7d5e486376cfa2ac556bd1360ad61a",
        "title": "EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset",
        "authors": [
            "Hao Tang",
            "Kevin J Liang",
            "Kristen Grauman",
            "Matt Feiszli",
            "Weiyao Wang"
        ],
        "date": "9 January 2023",
        "abstract": "EgoTracks is a new dataset for long-term egocentric visual object tracking, sourced from the Ego4D dataset, and presents a significant challenge to recent state-of-the-art single-object tracking models, which are found to score poorly on traditional tracking metrics for this new dataset, compared to popular benchmarks. Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their\"framed\"nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.",
        "references": [
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "d1e61fa7824709cae37fb59483dd0772e3101c08",
            "fc50c9392fd23b6c88915177c6ae904a498aacea",
            "982cb4421cedce057ae2fc864efac8e43d9c0a5a",
            "9e1f07016fb532800fcaccab03582cb16234b888",
            "ec3d9f304f173362007f5c2e3c6c616d24b85c9f",
            "847a153286d7f6f496f1ff61089831c267d68e30",
            "2a0957d2238a8dca24aa4a276c3370fad671c104",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "5664e24cacf3f6374c26b5597765099ee9537413"
        ],
        "related_topics": [],
        "reference_count": "96",
        "citation_count": "5"
    },
    {
        "Id": "49d25cd7adc77ad36c6aef38d37eb2a7e1a0968b",
        "title": "Instance Tracking in 3D Scenes from Egocentric Videos",
        "authors": [
            "Yunhan Zhao",
            "Haoyu Ma",
            "Shu Kong",
            "Charless C. Fowlkes"
        ],
        "date": "7 December 2023",
        "abstract": "The problem of egocentric instance tracking is made easier by leveraging camera pose and using a 3D allocentric (world) coordinate representation, and this method significantly outperforms SOT-based approaches. Egocentric sensors such as AR/VR devices capture human-object interactions and offer the potential to provide task-assistance by recalling 3D locations of objects of interest in the surrounding environment. This capability requires instance tracking in real-world 3D scenes from egocentric videos (IT3DEgo). We explore this problem by first introducing a new benchmark dataset, consisting of RGB and depth videos, per-frame camera pose, and instance-level annotations in both 2D camera and 3D world coordinates. We present an evaluation protocol which evaluates tracking performance in 3D coordinates with two settings for enrolling instances to track: (1) single-view online enrollment where an instance is specified on-the-fly based on the human wearer's interactions. and (2) multi-view pre-enrollment where images of an instance to be tracked are stored in memory ahead of time. To address IT3DEgo, we first re-purpose methods from relevant areas, e.g., single object tracking (SOT) -- running SOT methods to track instances in 2D frames and lifting them to 3D using camera pose and depth. We also present a simple method that leverages pretrained segmentation and detection models to generate proposals from RGB frames and match proposals with enrolled instance images. Perhaps surprisingly, our extensive experiments show that our method (with no finetuning) significantly outperforms SOT-based approaches. We conclude by arguing that the problem of egocentric instance tracking is made easier by leveraging camera pose and using a 3D allocentric (world) coordinate representation.",
        "references": [
            "b639cf6a3b418544870b565d491f2c967d641ae2",
            "fa67203a3cd2e1a901d4e1a8b6ac60ffff119e55",
            "9e1f07016fb532800fcaccab03582cb16234b888",
            "27942ff872056d886977e465df90d2bf93675cf4",
            "f85812b3a4276a16fc9cd36991c55a1b3d1bfdb7",
            "0a8f2a341b7d5e486376cfa2ac556bd1360ad61a",
            "847a153286d7f6f496f1ff61089831c267d68e30",
            "464890394a85587ca8499f4f56cba0add7dcde9a",
            "2a0b8be3594e8163f9ea4988658223d7c46cfcb3",
            "2ec7f8b7b419a54be652d174f9095b4390e010ac"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "85"
    },
    {
        "Id": "f983a4213fbd4b3504472c7b926cac666e75ef9d",
        "title": "360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking",
        "authors": [
            "Huajian Huang",
            "Yin Xu",
            "Yingshu Chen",
            "Sai-Kit Yeung"
        ],
        "date": "27 July 2023",
        "abstract": "This paper explores 360\u00b0 images for visual object tracking and perceive new challenges caused by large distortion, stitching artifacts, and other unique attributes of 360\u00b0 images, and introduces a general 360 tracking framework that can adopt typical trackers for omnidirectional tracking. 360\u00b0 images can provide an omnidirectional field of view which is important for stable and long-term scene perception. In this paper, we explore 360\u00b0 images for visual object tracking and perceive new challenges caused by large distortion, stitching artifacts, and other unique attributes of 360\u00b0 images. To alleviate these problems, we take advantage of novel representations of target localization, i.e., bounding field-of-view, and then introduce a general 360 tracking framework that can adopt typical trackers for omnidirectional tracking. More importantly, we propose a new large-scale omnidirectional tracking benchmark dataset, 360VOT, in order to facilitate future research. 360VOT contains 120 sequences with up to 113K high-resolution frames in equirectangular projection. The tracking targets cover 32 categories in diverse scenarios. Moreover, we provide 4 types of unbiased ground truth, including (rotated) bounding boxes and (rotated) bounding field-of-views, as well as new metrics tailored for 360\u00b0 images which allow for the accurate evaluation of omnidirectional tracking performance. Finally, we extensively evaluated 20 state-of-the-art visual trackers and provided a new baseline for future comparisons. Homepage: https://360vot.hkustvgd.com",
        "references": [
            "88ba68b2c565b62b900e0f3fb96853a30294b266",
            "0bf689dde6301dba14c8492402157ea31dcce6bd",
            "5648597dc65a3e1fdc6d8e0aeccbf9bf6fe82dcb",
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "ce1da08be62183845cac70b7236ee9de5f2dde43",
            "019ed484b6df5063eed535152fd04f230c1ae786",
            "4e0b250480b34070a850e10f12e13ff80ff8deff",
            "8a0cb93b7a4e0cfe6c8cb6e5cdd0bc199b515ebc",
            "703505a00579c0aa67712836acc41d94fa6d6edc"
        ],
        "related_topics": [],
        "reference_count": "60",
        "citation_count": "One"
    },
    {
        "Id": "a7e7659c84e257bafe69f6934ff37b728f0a0775",
        "title": "EXOT: Exit-aware Object Tracker for Safe Robotic Manipulation of Moving Object",
        "authors": [
            "Hyunseo Kim",
            "Hye Jung Yoon",
            "Minji Kim",
            "Dong-Sig Han",
            "Byoung-Tak Zhang"
        ],
        "date": "29 May 2023",
        "abstract": "The EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation is proposed for safe robot operation and is the first approach of applying an out-of-distribution classification technique to a tracker output. Current robotic hand manipulation narrowly operates with objects in predictable positions in limited environments. Thus, when the location of the target object deviates severely from the expected location, a robot sometimes responds in an unexpected way, especially when it operates with a human. For safe robot operation, we propose the EXit-aware Object Tracker (EXOT) on a robot hand camera that recognizes an object's absence during manipulation. The robot decides whether to proceed by examining the tracker's bounding box output containing the target object. We adopt an out-of-distribution classifier for more accurate object recognition since trackers can mistrack a background as a target object. To the best of our knowledge, our method is the first approach of applying an out-of-distribution classification technique to a tracker output. We evaluate our method on the first-person video benchmark dataset, TREK-150, and on the custom dataset, RMOT-223, that we collect from the UR5e robot. Then we test our tracker on the UR5e robot in real-time with a conveyor-belt sushi task, to examine the tracker's ability to track target dishes and to determine the exit status. Our tracker shows 38% higher exit-aware performance than a baseline method. The dataset and the code will be released at https://github.com/hskAlena/EXOT.",
        "references": [
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "900ab48d25b44c076e31224b7befa503d9550c53",
            "0e3a7035672d19abd71f72c612be0d1caeecb63c",
            "646a669f1dc38ae961fe41fbd3c83cab64ce9d53",
            "fe25d4c99e39ce28107b56adaa3b5a43219a3a8a",
            "1c7e078611c9df412e6eb3a356f31a0da0c1f99c",
            "3ac59cc39f4962b85eedcdc81d5126f4704563ab",
            "ac0d88ca5f75a4a80da90365c28fa26f1a26d4c4",
            "962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "9f69382b0112b5467b8118eafaade5d30720c3b2"
        ],
        "related_topics": [
            "Target Object",
            "Object Recognition",
            "TREK-150",
            "Trackers"
        ],
        "reference_count": "0",
        "citation_count": "32"
    },
    {
        "Id": "dcc0d2d3a9445d22771a73cc6e2fb2a2a3308db9",
        "title": "PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework",
        "authors": [
            "Bowen Li",
            "Ziyuan Huang",
            "Junjie Ye",
            "Yiming Li",
            "Sebastian A. Scherer",
            "Hang Zhao",
            "Changhong Fu"
        ],
        "date": "21 November 2022",
        "abstract": "Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency. Visual object tracking is essential to intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and onboard computation is limited, the latency issue can be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency. Our code is public at https: //github.com/Jaraxxus-Me/PVT_pp.git.",
        "references": [
            "3a7b1e19efee0ddee65493a9516801ebd201c77d",
            "4ef52feb1997b1a71f1ca4d49f72a5ce4d43a8b0",
            "894e4376750b83b63649cc518b121f345ca0df83",
            "689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "0619650ae0f698bcc38244a6858cc270df9dfaad",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "921901edc30c30554dc78cab724d06ea9097389f",
            "9269b52994d8af23dc17dfbc225cd25c0902686c",
            "320d05db95ab42ade69294abe46cd1aca6aca602"
        ],
        "related_topics": [],
        "reference_count": "67",
        "citation_count": "One"
    },
    {
        "Id": "58bc1f07784089ed3c49c93bd835e8b73af5ff70",
        "title": "Missing Person Detection Using AI",
        "authors": [
            "Dhanush M S",
            "Thenmozhi M"
        ],
        "date": "23 December 2022",
        "abstract": "The main takeaway is that the model's depth is crucial to its excellent performance, which is computationally expensive but is made possible by the use of graphics processing units (GPUs) during training. The focus of the planned study is on finding lost people in crowded environments including public events, festivals, temples, and meetings. In today's busy environments, single-person identification is a challenging endeavor. This problem is addressed by applying a deep learning idea to arrive at a workable solution. Individuals are recognized through the use of a Convolutional Neural Network (CNN). Several face characteristics are used to positively identify the missing person. The use of Face Detection is crucial to the success of this endeavor. The ImageNet Large-Scale Visual Recognition Challenge participant AlexNet is used. The main takeaway is that the model's depth is crucial to its excellent performance, which is computationally expensive but is made possible by the use of graphics processing units (GPUs) during training. With sufficient training using a wide variety of images, it is possible to locate the sought-after object in the allotted space. The project's momentum is based on watching the live feed. Faces are extracted from the video and saved to a database. A collection of photos is available for use in making identifications. We use our own dataset to train the AlexNet's many layers. The images in the database are labelled according to whether or not they contain a person by utilizing the pretrained network. Additionally, the KLT method should be used to determine the person's location and enable real-time tracking.",
        "references": [
            "b0a146f216c6fba23e58d1345374632dbb23cc11",
            "835c549303a256abde0d8fae147e3822e4cd0091",
            "4b36e3bf8d95dee27706faa466aedcc575a9c259",
            "6bda65c75fb9f73678972e540599b170756fe823",
            "f8e50a29f016c90b948a36153799ff3f1927b95c",
            "ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "1beab7fd9ef7382bd6a896183f05c7efa039ff96",
            "521e288f82fa348dde19081c5d5e1a07e84a4606",
            "8f6272b5ca57b0b3c449b73d7109f68a28d7ff9d",
            "8cdb3279f8b29b6a19c228af02e8ee1deb591e21"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "23"
    },
    {
        "Id": "0fd84fa8f5a24fa55c1e1fe35bf81e2126b23225",
        "title": "MECCANO: A Multimodal Egocentric Dataset for Humans Behavior Understanding in the Industrial-like Domain",
        "authors": [
            "Francesco Ragusa",
            "Antonino Furnari",
            "Giovanni Maria Farinella"
        ],
        "date": "19 September 2022",
        "abstract": "Semantic Scholar extracted view of \"MECCANO: A Multimodal Egocentric Dataset for Humans Behavior Understanding in the Industrial-like Domain\" by Francesco Ragusa et al.",
        "references": [
            "a58a0732664b97b471b795df5812f98f24840490",
            "b67282a73c79438095799de916bf44ae55f1d310",
            "fc50c9392fd23b6c88915177c6ae904a498aacea",
            "355f769cc896ff3ea302423587c9a1b7c2301c4e",
            "00a6ee03c2a8d22644513f1e983d5159197b201a",
            "97429331740c98c6d03d59f63789db183784c79b",
            "20d89f07ca06098cc4a0aa30f27fbf93cae802cd",
            "07cf6c4c1714a0cd88e5c1566aac9df40e111db7",
            "6021af236342c11c44f681d2aa21b0b46756236a",
            "847a153286d7f6f496f1ff61089831c267d68e30"
        ],
        "related_topics": [
            "MECCANO",
            "Depth Map",
            "Active Object Detection",
            "RGB Videos",
            "Egocentric Dataset",
            "Action Recognition",
            "Tasks",
            "Third Person Vision",
            "Action Anticipation",
            "Wearable Camera"
        ],
        "reference_count": "107",
        "citation_count": "11"
    },
    {
        "Id": "e85e8c5851ab1cc44ea6dd2d3d4b5af9789db9af",
        "title": "Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking",
        "authors": [
            "Reza Jalil Mozhdehi",
            "Henry Medeiros"
        ],
        "date": "7 July 2021",
        "abstract": "Semantic Scholar extracted view of \"Deep Convolutional Correlation Iterative Particle Filter for Visual Tracking\" by Reza Jalil Mozhdehi et al.",
        "references": [
            "85a1ec62bbe3c781cafaeeb68662cfa76a4a3493",
            "358ead99d9c9237a361929a2530b2d8526929c9b",
            "10d0b3786bcce043f9ba8d42fad19283b4625754",
            "0cb3ba7772f95e5d55b13ad592d16c0eef946217",
            "6577d4f195fc14cba23d2d8a91c6961693d190a3",
            "000178cd12c8a6e5da8215b6365fae03c20fd18d",
            "d34ab5269af57e3f57c7b8e41ad5bb51b82cd49f",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "dda1be806ab56ca58187621a0c2e4d2b8ad429ac",
            "5c8a6874011640981e4103d120957802fa28f004"
        ],
        "related_topics": [
            "Iterative Particle Filter",
            "Visual Tracking",
            "Resampling",
            "Posterior Distributions",
            "Prior Distribution",
            "Correlation Filters",
            "Benchmark Dataset"
        ],
        "reference_count": "81",
        "citation_count": "7"
    },
    {
        "Id": "adacccd99a42c3145ec6392a1a6b08878376e38b",
        "title": "High-Performance Long-Term Tracking With Meta-Updater",
        "authors": [
            "Kenan Dai",
            "Yunhua Zhang",
            "Dong Wang",
            "Jianhua Li",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "1 April 2020",
        "abstract": "This work proposes a novel offline-trained Meta-Updater that can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Long-term visual tracking has drawn increasing attention because it is much closer to practical applications than short-term tracking. Most top-ranked long-term trackers adopt the offline-trained Siamese architectures, thus,they cannot benefit from great progress of short-term trackers with online update. However, it is quite risky to straightforwardly introduce online-update-based trackers to solve the long-term problem, due to long-term uncertain and noisy observations. In this work, we propose a novel offline-trained Meta-Updater to address an important but unsolved problem: Is the tracker ready for updating in the current frame? The proposed meta-updater can effectively integrate geometric, discriminative, and appearance cues in a sequential manner, and then mine the sequential information with a designed cascaded LSTM module. Our meta-updater learns a binary output to guide the tracker\u2019s update and can be easily embedded into different trackers. This work also introduces a long-term tracking framework consisting of an online local tracker, an online verifier, a SiamRPN-based re-detector, and our meta-updater. Numerous experimental results on the VOT2018LT,VOT2019LT, OxUvALT, TLP, and LaSOT benchmarks show that our tracker performs remarkably better than other competing algorithms. Our project is available on the website: https://github.com/Daikenan/LTMU.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "50c60583dc0ef09484358deab329f82ee22c2b66",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "383e67e0de2fdac787976543ba38bada48d046fc",
            "834baad9db5a1de1bfe993ff4a55a8a957eb9e0a",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "4f445f3e44f2f2ffb431cf1414c59ccba5a0b27d"
        ],
        "related_topics": [
            "Meta-Updater",
            "VOT2019LT",
            "Long-term Tracking",
            "VOT2018LT",
            "Skimming Module",
            "GlobalTrack",
            "Long-term Visual Tracking",
            "Long-Term Tracker",
            "Global Re-detector",
            "SPLT"
        ],
        "reference_count": "57",
        "citation_count": "141"
    },
    {
        "Id": "c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
        "title": "Performance Evaluation Methodology for Long-Term Single-Object Tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "2 April 2020",
        "abstract": "A long-term visual object tracking performance evaluation methodology and a benchmark are proposed and it is shown that these measures generalize the short-term performance measures, thus linking the two tracking problems. A long-term visual object tracking performance evaluation methodology and a benchmark are proposed. Performance measures are designed by following a long-term tracking definition to maximize the analysis probing strength. The new measures outperform existing ones in interpretation potential and in better distinguishing between different tracking behaviors. We show that these measures generalize the short-term performance measures, thus linking the two tracking problems. Furthermore, the new measures are highly robust to temporal annotation sparsity and allow annotation of sequences hundreds of times longer than in the current datasets without increasing manual annotation labor. A new challenging dataset of carefully selected sequences with many target disappearances is proposed. A new tracking taxonomy is proposed to position trackers on the short-term/long-term spectrum. The benchmark contains an extensive evaluation of the largest number of long-term trackers and comparison to state-of-the-art short-term trackers. We analyze the influence of tracking architecture implementations to long-term performance and explore various redetection strategies as well as the influence of visual model update strategies to long-term tracking drift. The methodology is integrated in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the future development of long-term trackers.",
        "references": [],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "b47943161a0cefb8963ad0a7830e51c396bff3b1",
        "title": "Effective Local and Global Search for Fast Long-Term Tracking",
        "authors": [
            "Haojie Zhao",
            "B. Yan",
            "D. Wang",
            "Xuesheng Qian",
            "Xiaoyun Yang",
            "Huchuan Lu"
        ],
        "date": "23 February 2022",
        "abstract": "A novel robust and real-time long-term tracking framework based on the proposed local search module and re-detection module that can estimate the target position and target size for a given base tracker and is flexibly integrated into many other tracking algorithms for long- term tracking. Compared with short-term tracking, long-term tracking remains a challenging task that usually requires the tracking algorithm to track targets within a local region and re-detect targets over the entire image. However, few works have been done and their performances have also been limited. In this paper, we present a novel robust and real-time long-term tracking framework based on the proposed local search module and re-detection module. The local search module consists of an effective bounding box regressor to generate a series of candidate proposals and a target verifier to infer the optimal candidate with its confidence score. For local search, we design a long short-term updated scheme to improve the target verifier. The verification capability of the tracker can be improved by using several templates updated at different times. Based on the verification scores, our tracker determines whether the tracked object is present or absent and then chooses the tracking strategies of local or global search, respectively, in the next frame. For global re-detection, we develop a novel re-detection module that can estimate the target position and target size for a given base tracker. We conduct a series of experiments to demonstrate that this module can be flexibly integrated into many other tracking algorithms for long-term tracking and that it can improve long-term tracking performance effectively. Numerous experiments and discussions are conducted on several popular tracking datasets, including VOT, OxUvA, TLP, and LaSOT. The experimental results demonstrate that the proposed tracker achieves satisfactory performance with a real-time speed. Code is available at https://github.com/difhnp/ELGLT.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "d20d7d3490fd970992b3631048c75a8c5fe2e4e3",
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "d3d36c3caa255053877a7e3250d47d906eec81d2",
            "913cebc279c363fb9476496f096519e27212b3d5",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "c4c45661501c16064eead6e5d37dcb80d41c7a78"
        ],
        "related_topics": [
            "Long-term Tracking",
            "Re-detection Module",
            "Local Search Module",
            "Tracking Dataset",
            "Visual Object Tracking",
            "Bounding-box Regressors",
            "Confidence Scores",
            "Large-scale Single Object Tracking",
            "Candidate Proposals",
            "Local Regions"
        ],
        "reference_count": "62",
        "citation_count": "9"
    },
    {
        "Id": "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
        "title": "Now you see me: evaluating performance in long-term visual tracking",
        "authors": [
            "Alan Luke{\\vz}i{\\vc}",
            "Luka Cehovin Zajc",
            "Tom{\\&#x27;a}s Voj{\\&#x27;i}r",
            "Jiri Matas",
            "Matej Kristan"
        ],
        "date": "19 April 2018",
        "abstract": "An extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluatinglong-term tracking - tracking precision, recall and F-score shows that a good model update strategy and the capability of image-wide re-detection are critical for long- term tracking performance. We propose a new long-term tracking performance evaluation methodology and present a new challenging dataset of carefully selected sequences with many target disappearances. We perform an extensive evaluation of six long-term and nine short-term state-of-the-art trackers, using new performance measures, suitable for evaluating long-term tracking - tracking precision, recall and F-score. The evaluation shows that a good model update strategy and the capability of image-wide re-detection are critical for long-term tracking performance. We integrated the methodology in the VOT toolkit to automate experimental analysis and benchmarking and to facilitate the development of long-term trackers.",
        "references": [
            "19d6b9725a59f4b624205829d5f03ac893ca1367",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "cdafc80c2d68c727756c9a5b528c86389f67b10b",
            "0c7c61e2d85081bc4c63556f41d7bc71fdf0f5ac",
            "3c74b636c0f74c1a0cbbd6e165c2760264044971",
            "a6ee08763d994b1687c594bb9367f8d5cf419113",
            "d3d36c3caa255053877a7e3250d47d906eec81d2"
        ],
        "related_topics": [
            "Target Disappearances",
            "Tracking Recall",
            "Long-Term Tracker",
            "LTB35",
            "FCLT",
            "Short-term Trackers",
            "VOT Toolkit",
            "Omni Directional Videos",
            "Re-Detection",
            "TLP Dataset"
        ],
        "reference_count": "36",
        "citation_count": "57"
    },
    {
        "Id": "50c60583dc0ef09484358deab329f82ee22c2b66",
        "title": "Meta-Tracker: Fast and Robust Online Adaptation for Visual Object Trackers",
        "authors": [
            "Eunbyung Park",
            "Alexander C. Berg"
        ],
        "date": "9 January 2018",
        "abstract": "This paper improves state-of-the-art visual object trackers that use online adaptation by using an offline meta-learning-based method to adjust the initial deep networks used in online adaptation-based tracking. This paper improves state-of-the-art visual object trackers that use online adaptation. Our core contribution is an offline meta-learning-based method to adjust the initial deep networks used in online adaptation-based tracking. The meta learning is driven by the goal of deep networks that can quickly be adapted to robustly model a particular target in future frames. Ideally the resulting models focus on features that are useful for future frames, and avoid overfitting to background clutter, small parts of the target, or noise. By enforcing a small number of update iterations during meta-learning, the resulting networks train significantly faster. We demonstrate this approach on top of the high performance tracking approaches: tracking-by-detection based MDNet and the correlation based CREST. Experimental results on standard benchmarks, OTB2015 and VOT2016, show that our meta-learned versions of both trackers improve speed, accuracy, and robustness.",
        "references": [
            "dda27eb7ddc4510f94cac0e5134b5d56aa77b075",
            "0b104e517e0440e3bdace01b5f6706c5fa944149",
            "5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "61394599ed0aabe04b724c7ca3a778825c7e776f",
            "c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "db39754bde43c5555d7086261d1a6fd55af7de06",
            "6179ac06f1a8fd1ac6b693b02824948dff438d54",
            "c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "1b3a107739e7f7e05c50999a3d79b8225746f662"
        ],
        "related_topics": [
            "Meta-Tracker",
            "Deep Network",
            "Future Frames",
            "Target Appearance Variation",
            "Training Signals",
            "Deep Learning",
            "Online Tracking Benchmark",
            "Object Tracking Benchmark",
            "Visual Object Trackers",
            "Tracking By Detection"
        ],
        "reference_count": "58",
        "citation_count": "158"
    },
    {
        "Id": "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
        "title": "Global Tracking via Ensemble of Local Trackers",
        "authors": [
            "Zikun Zhou",
            "Jianqiu Chen",
            "Wenjie Pei",
            "Kaige Mao",
            "Hongpeng Wang",
            "Zhenyu He"
        ],
        "date": "30 March 2022",
        "abstract": "This work combines the advantages of both strategies: tracking the target in a global view while exploiting the temporal context; and performs global tracking via ensemble of local trackers spreading the full image. The crux of long-term tracking lies in the difficulty of tracking the target with discontinuous moving caused by out-of-view or occlusion. Existing long-term tracking methods follow two typical strategies. The first strategy employs a local tracker to perform smooth tracking and uses another re-detector to detect the target when the target is lost. While it can exploit the temporal context like historical appearances and locations of the target, a potential limitation of such strategy is that the local tracker tends to misidentify a nearby distractor as the target instead of activating the re-detector when the real target is out of view. The other long-term tracking strategy tracks the target in the entire image globally instead of local tracking based on the previous tracking results. Unfortunately, such global tracking strategy cannot leverage the temporal context effectively. In this work, we combine the advantages of both strategies: tracking the target in a global view while exploiting the temporal context. Specifically, we perform global tracking via ensemble of local trackers spreading the full image. The smooth moving of the target can be handled steadily by one local tracker. When the local tracker accidentally loses the target due to suddenly discontinuous moving, another local tracker close to the target is then activated and can readily take over the tracking to locate the target. While the activated local tracker performs tracking locally by leveraging the temporal context, the ensemble of local trackers renders our model the global view for tracking. Extensive experiments on six datasets demonstrate that our method performs favorably against state-of-the-art algorithms.",
        "references": [
            "5664e24cacf3f6374c26b5597765099ee9537413",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "09b734072ad4f610478847c9cdc59a4a0c309b37",
            "48c6ca17f17038ff9933dca86a23f8516168a3ea",
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "ae066f27f2edc1c51847ce4cb21b6e1a3db44fa2",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "adacccd99a42c3145ec6392a1a6b08878376e38b",
            "811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "16cf8225d1b86c54a18b917a9475bfbd68b46306"
        ],
        "related_topics": [
            "Local Tracker",
            "Long-term Tracking",
            "Re-detector",
            "Historical Appearances",
            "Ensemble",
            "Out-of-view"
        ],
        "reference_count": "49",
        "citation_count": "13"
    },
    {
        "Id": "09b734072ad4f610478847c9cdc59a4a0c309b37",
        "title": "\u2018Skimming-Perusal\u2019 Tracking: A Framework for Real-Time and Robust Long-Term Tracking",
        "authors": [
            "B. Yan",
            "Haojie Zhao",
            "Dong Wang",
            "Huchuan Lu",
            "Xiaoyun Yang"
        ],
        "date": "4 September 2019",
        "abstract": "This work presents a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules, designed to efficiently choose the most possible regions from a large number of sliding windows for image-wide global search. Compared with traditional short-term tracking, long-term tracking poses more challenges and is much closer to realistic applications. However, few works have been done and their performance have also been limited. In this work, we present a novel robust and real-time long-term tracking framework based on the proposed skimming and perusal modules. The perusal module consists of an effective bounding box regressor to generate a series of candidate proposals and a robust target verifier to infer the optimal candidate with its confidence score. Based on this score, our tracker determines whether the tracked object being present or absent, and then chooses the tracking strategies of local search or global search respectively in the next frame. To speed up the image-wide global search, a novel skimming module is designed to efficiently choose the most possible regions from a large number of sliding windows. Numerous experimental results on the VOT-2018 long-term and OxUvA long-term benchmarks demonstrate that the proposed method achieves the best performance and runs in real-time. The source codes are available at https://github.com/iiau-tracker/SPLT.",
        "references": [
            "3d372b63020c4d2c9510624f370b50d9f292bcde",
            "3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "219e9a4527110baf1feb3df20db12064eeafdfb7",
            "d20d7d3490fd970992b3631048c75a8c5fe2e4e3",
            "e73590fdfd6dab391111bb734053ae24207e2c71",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad"
        ],
        "related_topics": [
            "Perusal Modules",
            "Long-term Tracking",
            "Skimming Module",
            "Skimming-Perusal",
            "SPLT",
            "MaxGM",
            "Global Re-detection",
            "Short-term Tracking",
            "VOT2018LT",
            "Global Re-detector"
        ],
        "reference_count": "43",
        "citation_count": "130"
    },
    {
        "Id": "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
        "title": "Learning Regression and Verification Networks for Robust Long-term Tracking",
        "authors": [
            "Yunhua Zhang",
            "Lijun Wang",
            "Dong Wang",
            "Jinqing Qi",
            "Huchuan Lu"
        ],
        "date": "18 June 2021",
        "abstract": "A new visual tracking algorithm is proposed, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking and achieves state-of-the-art performance on the OxUvA long- term tracking dataset. This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.",
        "references": [
            "09769e80cdf027db32a1fcb695a1aa0937214763",
            "c63a34ac6a4e049118070e707ca7679fbb132d33",
            "c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "2ce63d77eecc35faef85a3b752a314c93a077ac9",
            "bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "e73590fdfd6dab391111bb734053ae24207e2c71",
            "3f0da079ac950a4dfb699c41a90c087000e6ac38",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1"
        ],
        "related_topics": [
            "Verification Networks",
            "Target Appearance",
            "Long-term Tracking",
            "Regression",
            "Target Candidates",
            "Long-term Tracking Challenge"
        ],
        "reference_count": "38",
        "citation_count": "10"
    },
    {
        "Id": "0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
        "title": "SiamDA: distribution-aware Siamese network for visual tracking",
        "authors": [
            "Qiuhan Ji",
            "Hong-bo Shi",
            "Shuai Tan",
            "Bing Song",
            "Yang Tao"
        ],
        "date": "1 November 2022",
        "abstract": "An efficient Siamese distribution-aware (DA) anchor-free (SiamDA) network for object tracking that is modeled by a flexible general distribution, avoiding the ambiguity of traditional rectangle representation in some complex scenes, such as occlusion and background clutter. Abstract. Siamese-based trackers have achieved remarkable performance and widespread applications in visual tracking. However, most of the existing Siamese trackers are usually restricted by the difficulty of handling the misalignment between target localization and state estimation. To address the misalignment issue, we propose an efficient Siamese distribution-aware (DA) anchor-free (SiamDA) network for object tracking. First, the bounding box of regression targets is modeled by a flexible general distribution, avoiding the ambiguity of traditional rectangle representation in some complex scenes, such as occlusion and background clutter. On this basis, to eliminate the misalignment issue, a DA module is designed to connect the target localization and state estimation. In addition, the Pseudo-Intersection over Union sample assignment rule provides our anchor-free tracker with high-quality samples effectively during the training process, making it possible to further improve the tracking performance. Extensive experiments on official visual tracking benchmarks demonstrate the effectiveness and efficiency of SiamDA.",
        "references": [
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "4b6a190bb897a626639849c09426d30d5a1462c8",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "738165f33c50b059e87b14d8b4a129230e14eacd",
            "5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "d81dc62436e93135bc5d4a13b2856a2db4f815b3"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "49"
    },
    {
        "Id": "0a17d476e9543756e09b9c3240398af2f7f5af9f",
        "title": "Siamese Attention Networks with Adaptive Templates for Visual Tracking",
        "authors": [
            "Bo Zhang",
            "Zhixue Liang",
            "Wenyong Dong"
        ],
        "date": "19 September 2022",
        "abstract": "A visual tracking framework with adaptive template update and spatiotemporal attention, named SiamAttnAT, which achieves the outstanding performance with a considerable real-time speed and applies the proposed mechanisms to the employed baseline SiamRPN++. Visual object tracking takes an important role in realistic applications, such as video understanding, unmanned auto vehicles, and autonomous robots. Although the Siamese-based tracker has achieved good performance in tracking tasks, the existing methods using initial template or updating template with simple strategy result in the performance degradation of the model when the target varies in realistic scenarios such as target occlusion, scale variation, and deformation. In this paper, we propose a visual tracking framework with adaptive template update and spatiotemporal attention, named SiamAttnAT. Specially, we propose a historical template selecting strategy and a template adaptively generating method for robust tracking. In addition, we apply the proposed mechanisms to the employed baseline SiamRPN++. Extensive experiments and comparisons with state-of-the-art trackers on short-term and long-term visual tracking benchmarks including VOT2018, OTB-100, UAV123, NFS, and LaSOT show that the proposed framework achieves the outstanding performance with a considerable real-time speed, verifying its efficiency and effectiveness.",
        "references": [
            "5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "320d05db95ab42ade69294abe46cd1aca6aca602",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "157ff6e216985911cc2f9775155d2a424ba2984b",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "d1a4135a2edd1af8a1e501109bbf7c2c720f10f8"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "36"
    },
    {
        "Id": "c7d39e7be53dd2538d60db48cc47cae177a554a1",
        "title": "Robust Tracking for Visual Complex Environments",
        "authors": [
            ""
        ],
        "date": "2022",
        "abstract": "Experimental results show that the algorithm in this paper can effectively cope with different challenges of target tracking in complex environments, robustly tracking the target while maintaining high accuracy. A BSTRACT Achieving accurate tracking and robust tracking in visually complex scenes remains a challenging task. This requires to ensure that a robust appearance representation is captured while improving the generalization ability of the model to cope with challenges such as object deformation, illumination changes, scale changes, and mo- tion blur. In this paper, we propose a robust tracking technique in complex tracking scenarios based on efficient convolution operator (ECO) tracker. It adopts two-fold ideas: a) extract deep features using the Conformer network after expanding the number of under- lying channels, and b) adaptively adjust the fusion weight of shallow features and deep features according to the peak to sidelobe ratio and the joint score of adjacent frame trajectory smoothness. By doing so, the generalization ability of the tracking model and its adaptabil- ity in complex environments are improved, while making full use of the complementarity of deeper-layer and shallow-layer features. Experimental results show that the algorithm in this paper can effectively cope with different challenges of target tracking in complex environments, robustly tracking the target while maintaining high accuracy.",
        "references": [
            "5c8a6874011640981e4103d120957802fa28f004",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "a5278fc76eff08668bc1957b01b22eb627fa2c36",
            "0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
            "0eb1b75b98d4f4f69a5fb7669ad86d85cdd76848",
            "f233c16a87d518bfe9f923ea7af48ed3eb6bb7d5",
            "70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "ece7625a346edbc5f6fab541c0c246ec06939121",
            "09769e80cdf027db32a1fcb695a1aa0937214763",
            "311bc4e48838d8e5ef619df3ce0bc598aba788a1"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "48"
    },
    {
        "Id": "5bc33c1a2c7fbb3badd59a5584570df6f1acab1b",
        "title": "Learning rich feature representation and aggregation for accurate visual tracking",
        "authors": [
            "Yijin Yang",
            "Xiaodong Gu"
        ],
        "date": "22 September 2023",
        "abstract": "A novel feature representation and aggregation network is proposed and introduced into the tracking-by-segmentation framework to extract and integrate rich features for accurate and robust segmentation tracking and results show that the proposed tracker outperforms most of the state-of-the-art trackers and achieves very promising tracking performance on the OTB100 and VOT2018 benchmarks. Visual tracking is a key component of computer vision and has a wide range of practical applications. Recently, the tracking-by-segmentation framework has been widely applied in visual tracking due to its astonishing performance on accuracy. It attempts to learn from the framework of video object segmentation to realize accurate tracking. Although segmentation-based trackers are effective for target scale estimation, the segmentation network makes the trackers have high requirements for the extracted target features due to the need for pixel-level segmentation. Therefore, in this article, we propose a novel feature representation and aggregation network and introduce it into the tracking-by-segmentation framework to extract and integrate rich features for accurate and robust segmentation tracking. To be specific, firstly, the proposed approach models three complementary feature representations, including contextual semantic, local position, and structural patch feature representations, through cross-attention, cross-correlation and dilated involution mechanisms respectively. Secondly, these features are fused by a simple feature aggregation network. Thirdly, the fusion features are fed into the segmentation network to obtain accurate target state estimation. In addition, to adapt the segmentation network to the appearance changes and partial occlusion, we introduce a template update strategy and a bounding box refinement module for robust segmentation and tracking. The extensive experimental results on twelve challenging tracking benchmarks show that the proposed tracker outperforms most of the state-of-the-art trackers and achieves very promising tracking performance on the OTB100 and VOT2018 benchmarks.",
        "references": [
            "965dcac204f056bff465247c3efce94e5cb53a7c",
            "10e11dc9dafdca6bedf1327b988971f3cea0c83a",
            "48c6ca17f17038ff9933dca86a23f8516168a3ea",
            "bbc97484fe622da1e07cf5816ecd8e6b32235d78",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "e945c2f83b9d2c1b0cce47e33e5468168a82d630",
            "1c9f5cb6e46c4ad37cef7890256e9c0fa217c8ba",
            "af7699c3ffcf8516d83c1444ae1fc53aa9efc33d",
            "9d5f536bd3889f205de8f9dba579a2bc3974ab73",
            "5a1d617fd28eac356e7bdb84564ab87bc7113d7b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "66"
    },
    {
        "Id": "be5e2ba71cdc5f043e560958e6bda442a1289257",
        "title": "Efficient and Lightweight Visual Tracking with Differentiable Neural Architecture Search",
        "authors": [
            "Peng Gao",
            "Xiao Liu",
            "Hong-Chuan Sang",
            "Yu Wang",
            "Fei Wang"
        ],
        "date": "27 August 2023",
        "abstract": "The results indicate that the proposed TrackNAS achieves competitive performance in terms of accuracy and robustness, and the number of network parameters and computation volume are far smaller than those of other advanced Siamese trackers, meeting the requirements for lightweight deployment to resource-constrained devices. Over the last decade, Siamese network architectures have emerged as dominating tracking paradigms, which have led to significant progress. These architectures are made up of a backbone network and a head network. The backbone network comprises two identical feature extraction sub-branches, one for the target template and one for the search candidate. The head network takes both the template and candidate features as inputs and produces a local similarity score for the target object in each location of the search candidate. Despite promising results that have been attained in visual tracking, challenges persist in developing efficient and lightweight models due to the inherent complexity of the task. Specifically, manually designed tracking models that rely heavily on the knowledge and experience of relevant experts are lacking. In addition, the existing tracking approaches achieve excellent performance at the cost of large numbers of parameters and vast amounts of computations. A novel Siamese tracking approach called TrackNAS based on neural architecture search is proposed to reduce the complexity of the neural architecture applied in visual tracking. First, according to the principle of the Siamese network, backbone and head network search spaces are constructed, constituting the search space for the network architecture. Next, under the given resource constraints, the network architecture that meets the tracking performance requirements is obtained by optimizing a hybrid search strategy that combines distributed and joint approaches. Then, an evolutionary method is used to lighten the network architecture obtained from the search phase to facilitate deployment to devices with resource constraints (FLOPs). Finally, to verify the performance of TrackNAS, comparison and ablation experiments are conducted using several large-scale visual tracking benchmark datasets, such as OTB100, VOT2018, UAV123, LaSOT, and GOT-10k. The results indicate that the proposed TrackNAS achieves competitive performance in terms of accuracy and robustness, and the number of network parameters and computation volume are far smaller than those of other advanced Siamese trackers, meeting the requirements for lightweight deployment to resource-constrained devices.",
        "references": [
            "fce3655dc22a783b1f82f09190410f070c7bf42c",
            "1fbb4201af091aef55360f113ba35814063923e4",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "0723e41673d2bdeeec37c21aa3c3e73ba20e7b04",
            "2d9048a88a7ead488581ecdeccf09e8ac8256c75",
            "ffe58d5dd37de1832b44befd4b358deffb406ef8",
            "be412c7c7128cf91455233b652d6c94a6001a7c8",
            "177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "a6059dd431b0c292ffff8f43a729735853e7e258",
            "e284bc13c2b76d0d0c7ad61d976f8a9d3eef8461"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "27"
    },
    {
        "Id": "409b43b8cd8a2ba69f93e80c2bacc0126238b550",
        "title": "Mobile Vision Transformer-based Visual Object Tracking",
        "authors": [
            "Goutam Yelluru Gopal",
            "Maria A. Amer"
        ],
        "date": "11 September 2023",
        "abstract": "This work proposes a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time and presents a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The introduction of robust backbones, such as Vision Transformers, has improved the performance of object tracking algorithms in recent years. However, these state-of-the-art trackers are computationally expensive since they have a large number of model parameters and rely on specialized hardware (e.g., GPU) for faster inference. On the other hand, recent lightweight trackers are fast but are less accurate, especially on large-scale datasets. We propose a lightweight, accurate, and fast tracking algorithm using Mobile Vision Transformers (MobileViT) as the backbone for the first time. We also present a novel approach of fusing the template and search region representations in the MobileViT backbone, thereby generating superior feature encoding for target localization. The experimental results show that our MobileViT-based Tracker, MVT, surpasses the performance of recent lightweight trackers on the large-scale datasets GOT10k and TrackingNet, and with a high inference speed. In addition, our method outperforms the popular DiMP-50 tracker despite having 4.7 times fewer model parameters and running at 2.8 times its speed on a GPU. The tracker code and models are available at https://github.com/goutamyg/MVT",
        "references": [
            "a33df432bcdd3a7c06bddc08247389f69a1e59ed",
            "bbd2e7552f586bce5adf015213b62d30792e55fd",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "3e86599d179ae530a31e6a7012c15777c88f21d1",
            "badd83f91a8f97ba2f3010601af26d8a60edfbba",
            "703505a00579c0aa67712836acc41d94fa6d6edc",
            "7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
            "72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "e284bc13c2b76d0d0c7ad61d976f8a9d3eef8461"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "34"
    },
    {
        "Id": "e73906acc9a9116e372cfd87efc5dd44d3621637",
        "title": "Ef\ufb01cient and Lightweight Visual Tracking with Differentiable Neural Architecture Search",
        "authors": [
            "Peng Gao",
            "Xiao Liu",
            "Hong-Chuan Sang",
            "Yu Wang",
            "Fei Wang"
        ],
        "date": "",
        "abstract": "A novel Siamese tracking approach called TrackNAS based on neural architecture search is proposed to reduce the complexity of the neural architecture applied in visual tracking, meeting the requirements for lightweight deployment to resource-constrained devices. : Over the last decade, Siamese network architectures have emerged as dominating tracking paradigms, which have led to signi\ufb01cant progress. These architectures are made up of a backbone network and a head network. The backbone network comprises two identical feature extraction sub-branches, one for the target template and one for the search candidate. The head network takes both the template and candidate features as inputs and produces a local similarity score for the target object in each location of the search candidate. Despite promising results that have been attained in visual tracking, challenges persist in developing ef\ufb01cient and lightweight models due to the inherent complexity of the task. Speci\ufb01cally, manually designed tracking models that rely heavily on the knowledge and experience of relevant experts are lacking. In addition, the existing tracking approaches achieve excellent performance at the cost of large numbers of parameters and vast amounts of computations. A novel Siamese tracking approach called TrackNAS based on neural architecture search is proposed to reduce the complexity of the neural architecture applied in visual tracking. First, according to the principle of the Siamese network, backbone and head network search spaces are constructed, constituting the search space for the network architecture. Next, under the given resource constraints, the network architecture that meets the tracking performance requirements is obtained by optimizing a hybrid search strategy that combines distributed and joint approaches. Then, an evolutionary method is used to lighten the network architecture obtained from the search phase to facilitate deployment to devices with resource constraints (FLOPs). Finally, to verify the performance of TrackNAS, comparison and ablation experiments are conducted using several large-scale visual tracking benchmark datasets, such as OTB100, VOT2018, UAV123, LaSOT, and GOT-10k. The results indicate that the proposed TrackNAS achieves competitive performance in terms of accuracy and robustness, and the number of network parameters and computation volume are far smaller than those of other advanced Siamese trackers, meeting the requirements for lightweight deployment to resource-constrained devices.",
        "references": [
            "fce3655dc22a783b1f82f09190410f070c7bf42c",
            "530c0e08362aceb66c226fa4379523820ef56ba3",
            "1fbb4201af091aef55360f113ba35814063923e4",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "0723e41673d2bdeeec37c21aa3c3e73ba20e7b04",
            "4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "cce1fecc800d2782da638f3060d5b2e887739f74",
            "0945def77cbe3a4f9e24a93f4f9c0e07ca28c85e",
            "2c8315ae713b3e27c6e9f291a158134d9c516166"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "77"
    },
    {
        "Id": "c5fac9a5097543ab458ca254f162dd9996fa79cd",
        "title": "Scale-Aware Tracking Method with Appearance Feature Filtering and Inter-Frame Continuity",
        "authors": [
            "Haiyu He",
            "Zhen Chen",
            "Z. Li",
            "Xiangdong Liu",
            "Haikuo Liu"
        ],
        "date": "30 August 2023",
        "abstract": "This paper proposes a practical and efficient solution that can handle scale changes without using multi-scale features and can be combined with any DCF-based tracker as a plug-in module and achieves competitive accuracy and robustness while significantly reducing the computational cost. Visual object tracking is a fundamental task in computer vision that requires estimating the position and scale of a target object in a video sequence. However, scale variation is a difficult challenge that affects the performance and robustness of many trackers, especially those based on the discriminative correlation filter (DCF). Existing scale estimation methods based on multi-scale features are computationally expensive and degrade the real-time performance of the DCF-based tracker, especially in scenarios with restricted computing power. In this paper, we propose a practical and efficient solution that can handle scale changes without using multi-scale features and can be combined with any DCF-based tracker as a plug-in module. We use color name (CN) features and a salient feature to reduce the target appearance model\u2019s dimensionality. We then estimate the target scale based on a Gaussian distribution model and introduce global and local scale consistency assumptions to restore the target\u2019s scale. We fuse the tracking results with the DCF-based tracker to obtain the new position and scale of the target. We evaluate our method on the benchmark dataset Temple Color 128 and compare it with some popular trackers. Our method achieves competitive accuracy and robustness while significantly reducing the computational cost.",
        "references": [
            "ad8f00891781f1d81e43a7799ac602bb8720a352",
            "ce8c76bfedc5d86faabf0d49dc42a4924f75876d",
            "0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "f5dbe4550d24d5374d9e10fce44a35b105c7ee07",
            "1cd30c9900f73195f9159183406019bb1561e5f3",
            "70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "3353495c5d2bab6a6a434190ff3b28ad9536a14c",
            "68b9059fe1606a365a984ceaf2d8e07cd57ddd61",
            "a57335f8db4c7a490713261511421b3b37d3ec19"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "29"
    },
    {
        "Id": "8b36c7e4445ce5be341cf1f1a48aa1c8222f300b",
        "title": "Facing Completely Occluded Short-Term Tracking Based on Correlation Filters",
        "authors": [
            "Yuanming Zhang",
            "Huihui Pan",
            "Jue Wang",
            "Weichao Sun"
        ],
        "date": "2023",
        "abstract": "This article proposes a generative association approach by utilizing raw images of pixels as well as adjacent trajectories for generating robust target descriptors and observable cumulative sequences and establishes a new record score in distance precision (DP) on the TB50, by exceeding the other 19 SOTA trackers. Accurate occlusion tracking is a difficult research in vision-based measurement. Most state-of-the-art (SOTA) approaches exploit suppression of templates learning of filters to avoid occlusion pollution. Such suppression strategies retard in the recognition of target obstruction and lack state processing during the occlusion phase. This article explores the precise and robust handling of the complete occlusion problem in the framework of discriminative correlation filters (DCFs). We propose a generative association approach by utilizing raw images of pixels as well as adjacent trajectories for generating robust target descriptors and observable cumulative sequences. The occlusion of the target is identified through the descriptors matching over the time series and estimates the transfer of the state by the generating function. Compared with suppression methods, our approach develops the apparent characteristics and cumulative state of the target, not relying on filter templates. This achieves a maximum gain of 5.0% of area under the curve (AUC) precision on the object tracking benchmark (OTB). Comprehensive evaluations are performed on six datasets: UAV123, TB50, OTB100, large-scale single-object tracking (LaSOT), VOT2018ST, and VOT2019ST. Our method establishes a new record score in distance precision (DP) on the TB50, by exceeding the other 19 SOTA trackers. Also, we reach the SOTA ranks on both the VOT2018ST and VOT2019ST datasets.",
        "references": [
            "754504cf01ef3846259783e748b1d3ea52fa2c81",
            "ece7625a346edbc5f6fab541c0c246ec06939121",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "ce8c76bfedc5d86faabf0d49dc42a4924f75876d",
            "01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "388d29f001411ff80650f80cf197afc440d98b51",
            "d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "a5278fc76eff08668bc1957b01b22eb627fa2c36",
            "0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "89"
    },
    {
        "Id": "34b8bca61c716bdf553b87e3133abd9675d27792",
        "title": "Transformers in Single Object Tracking: An Experimental Survey",
        "authors": [
            "Janani Kugarajeevan",
            "Thanikasalam Kokul",
            "Amirthalingam Ramanan",
            "Subha Fernando"
        ],
        "date": "23 February 2023",
        "abstract": "An in-depth literature analysis of Transformer tracking approaches is conducted by categorizing them into CNN-Transformer based trackers, Two-stream Two-stage fully-Trans transformers, and One-stream One-stage Fully Transformer basedTrackers. Single-object tracking is a well-known and challenging research topic in computer vision. Over the last two decades, numerous researchers have proposed various algorithms to solve this problem and achieved promising results. Recently, Transformer-based tracking approaches have ushered in a new era in single-object tracking by introducing new perspectives and achieving superior tracking robustness. In this paper, we conduct an in-depth literature analysis of Transformer tracking approaches by categorizing them into CNN-Transformer based trackers, Two-stream Two-stage fully-Transformer based trackers, and One-stream One-stage fully-Transformer based trackers. In addition, we conduct experimental evaluations to assess their tracking robustness and computational efficiency using publicly available benchmark datasets. Furthermore, we measure their performances on different tracking scenarios to identify their strengths and weaknesses in particular situations. Our survey provides insights into the underlying principles of Transformer tracking approaches, the challenges they encounter, and the future directions they may take.",
        "references": [
            "39b27ee48caa5bb68a8c50ef5f02121729847334",
            "75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
            "1fbb4201af091aef55360f113ba35814063923e4",
            "177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "882ba4364a695cc8d2dec70c681f55861a514331",
            "0530cbeb847f5e5002d1183c482759dff5f8c439",
            "8199f7bd56ffc62bdb1272d1b7e13f5e51f3d4c7",
            "26e2ca763087be09e3799ad294302aa91077942d",
            "f56fa871dade366fd20551e25dd9c9492ab803bc"
        ],
        "related_topics": [
            "Single Object Tracking",
            "Transformer",
            "Tracking Scenario",
            "Computer Vision",
            "Benchmark Dataset"
        ],
        "reference_count": "124",
        "citation_count": "5"
    },
    {
        "Id": "06b05fa46ea957aa1e16aa809a23828e601d4f49",
        "title": "STUDY ON LEVEL SET SEGMENTATION BASED CLASSIFICATION USING MAMMOGRAMS",
        "authors": [
            "Payal Nemade",
            "Aarti Sharma",
            "Chiranji Lal Chowdhary"
        ],
        "date": "2017",
        "abstract": "An algorithm based on region and contour based and some clustering segmentation techniques to recognize tumors in breast images and classification algorithms are used to make difference between tumours affected image or normal image. Application of image processing algorithms on medical images is always challenging. This always help in early detection stages and later for diagnosis of different cancer issues. As breast cancer being second major cause of death in women, so our interest is on this topic to improve the accuracy to segment the tumors in mammograms. For this purpose in our paper, we present algorithm based on region and contour based and some clustering segmentation techniques to recognize tumors in breast images. Our first step in this algorithm require the level set. We have used Spatial Fuzzy Clustering (SFC) to improve region growing. In Second step, Artificial Neural Network is used to regulate all level set parameters. This approach is used with Genetic Algorithm. Third step is of feature extraction. This step is used to extract features from segmented images to train a classified to determine whether a tumour in label as normal or tumour area. As last step, classification algorithms are used to make difference between tumours affected image or normal image. We will test with ANN algorithms.",
        "references": [
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "c55550ff224ff080d95d93200aea8838267fc21c",
            "6d8c60d47e2be4de763bb2f8044e09981016396e",
            "e065a189e661b7fc0391bfea819355bf476b8d74",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "9a105c4792cda90bbd81efe3912da80302bcbe6d",
            "8ece55a94960504cd809b4058382afa746085911",
            "a23937d6345c6e707cf17509ec27c76fef4ce7cc",
            "9ea6483cc8d60186a813d9f45d7f71a03a0ea54a",
            "8a15dcdc34a229897e5a756f8963f8101941e76b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "26"
    },
    {
        "Id": "301fee6989cc75f84343836cab5a25691d58dc5c",
        "title": "Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature",
        "authors": [
            "Tariq Sadad",
            "Asim Munir",
            "Tanzila Saba",
            "Ayyaz Hussain"
        ],
        "date": "1 November 2018",
        "abstract": "Semantic Scholar extracted view of \"Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature\" by Tariq Sadad et al.",
        "references": [
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "d5abfbdef48c20ed124833024e61e4773e944966",
            "82917b65ec0e7fa1bbdc0ba08642fd8c336196a1",
            "d10fa706f2383510f8ecbf69a6c404fc4a5837f8",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "2537fbb22837de4c9f2b167d7905b2810d84e28f",
            "00aefc484dd5684ea999ba310dc152ecbfc7963e",
            "9c204bc24341157f1dbd72fde781a4906d8f17cd",
            "c82fff512c57f12059632bfc02912b494144d984",
            "4fc76c0c9c547cae53da984918cf4893dbca38a4"
        ],
        "related_topics": [
            "Local Phase Quantization",
            "Tumors",
            "Mammographic Image Analysis Society",
            "Mammograms",
            "Classification",
            "Classifier",
            "Radiologist",
            "Decision Trees",
            "DDSM Dataset",
            "Classification Accuracy"
        ],
        "reference_count": "53",
        "citation_count": "83"
    },
    {
        "Id": "64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
        "title": "Automatic Breast Tumor Classification Using a Level Set Method and Feature Extraction in Mammography",
        "authors": [
            "Soheil Pashoutan",
            "Shahriar Baradaran Shokouhi",
            "Meisam Pashoutan"
        ],
        "date": "1 November 2017",
        "abstract": "The results focus on the appropriate efficiency of proposed segmentation and features extraction methods and features related to GLCM are among the best results with the accuracy of 93.37 and sensitivity of 94.18. Breast cancer is one of the leading factors of cancer-related deaths among women, therefore designing Computer Aided Diagnosis (CADx) systems to detect malignant and benign tumors of breast masses is extensively essential. Using a segmentation method and subsequently a proper feature extraction is crucial to obtain an appropriate performance in CADx system. In this paper, the Mammography Imaging Analysis Society (MIAS) data is used in order to detect whether breast masses are malignant or benign. A method is based on level set and with the purpose of segmenting the region of the tumor in mammography for the first time and in following, four additional methods were introduced. Including wavelet transform, Gabor wavelet transform Zernike moments and Gray-Level Co-occurrence Matrix (GLCM) to extract features and each one leads to the extraction of a group of the segmented tumor features. Proper features are selected using P value. Consequently, in order to investigate the efficiency of selected features, each group of features are used within one Multilayer Perceptron (MLP). In this paper, the results focus on the appropriate efficiency of proposed segmentation and features extraction methods. Among these considered features, the features related to GLCM are among the best results with the accuracy of 93.37 and sensitivity of 94.18.",
        "references": [
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "40bbb7667b260bad22b0e2cb34562f57735d67f8",
            "474ae46626676d01c7b38328c107b1531b181b46",
            "001ec101a06cf8ce254db94a9c6130b5fe43aabe",
            "2dfb1fd3adfa58a3448251e03b1a5a78239958b3",
            "9bf32a68edfea8b8c072bcd3ee0d696687bab403",
            "33350f776b506e0e8cdee488480d0ce7531282ee"
        ],
        "related_topics": [
            "Malignant",
            "Computer Aided Detection ( Cade ) And Diagnosis",
            "Level Set Methods",
            "Zernike Moments",
            "Mammographic Image Analysis Society",
            "Level Sets",
            "Multi-layer Perceptron"
        ],
        "reference_count": "27",
        "citation_count": "6"
    },
    {
        "Id": "6d5cee9e834907d557ad6eaf1a3edfd610fa77c8",
        "title": "A hybrid computer-aided diagnosis system for abnormality detection in mammograms",
        "authors": [
            "Vinod Kumar",
            "Figlu Mohanty",
            "Bodhisattva Dash",
            "Suvendu Rup"
        ],
        "date": "1 May 2017",
        "abstract": "A contourlet-based feature extraction technique is employed and it is observed that SVM produces more accurate results than that of the k-NN classifier, which outperforms some of the existing systems. In recent years, breast cancer is one of the most death causing reasons worldwide whosetreatment involves early detection and diagnosis. A Computer-aided Diagnosis (CAD) system is used for the diagnosis of abnormalities present in the human breast in order to reach a decision. Hence, it is considered to be an effective means of reducing the mortality rate. In the proposed scheme, initially, a contourlet-based feature extraction technique is employed. Further, a filter approach, namely, the two-sample t-test is utilized to find out the most relevant features. Then, the mammograms with the reduced feature set are classified with SVM and k-NN classifiers. For validation of the proposed work, mammograms from the standard dataset, namely, Mammographic Image Analysis Society (MIAS) are considered. From the result analysis, it is observed that SVM produces more accurate results than that of the k-NN classifier. The achieved classification accuracies for normal-abnormal, and benign-malignant are 99.3%, and 99.98%, respectively for SVM classifier. The obtained results show that the presented CAD framework outperforms some of the existing systems.",
        "references": [
            "241b3094b2cc3682df765ed3ed70f8a616358729",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "119e4bce7a27937fb5c266c8d35b124750494a4f",
            "0003a45ae653fcccb568b90dbb339cf5811a18bf",
            "8181b2a289f4a9267a846cf03f50f0e337835af8",
            "06e647bea552a623e406f58746a8faab1e07dde4",
            "f0a4d96b5d7f6668d607512f8744e07dbe1c9b27",
            "e23eb86e5759082d38cca9aaf45709b80f9aeead",
            "5075eb283b5257d78c4d6d40d1b728ae316fe5ef",
            "a06de6ddabd1511b0b64b504c55a653b41ee1576"
        ],
        "related_topics": [
            "Support Vector Machines",
            "Mammograms",
            "Mammographic Image Analysis Society",
            "Cylindrical Algebraic Decomposition",
            "Filter Approach",
            "Classification Accuracy"
        ],
        "reference_count": "14",
        "citation_count": "2"
    },
    {
        "Id": "f647a77af6ee361456d7c9c876d78780413b2695",
        "title": "Automatic computer-aided diagnosis system for mass detection and classification in mammography",
        "authors": [
            "Ilhame Ait Lbachir",
            "Imane Daoudi",
            "Saadia Tallal"
        ],
        "date": "11 November 2020",
        "abstract": "A complete CAD system for mass detection and diagnosis, which consists of four steps where the preprocessing where the image is enhanced and the noise removed, and the support vector machine (SVM) is used to classify the abnormalities as malignant or benign. Mammography is currently the most powerful technique for early detection of breast cancer. To assist radiologists to better interpret mammogram images, computer-aided detection and diagnosis (CAD) systems have been proposed. This paper proposes a complete CAD system for mass detection and diagnosis, which consists of four steps. The first step consists of the preprocessing where the image is enhanced and the noise removed. In the second step, the abnormalities are segmented using the proposed HRAK algorithm. In the third step, the false positives are reduced using texture and shape features and the bagged trees classifier. Finally, the support vector machine (SVM) is used to classify the abnormalities as malignant or benign. The proposed CAD system is verified with both the MIAS and CBIS-DDSM databases. The experimental results proved to be successful. The accuracy detection rate achieves 93,15% for sensitivity and 0,467 FPPI for MIAS and 90,85% for sensitivity and 0,65 FPPI for CBIS-DDSM. The accuracy classification rate achieves 94,2% and the AUC 0,95 for MIAS and 90,44% and 0,9 for CBIS-DDSM.",
        "references": [
            "f7c940ebe313088b7638d97c9913dcec9bdbbde9",
            "7b17689f9e3e63e47fe19be3b9a43acac264369c",
            "c9b310d670c105f82fff94f4ae14532714f85934",
            "831a1e8106571d82f22cd8dac7725964852b0154",
            "a179d495a865e84251b1d3e79ed79c31b26bbf7f",
            "662d927da3c17bfde61da5dbc24c037dce30ce25",
            "1f3c875590b851b900ae95e1fe197da4baeaf0be",
            "00498583b8166055d650e5fe9565631f3599eebb",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741",
            "69ab94d05d3da8f07b5b023e8a69f0078f1dbd52"
        ],
        "related_topics": [
            "Mammographic Image Analysis Society",
            "Cylindrical Algebraic Decomposition",
            "False Positives Per Image",
            "Mass Detection",
            "Support Vector Machines",
            "CBIS-DDSM",
            "Malignant",
            "Classification",
            "Area Under The ROC Curve"
        ],
        "reference_count": "44",
        "citation_count": "32"
    },
    {
        "Id": "add610bbec6306dbf911cd38ecb5b5a1e36ae978",
        "title": "Segmentation of mammogram abnormalities using ant system based contour clustering algorithm",
        "authors": [
            "Sudha Subramanian",
            "Ganesan Rasu Thevar"
        ],
        "date": "14 April 2022",
        "abstract": "This research work focuses on detection of microcalcifications from the digital mammograms using a novel segmentation approach based on novel Ant Clustering approach called Ant System based Contour Clustered (ASCC) that simulates the ants\u2019 foraging behavior. Breast cancer is the most widespread cancer that affects females all over the world. The Computer-aided Detection Systems (CADs) could assist radiologists\u2019 in locating and classifying the breast tissues into normal and abnormal, however the absolute decisions are still made by the radiologist. In general, CAD system consists of four stages: Pre-processing, segmentation, feature extraction, and classification. This research work focuses on the segmentation step, where the abnormal tissues are segmented from the normal tissues. There are numerous approaches presented in the literature for mammogram segmentation. The major limitation of these methods is that they have to test each and every pixel of the image at least once, which is computationally expensive. This research work focuses on detection of microcalcifications from the digital mammograms using a novel segmentation approach based on novel Ant Clustering approach called Ant System based Contour Clustering (ASCC) that simulates the ants\u2019 foraging behavior. The performance of the ASCC based segmentation algorithm is investigated with the mammogram images received from Mammographic Image Analysis Society (MIAS) database.",
        "references": [
            "bb36fe5d0928b4e97c4c7139a31706ca03914b0d",
            "f757d4cf2a1a885810bd0c97f9735612750f329c",
            "2555bce05ed69d0e9f01cdbd04b87e4596c2a8ae",
            "0661e557e315aa7faf90d381e6948450046bb771",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "d24419d4ce3625599d627e6a3f001780a1fe72f6",
            "7c42c629a0946cf211ef262d34792ac8bf7f8010",
            "c17c352f525b714d42d51e62b11274fd2ec716f0",
            "edac407f531bc37adbbb3825a22673720b22c67b",
            "8b317d33282d5f9d2b7d60368c0e5431348c21ce"
        ],
        "related_topics": [
            "Pixel",
            "Microcalcifications",
            "Ant System",
            "Mammographic Image Analysis Society",
            "Mammograms"
        ],
        "reference_count": "0",
        "citation_count": "34"
    },
    {
        "Id": "14f52a48954bdc3fd7ab872a05c49d60da1fdb8d",
        "title": "An Efficient Method for Breast Mass Segmentation and Classification in Mammographic Images",
        "authors": [
            "Marwa Hmida",
            "Kamel Hamrouni",
            "Basel Solaiman",
            "Sana Boussetta"
        ],
        "date": "2017",
        "abstract": "A method to segment and classify masses using the regions of interest of mammographic images using a fuzzy active contour model obtained by combining Fuzzy C-Means and the Chan-Vese model is proposed. According to the World Health Organization, breast\ncancer is the main cause of cancer death among women in\nthe world. Until now, there are no effective ways of preventing\nthis disease. Thus, early screening and detection is the most\neffective method for rising treatment success rates and reducing\ndeath rates due to breast cancer. Mammography is still the most\nused as a diagnostic and screening tool for early breast cancer\ndetection. In this work, we propose a method to segment and\nclassify masses using the regions of interest of mammographic\nimages. Mass segmentation is performed using a fuzzy active\ncontour model obtained by combining Fuzzy C-Means and the\nChan-Vese model. Shape and margin features are then extracted\nfrom the segmented masses and used to classify them as benign\nor malignant. The generated features are usually imprecise and\nreflect an uncertain representation. Thus, we propose to analyze\nthem by a possibility theory to deal with imprecise and uncertain\naspect. The experimental results on Regions Of Interest (ROIs)\nextracted from MIAS database indicate that the proposed method\nyields good mass segmentation and classification results.",
        "references": [
            "70ffedec6058b04159501170bb1c4e30e1769075",
            "a9d0b3485f3091e832f87edb469c350c90cabae1",
            "3c948ca247c6f55ef994400e713412b5f845dd40",
            "34c44883a6152c5298f2c452670c1127072400e6",
            "1f4a786c26081086a7e947d0ce001e89862e8e2b",
            "27d4dcb58a1918e098b5e6ddec791ef9e31972fc",
            "9b5d0a48b0feb156a1270da54d90d0963a3f0404",
            "8b317d33282d5f9d2b7d60368c0e5431348c21ce",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "4a18fe753a3d4f6619928220fb23f7bdd3ba4cff"
        ],
        "related_topics": [
            "Classification",
            "Region Of Interest",
            "Mass Segmentation",
            "Chan-Vese Model",
            "Breast Mass Segmentation",
            "Malignant",
            "Fuzzy Active Contour Models"
        ],
        "reference_count": "27",
        "citation_count": "5"
    },
    {
        "Id": "5f0f6207844c6ef3df53dcfa21591d6359fbffe0",
        "title": "Mammographic mass segmentation using fuzzy contours",
        "authors": [
            "Marwa Hmida",
            "Kamel Hamrouni",
            "Bassel Solaiman",
            "Sana Boussetta"
        ],
        "date": "18 July 2018",
        "abstract": "Semantic Scholar extracted view of \"Mammographic mass segmentation using fuzzy contours\" by M. Hmida et al.",
        "references": [
            "a9d0b3485f3091e832f87edb469c350c90cabae1",
            "1f4a786c26081086a7e947d0ce001e89862e8e2b",
            "a8a78eca9761288464ec960a967bf73bf0e852df",
            "583be7c995db7695891b8961678e8eecc9aad5f3",
            "cd8888f739a8e6e58a5aa7ea5669bc09d60ac202",
            "1448eded35e963c7316f64f9c1b54abe519665fa",
            "23b8fc39e41d8e028085a4c02c1ca5663f69251e",
            "cce870a907ef830be6883d767ce3e17e0495e6bc",
            "00498583b8166055d650e5fe9565631f3599eebb",
            "ac0384f2c26cf0d626608e580bd59f7ee15800d6"
        ],
        "related_topics": [
            "Mass Segmentation",
            "Mammographic Mass Segmentation",
            "Chan-Vese Model"
        ],
        "reference_count": "42",
        "citation_count": "13"
    },
    {
        "Id": "20e8be521ad4aa3dac54b5cc6c3acceba4f298f2",
        "title": "DELINEATION AND CLASSIFICATION OF LIVER CANCER USING LEVEL SET METHOD IN CT IMAGES",
        "authors": [
            "Amita Das",
            "Sauris Panda",
            "Sukanta Kumar Sabut"
        ],
        "date": "28 December 2017",
        "abstract": "A combined framework of reorganization and extraction of region of interest (ROI), texture feature extraction followed by texture classification by different machine learning approaches has been presented. The paper proposes a modified approach of delineation and classification of two different types of liver cancers viz. Hepatocellular Carcinoma (HCC) and Metastatic Carcinoma (MET) from different slices of computed tomography (CT) scans images. A combined framework of reorganization and extraction of region of interest (ROI), texture feature extraction followed by texture classification by different machine learning approaches has been presented. Initially, adaptive thresholding has been applied to segment the liver region from CT images. Level set algorithm has been used for detecting the region of cancer tissues. In the classification stage, the delineated output lesions have been extracted with 38 features to build up the dataset. Two machine learning classifiers, support vector machine (SVM) and random forest (RF), have been used to train the dataset for correct prediction of cancer classes. Ten-fold cross-validation has been used to evaluate the performance of two classifiers. The efficiency of the pr...",
        "references": [
            "2fee907bce3bcddd475b1f062b00ddacf6383a57",
            "eb49502d0393993eec094f99482557c0601b5e61",
            "9c377d22c968ae21d97e2d4576a944084447c39f",
            "5e8187c85bace848e6d9661707b9d87ac786aaee",
            "e4c2ea600fcfc680514d83288f3dfeffb887e077",
            "7c9ac6e84f8873a16ec8e186f0f9f3ec5340c9f2",
            "2415fc06de82ab41ad8b9615162247afb02974af",
            "66cfefa3ea18690c878e56920831ef1a5b99074d",
            "294478ca3f28fcdd18f47b0459a1f45be4b29767",
            "2b9c1d8b3164eb86c609510f2d9130a08beb75a4"
        ],
        "related_topics": [
            "Classification",
            "Machine Learning",
            "Support Vector Machines",
            "Classifier",
            "Random Forests",
            "Level Set Methods",
            "Region Of Interest",
            "Minimum Execution Time"
        ],
        "reference_count": "30",
        "citation_count": "2"
    },
    {
        "Id": "3692b880928fe13be40b281e7286b6917fdd90a3",
        "title": "Computer-aided detection and diagnosis of mammographic masses using multi-resolution analysis of oriented tissue patterns",
        "authors": [
            "Jayasree Chakraborty",
            "Abhishek Midya",
            "Rinku Rabidas"
        ],
        "date": "1 June 2018",
        "abstract": "Semantic Scholar extracted view of \"Computer-aided detection and diagnosis of mammographic masses using multi-resolution analysis of oriented tissue patterns\" by Jayasree Chakraborty et al.",
        "references": [
            "bfa836714217db5d652aace6c87758d14bfa0994",
            "6bc96dfb837e554db6097d97708edd806bdb9061",
            "d7e7088733aa7a901884ac0f543471af3e10c031",
            "d0139c82ceeadb6987add17973245657075de94d",
            "ac0ca4295b0622e370c19716071614b6d2d723f9",
            "96f1f1084b32275ae4f0e9a47e6675086f487fa7",
            "37cfc1e4739ace79533e9aecd9a80945614ca4ff",
            "28a634cc15a1b8c8a6249a5b5f9e30241cd8187c",
            "6c1d2c89a4415a2c72c3aa2bbb3e887853235e19",
            "f4a3552253b014e993bf56631e463dc95d2aaa66"
        ],
        "related_topics": [
            "Malignant",
            "Digital Database For Screening Mammography",
            "Mammographic Masses"
        ],
        "reference_count": "67",
        "citation_count": "37"
    },
    {
        "Id": "a7f3cd1885a0f1548596b288eb6768d818b8463d",
        "title": "Breast Cancer Detection and Classification from Mammogram Images Using Multi-model Shape Features",
        "authors": [
            "V. R. Gurudas",
            "S. G. Shaila",
            "A. Vadivel"
        ],
        "date": "28 July 2022",
        "abstract": "This paper proposes a framework that automatically classifies the benign and malignant tumors in mammogram images using the support vector machine (SVM) and artificial neural network (ANN) classifiers, which have achieved good results and are promising. Nowadays, breast cancer has become one of the common diseases and is leading in causes of deaths in women. Early detection of breast cancer is very much needed and critical, and mammography is considered as one of the best-suited procedures. The masses are classified as benign or malignant tumors. The size and shape of the masses are characterized by its shapes as per BI-RADS (Breast Imaging-Reporting and Data System), which can discriminate benign and malignant effectively. In this paper, we propose a framework that automatically classifies the benign and malignant tumors in mammogram images. We have considered INBreast and CBIS-DDSM dataset experiments. The histogram-processing multi-level Otsu thresholding on the extracted Region of Interest (ROI) is applied as pre-processing steps for segmenting it. Eighteen features are extracted from the ROI and characterized structure, shape, size, and boundaries of mass present in images belong to both the datasets. The features extracted from the datasets are cross-validated for training and testing using stratified cross-validation techniques. The support vector machine (SVM) and artificial neural network (ANN) classifiers are trained and validated for benign and malignant tumor classification. The experimental results have achieved good results and are promising.",
        "references": [
            "64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
            "04f3e376672318a7c8bfe12059069d033c688951",
            "82b09f56d68eefdb3a3442cfe29433cd72480d64",
            "870d52e025216445bbc51434527f450c7be630fd",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "c4f73e387ad859a54de1d2f9938b8c7dbd1c2a97",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "34b5e888f948e384f434ba3fd1b74e26ba104d5a",
            "f757d4cf2a1a885810bd0c97f9735612750f329c",
            "7ff70f493df1a7d5dfc0c437e7fce6a5172be933"
        ],
        "related_topics": [
            "Support Vector Machines",
            "Classifier",
            "INbreast",
            "Classification",
            "CBIS-DDSM Dataset",
            "Approximate Nearest Neighbor",
            "Region Of Interest",
            "BI-RADS"
        ],
        "reference_count": "42",
        "citation_count": "One"
    },
    {
        "Id": "2f4d931cba7f6db8fe887c2585b2cdcf2f524e66",
        "title": "Breast Cancer Detection using Image Processing and Machine Learning: A Comprehensive Review and Improved Segmentation Approach",
        "authors": [
            "D.D.H. Erandika",
            "UA. Piumi Ishanka"
        ],
        "date": "30 December 2023",
        "abstract": "This study enables an effective image segmentation approach with two stages: removing the background using thresholding, tumor region segmentation using a combination of thresholding, and Canny edge detection. Breast cancer stands as one of the most prevalent health concerns for women. Early detection of breast cancer can significantly improve the chances of survival. In developed countries, more than 19.9% of women will die per year due to breast cancer. Regular breast cancer screening is an important way to detect cancer early. Image processing techniques are highly used for different types of cancer detection applications with medical screening. Segmentation of the breast tumor region is a critical step in image processing related to this manner. Lots of research work can be found on developing ways for detecting breast cancers. However still, there is a need for a standard and robust cancer region segmentation method. Right cancer region segmentation is significant for better feature extraction and better classification. This study presents a comprehensive literature review about technologies used with image enhancement and tumor segmentation. Further, the study proposes a robust approach to image enhancement and cancer region segmentation for breast cancer detection using image processing techniques and machine learning. In this work, mammograms are enhanced using Contrast Limited Adaptive Histogram Equalization and denoised using the Median blurring filter. This study enables an effective image segmentation approach with two stages: removing the background using thresholding, tumor region segmentation using a combination of thresholding, and Canny edge detection. Segmented tumor region\u2019s features are extracted using the Gabor filter. Here the accuracy of the approach is compared with three main machine learning classifiers: Decision Tree, Random Forest, Multinomial Logistic Regression and mammograms are classified into three classes (malignant, benign and normal). Finally, an ensemble approach is proposed using the hard voting mechanism to improve the accuracy. With the dataset of mini-MIAS this proposed approach achieved 78.89% accuracy. Results demonstrated that the proposed breast cancer detection approach improves the performance of segmentation breast tumor regions.",
        "references": [
            "90501314674d826a2a92d6bab0ece643925d8f86",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "9f9f9ac575d597ee92ce257784a6c7785e8c3e60",
            "f349b8ed7351584fce22f689f780160c66781d7d",
            "a892188c5efc9595ebe3b7131b6d0072ba732578",
            "fa987300f738053cf8041183dec1e6480ea451d0",
            "be1531000feb53773d74ab4f3c6ac2e9c3b3e264",
            "53fceb33baa404a61d79f47bb184cb6aaaae534f",
            "dbb31fcecd867438b44b36cae348a6d87e2aae46",
            "18dbe0aea40054ac1770f596b3a8f9f78ef2a7ef"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "35"
    },
    {
        "Id": "ddf4b612c1c944dbf47999b9c319c8186df709e0",
        "title": "Automated Deep Learning Empowered Breast Cancer Diagnosis Using Biomedical Mammogram Images",
        "authors": [
            "Jos{\\&#x27;e} Escorcia-Gutierrez",
            "Romany Fouad Mansour",
            "Kelvin Bele{\\~n}o",
            "Javier Jim{\\&#x27;e}nez-Cabas",
            "Meglys P{\\&#x27;e}rez",
            "Natasha Madera",
            "Kevin Velasquez"
        ],
        "date": "2022",
        "abstract": "An automated deep learning based breast cancer diagnosis (ADL-BCD) model using digital mammograms is presented that outperforms the state of art methods in terms of different measures. : Biomedical image processing is a hot research topic which helps to majorly assist the disease diagnostic process. At the same time, breast cancer becomes the deadliest disease among women and can be detected by the use of different imaging techniques. Digital mammograms can be used for the earlier identification and diagnostic of breast cancer to minimize the death rate. But the proper identification of breast cancer has mainly relied on the mammography findings and results to increased false positives. For resolving the issues of false positives of breast cancer diagnosis, this paper presents an automated deep learning based breast cancer diagnosis (ADL-BCD) model using digital mammograms. The goal of the ADL-BCD technique is to properly detect the existence of breast lesions using digital mammograms. The proposed model involves Gaussian filter based pre-processing and Tsallis entropy based image segmentation. In addition, Deep Convolutional Neural Network based Residual Network (ResNet 34) is applied for feature extraction purposes. Specifically, a hyper parameter tuning process using chimp optimization algorithm (COA) is applied to tune the parameters involved in ResNet 34 model. The wavelet neural network (WNN) is used for the classification of digital mammograms for the detection of breast cancer. The ADL-BCD method is evaluated using a benchmark dataset and the results are analyzed under several performance measures. The simulation outcome indicated that the ADL-BCD model outperforms the state of art methods in terms of different measures. presents an automated deep learning based breast cancer diagnosis (ADL-BCD) model using digital mammograms. The proposed model involves Gaussian filter based pre-processing and Tsallis entropy based image segmentation. In addition, Deep Convolutional Neural Network based Residual Network (ResNet 34) is applied for feature extraction purposes. Specifically, a hyper parameter tuning process using chimp optimization algorithm (COA) is applied to tune the parameters involved in ResNet 34 model. The wavelet neural network (WNN) is used for the classification of digital mammograms for the detection of breast cancer. The ADL-BCD method is evaluated using a benchmark dataset and the results are analyzed under several performance measures.",
        "references": [
            "34afbba50952c137539d525fa21bab026cce7250",
            "b9e840bab3facd93db9bdf9f3872b987fe7b0e11",
            "31cde645eb10b6dac5ab575267878d262429e310",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "3f02ff7afca151f2abfe14a863ef0d4b5a8b1393",
            "f36b3ee8d83ec14808a268183e28f05f4b7b3bc3",
            "257438fca36f8e1bcf1ca7aedc70364603496818",
            "de3c664200e753d01931229fb9a84346575524e9",
            "87466f870df7165622478e5a8834174ecb8093d2",
            "055f5fd928e3c9725c8ff9785fd6f0d3f503cc32"
        ],
        "related_topics": [],
        "reference_count": "25",
        "citation_count": "18"
    },
    {
        "Id": "ea703124880e7bc03792602b500a8ed37b56843a",
        "title": "Computer aided mass segmentation in mammogram images using Grey wolf Optimized Region growing technique",
        "authors": [
            "Ashi Ashok",
            "Devi Vijayan",
            "Lavanya R"
        ],
        "date": "3 June 2021",
        "abstract": "The proposed methodology achieved a highest accuracy of 96% by the fusion of global texture feature GLCM and LBP, which differentiates the masses as either benign or malignant in nature. One of the dangerous threats, that affect women all around the globe is breast cancer, leading to early mortality in women. According to researchers the survival rate of the breast cancer affected person can be improved by a greater amount by its early detection. Hence, there is need for development of an automated system, which can act as an aid for supporting the radiologists in making proper diagnostic decision. The proposed work involves detection of the breast masses by making use of an optimized region growing method, in which the optimal seed point selection and optimal threshold generation was achieved using Grey Wolf Optimization (GWO). In the proposed work the extraction of both global and local features are being considered. The global features considered includes shape features, Grey Level Co-occurrence Matrix (GLCM) and Grey Level Run Length Matrix (GLRLM) for extracting texture feature and local texture feature is extracted using Local Binary Pattern (LBP) and Scale invariant feature transform (SIFT). The fusion of the local and global features were being fed to Support Vector Machine (SVM) classifier, which differentiates the masses as either benign or malignant in nature. The proposed methodology achieved a highest accuracy of 96% by the fusion of global texture feature GLCM and LBP.",
        "references": [
            "8271f755d7cea799600e25662dd3f2ac8d23aeb1",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "4e70af0046174c2b7e8c28dc31b775470c9ea278",
            "19886c0d6b51b773dec8bc596cc5a3efbc9af88a",
            "9f8034a9f7448f59d0ddd4485d6e01dff2b270f9",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "173fcccefc89584eb68e089aadc0cac34a396efe",
            "84ea54639224471efa2b97d4a709f6e64c64816b",
            "a311571b9d475601eba635209bf78a90992102d2",
            "7ae2477b1a1df98d85497bf967a8f15738167b68"
        ],
        "related_topics": [],
        "reference_count": "26",
        "citation_count": "One"
    },
    {
        "Id": "f038517036bf5974ed662cdab74089d2db5e02b4",
        "title": "Breast tumor localization and segmentation using machine learning techniques: Overview of datasets, findings, and methods",
        "authors": [
            "Ramin Ranjbarzadeh",
            "Shadi Dorosti",
            "Saeid Jafarzadeh Ghoushchi",
            "A. Caputo",
            "Erfan Babaee Tirkolaee",
            "Sadia Samar Ali",
            "Zahra Arshadi",
            "Malika Bendechache"
        ],
        "date": "1 December 2022",
        "abstract": "Semantic Scholar extracted view of \"Breast tumor localization and segmentation using machine learning techniques: Overview of datasets, findings, and methods\" by R. Ranjbarzadeh et al.",
        "references": [
            "7c66cab0d95d65d0d12875f7c41547f75a684903",
            "7c8c21129cccf09508de82dd9ea785163f3e7834",
            "d6d67c6fd8c7d32346c253b446236d8b00d5f233",
            "d8a233cd1d6839b35abe606bd82882a05a24a362",
            "94da6a45acc2669b48fd06000313bdae05d6131a",
            "e1bb6577c62ebc8c05c2d5b0f1faf483139c4f4f",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "34599ec5ffe911a4350f6b89fcb6b11c0f9c2b79",
            "dfc4656c4fd266300c8b3ea63cf062440250e2cf",
            "85ec2efc743d270269de2a77a07b26bbcb690aa2"
        ],
        "related_topics": [
            "Cylindrical Algebraic Decomposition",
            "Supervised",
            "Deep Learning",
            "Radiologist",
            "Computer Vision",
            "Machine Learning",
            "Region Of Interest"
        ],
        "reference_count": "171",
        "citation_count": "21"
    },
    {
        "Id": "19e8f7afcba501eab8bff7c216ee532f0c73ed4f",
        "title": "A Robust Feature Extraction Technique for Breast Cancer Detection using Digital Mammograms based on Advanced GLCM Approach",
        "authors": [
            "L. Kanya Kumari",
            "B. N. Jagadesh"
        ],
        "date": "11 January 2022",
        "abstract": "A four-step process to extract features by using a novel technique called Advanced Gray-Level Co-occurrence Matrix (AGLCM) from pre-processed images and to classify the images using machine learning algorithms is proposed. INTRODUCTION: Breast cancer is the most hazardous disease among women worldwide. A simple, cost-effective, and efficient screening called mammographic imaging is used to find the breast abnormalities to detect breast cancer in the early stages so that the patient\u2019s health can be improved. OBJECTIVES: The main challenge is to extract the features by using a novel technique called Advanced Gray-Level Co-occurrence Matrix (AGLCM) from pre-processed images and to classify the images using machine learning algorithms. METHODS: To achieve this, we proposed a four-step process: image acquisition, pre-processing, feature extraction, and classification. Initially, a pre-processing technique called Contrast Limited Advanced Histogram Equalization (CLAHE) is used to increase the contrast of images and the features are retrieved using AGLCM which extracts texture, intensity and shape-based features as these are important to identify the abnormality. RESULTS: In our framework, a classifier called eXtreme Gradient Boosting (XGBoost) is applied on mammograms and the results are compared with other classifiers such as Random Forest (RF), K-Nearest Neighbor (KNN), Artificial Neural Networks (ANN), and Support Vector Machine (SVM). The experiments are done on the Mammographic Image Analysis Society (MIAS) dataset. CONCLUSION: The outcome achieved with CLAHE+ AGLCM+ XGBoost classifier is better than the existing methods. In future, we experiment on large datasets and also concentrate on optimal features selection to increase the classification. Neighbor, Artificial Neural Network, Random Forest and eXtreme Gradient Boosting.",
        "references": [
            "771eded215d7834d06b32b251aadc43f84a5be42",
            "870d52e025216445bbc51434527f450c7be630fd",
            "ef808555e64b318b1415bf9c87aeaa208a88f954",
            "7c66be06a11210b10afe5dddde51f7c355b98b14",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "fb53de4e80dd721cbc891caed0d63084a32b5a90",
            "14a65f82a9d7e25eeae4bdcf20090a5d6ead1f77",
            "3e5294e4ccec9ad1d2e8d3d6520e6db66209328e",
            "70ffedec6058b04159501170bb1c4e30e1769075",
            "a86be89ac1d21b7db5f0e1f71c752061118cc9d0"
        ],
        "related_topics": [
            "Classifier",
            "Contrast Limited Adaptive Histogram Equalization",
            "XGBoost",
            "Mammographic Image Analysis Society",
            "Random Forests",
            "Classification",
            "K-nearest Neighbors"
        ],
        "reference_count": "54",
        "citation_count": "7"
    },
    {
        "Id": "75212204e96763e1e8d505740c0097afa45ce77a",
        "title": "Mammogram Classification using Supervising Vector Machine and K-Nearest Neighbors for Diagnosis of Breast Cancer",
        "authors": [
            "Shada Omer Khanbari",
            "Adel Sallam M. Haider"
        ],
        "date": "1 December 2019",
        "abstract": "The proposed method, is incorporating the Local Contrast (LC) with the Contrast Limited Adaptive Histogram Equalization (CLAHE) to increase the contrast enhancement and to improve the appearance of the image. Breast cancer attacks women in their early productive years of life which become a public health problem, but if detected earlier it will be cured out with limited resources, while retching the advanced stage treating disease is too expensive and often poor outcome. The aim of this research is to obtain a method to classify the breast into either normal or abnormal tissues. The proposed method which is produced in this paper, is incorporating the Local Contrast (LC) with the Contrast Limited Adaptive Histogram Equalization (CLAHE), that will increase the contrast enhancement and to improve the appearance of the image. Region growing technique is used to extract and crop the region of interest (ROI), that contains the tumor with the texture features of that region automatically, with the help of using the Gray Level Co-occurrence Matrix (GLCM) technique. These features are fed into the Fine Gaussian Supper Vector Machine (SVM) classifier. As observed from the performance evaluation the proposed method classifies the mammography images with 97 % accuracy, 95% specificity and 98 % sensitivity.",
        "references": [
            "e90c52fc4099e2ad448563c1d368c97d78e0f086",
            "2f27a7b64c2223ebd922edb0b9644e76ffae0d14",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "078d88b4fde0a803d6c81c217bf65311b1581128",
            "0003a45ae653fcccb568b90dbb339cf5811a18bf",
            "8967ea71b194a28744c15a3be93fa6e32ddac1e9",
            "60608d5239008646f42c77275e54e7f02295f4ee",
            "551d601457375a6e7306ceb64860e52b6ad46ab7",
            "2137a584aabdb8cc81df773375b0b24632721b84",
            "408db8907a13129b0bca88c2c7991424af6b26cb"
        ],
        "related_topics": [
            "Contrast Limited Adaptive Histogram Equalization",
            "Contrast Enhancement",
            "Tumors",
            "Classifier",
            "Region Of Interest",
            "K-nearest Neighbors",
            "Support Vector Machines",
            "Local Contrast",
            "Texture Feature",
            "Mammogram Classification"
        ],
        "reference_count": "0",
        "citation_count": "24"
    },
    {
        "Id": "840e3ac4f017b602068b1c04a43d88761e53fb94",
        "title": "Mammogram Classification with Forest Optimization using Machine Learning Algorithms",
        "authors": [
            "L. KanyaKumari",
            "S. Jayaprada",
            "J RangaRao"
        ],
        "date": "2021",
        "abstract": "This work proposes a 6 step process which includes preprocessing, feature extraction, feature selection, splitting the data into training and testing, classification and performance measure, and shows that LBP based FOA with RF classifier achieved good accuracy in classifying the mammograms. Introduction: The deadly disease in Indian women is Breast Cancer (BC). A mammogram is used for identifying the tumours in the breast in the early stages which is efficient and cost-effective. Objective: The main objective is to predict BC in the early stages using image processing and machine learning techniques. Methods: Our proposed methodology is 6 step process which includes preprocessing, feature extraction, feature selection, splitting the data into training and testing, classification and performance measure. Results: The experiments are done on MIAS (Mammogram Image Analysis Society) dataset. As more noise in the images of this dataset, filters are applied to get more clarity in images. Features are extracted by Local Binary Patterns (LBP) and optimized by Forest Optimization Algorithm (FOA). These features are divided into 70% training and 30% testing data for classification. The classifiers used are KNearest Neighbor (KNN), Na\u00efve Bayes (NB) and Random Forest (RF). Conclusion: The experiments show that LBP based FOA with RF classifier achieved good accuracy in classifying the mammograms.",
        "references": [
            "8271f755d7cea799600e25662dd3f2ac8d23aeb1",
            "5683291fbde14e1afaa76a701f5ab7ad20ba86ea",
            "e015aafd5176c4e56bb1f64421fd479c76ca3866",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "a57698955faabc72d170ddcc39db7c4e6d94d491",
            "b3a8cf96a2f147bd06713d6be8f9f6464c75d13d",
            "0ded37a69e31887fee59dd0fb1e03ca07d45c87c",
            "618c4131cb6bc59f44e7aa74779f0d446c0400b1",
            "e8eadb51a5b98e526aed261f5bc68e00831fb3bc",
            "95274ca3be569765960464d24f898c6fe025bac9"
        ],
        "related_topics": [
            "Local Binary Patterns",
            "Machine Learning",
            "Classification",
            "Mammograms",
            "Random Forests",
            "Classifier",
            "Feature Selection",
            "Mammogram Classification",
            "Mammographic Image Analysis Society",
            "Forest Optimization Algorithm"
        ],
        "reference_count": "0",
        "citation_count": "24"
    },
    {
        "Id": "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
        "title": "An enhancement of mammogram images for breast cancer classification using artificial neural networks",
        "authors": [
            "Jalpa J. Patel",
            "Sarman K. Hadia"
        ],
        "date": "1 June 2021",
        "abstract": "In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification of mammogram images. Breast cancer is the most driving reason for death in women in both developed and developing nations. For the plan of effective classification of a system, the selection of features method must be used to decrease irregularity part in mammogram images. The proposed approach is used to crop the region of interests (ROIs) manually. Based on that number of features are extracted. In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification. A number of selected features is applied to train the neural network. In this proposed method accessible informational index from the mini\u2013mammographic image analysis society (MIAS) database was used. The classification of this mammogram database involved a neural networks classifier which attained an accuracy of 99.7% with a sensitivity of 99.5%, and specificity of 100% as the area under the curve (AUC) is 0.9975 and matthew\u2019s correlation coefficient (MCC) represents a binary class value which reached the value of 0.9931. It can be useful in a computer-aided diagnosis system (CAD) framework to help the radiologist in analyzing breast cancer. Results achieved with the proposed method are better compared to recent work.",
        "references": [
            "b7cd3daaf49d0d68579015680d123044683d70ee",
            "b06a69cf5663829d4f4f8168d820b2f7baf88918",
            "dd7282d139c163df0243f7cb508c7be979aa9fda",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "4a0c851fbffcfdd4d191cff4b38ac42e1d4d6fc9",
            "95bb0ee471480da79e41ae196bb4da02abe52a27",
            "d592e7ed372adffd2d2d8f5c72565d71e911237e",
            "ca30be76395ba443b24d20d7df5d3b5372df55ff",
            "0fc4a247cce443bc2a3d64818d8981ab8586bb32",
            "8133e66c9c03095ef605090e6a72b752dc774d92"
        ],
        "related_topics": [
            "Classification",
            "Region Of Interest",
            "Neural Network",
            "Radiologist",
            "Cylindrical Algebraic Decomposition",
            "Area Under The ROC Curve"
        ],
        "reference_count": "42",
        "citation_count": "10"
    },
    {
        "Id": "169f59ac3987301bfdecd77301eeb16ecc0e9358",
        "title": "Early detection of breast cancer using hybrid of series network and VGG-16",
        "authors": [
            "Gul Shaira Banu Jahangeer",
            "T. Dhiliphan Rajkumar"
        ],
        "date": "30 October 2020",
        "abstract": "The novelties have given in all the image processing aspects such as filtering, segmentation, feature extraction and classification and the conclusion of the proposed work has attained enhanced results on comparing with other state of the art approaches. Breast cancer is nowadays becoming a serious problem and acts as a main reason for death of women around the world. Hence various devices are being utilized for the detection of breast cancer at an earlier stage and diagnosing it in an earlier stage might even results in complete cure of the disease. Among the wide range of devices available, mammogram is one of the commonly employed and most effective approaches involved in the detection of breast cancer. It records the affected area in the form of mammogram images and these images are processed through image processing techniques for the detection of cancer affected regions. In this paper, novelties have given in all the image processing aspects such as filtering, segmentation, feature extraction and classification. The salt and pepper noises in the mammogram images are eliminated by the usage of novel decision based partial median filter. Then the filtered images are segmented based utilizing a novel technique which is formed on integrating the deep learning techniques of VGG-16 and series network. Features of the segmented images have extracted through BAT-SURF feature extraction, where the orientation of the interest points are extracted using Bat optimization algorithm along with SURF (i.e.) Speeded up Robust Features. It extract most important key points from SURF features and then the extracted image has classified by using the novel Gradient descent decision tree classifier in which a stable learning path provided for easy convergence. Then the performance of the proposed system has analyzed based on the performance metrics like accuracy, specificity, sensitivity, recall, precision, Jaccard coefficient, F score and missed classification. Based on the results obtained, the conclusion of the proposed work has attained enhanced results on comparing with other state of the art approaches. The accuracy value of the proposed hybrid VGG-16 and series network segmentation technique determined as 96.45 and similarly the accuracy value of the proposed Gradient Descent Decision Tree Classification technique has value shows 95.15.",
        "references": [
            "ca6a03d054bf28155509dd61b37878d4e4fcc831",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "54bfd99db17b0a85699f3d1a7582f526df46aa6c",
            "5c01842e5431bff0a47140c57b3dd5362107afc6",
            "be4c4ef2c0dd7c52112bebc8b98a74947673a255",
            "3089efdb9f20f81a8264f6e703c9f8fb7db13ee4",
            "f325e6c3ab1f2a85311310aa805eb526e37c4fdf",
            "9a2536a4920e4d7c4f70864ead78f3e44a276a13",
            "d47177c238f633b9092d719438e372eb75acbfa7",
            "b757776861a430dafa3d4ad2729f173716c88e23"
        ],
        "related_topics": [
            "VGG16",
            "Mammograms",
            "Classification"
        ],
        "reference_count": "26",
        "citation_count": "23"
    },
    {
        "Id": "9830278ec1efee0fb3c5a84ec8bf28dced81fbff",
        "title": "Detection and classification of the breast abnormalities in Digital Mammograms via Linear Support Vector Machine",
        "authors": [
            "Amara Nedra",
            "Muhammad Shoaib",
            "Said El Gattoufi"
        ],
        "date": "1 March 2018",
        "abstract": "An approach to detect tumors in mammogram images via linear support vector machine classifier to distinguish between two classes of patients: those with benign or malignant tumor in Digital Mammograms via Linear Support Vector Machine classifier. This paper presents an approach to detect tumors in mammogram images. Early detection of breast cancer is key to scheming extremely good treatment strategies. The objective of this work is to distinguish between two classes of patients: those with benign or malignant tumor in Digital Mammograms via Linear Support Vector Machine classifier. The proposed methodology has been implemented in three steps: 1) estimation of an efficient k value selection for k-means segmentation of breast tissues; 2) using the fast and robust features descriptor Bag of Features based on SURF interest point for the extraction of features from the segmented tumor region; 3) the linear support vector machine classifier will be trained for the predication of tumor type into benign or malignant. The performance analysis of our proposal work is compared with three other state-of-the-art classifiers, BPN, KNN, and Hybrid RGSA. The experiments show that we succeeded to improve the accuracy for Benign and Malignant Breast tumors to 99.0%.",
        "references": [
            "95bb0ee471480da79e41ae196bb4da02abe52a27",
            "d0139c82ceeadb6987add17973245657075de94d",
            "fc3e53a00559dc17a6a35fd8c2e9e9df94fb9232",
            "c6db34ade32b3681a92068b22a354903b2953d52",
            "eb8f6f8c48e6b61ea35e7294f26f7cfbbc3dd833",
            "1617e725b6a51cccf1048e7d8dd11dcb6d65c726",
            "7ac1f3e2ce898f52bfb2477be81b8a0993aaae6d",
            "325cfc0ecd5921b27e4ce83b16cd26ede2bdfced",
            "f25ade2078e35112f0cb9fbc7abdac7990e99594",
            "e279168d0b2d4e231ec2c179b66934fdb9cff1a0"
        ],
        "related_topics": [
            "Malignant",
            "Tumors",
            "Feature Descriptors",
            "Bag-of-features",
            "Tumor Regions",
            "Classification",
            "Linear Support Vector Machines",
            "K-nearest Neighbors"
        ],
        "reference_count": "10",
        "citation_count": "12"
    },
    {
        "Id": "5384355c1dcb8b6325b351e2317cf16eb8a2aba6",
        "title": "A comparison of methods for three-class mammograms classification.",
        "authors": [
            "Marina Milo{\\vs}evi{\\&#x27;c}",
            "\u017deljko Jovanovi{\\&#x27;c}",
            "Dragan Jankovic"
        ],
        "date": "2017",
        "abstract": "A CAD system based on feature extraction techniques for detecting abnormal patterns in digital mammograms is presented and experimental results indicate that the proposed three-class SVM classifier is more suitable for practical use than the other two methods. BACKGROUND\nMammography is considered the gold standard for early breast cancer detection but it is very difficult to interpret mammograms for many reason. Computer aided diagnosis (CAD) is an important development that may help to improve the performance in breast cancer detection.\n\n\nOBJECTIVE\nWe present a CAD system based on feature extraction techniques for detecting abnormal patterns in digital mammograms.\n\n\nMETHODS\nComputed features based on gray-level co-occurrence matrices (GLCM) are used to evaluate the effectiveness of textural information possessed by mass regions. A total of 20 texture features are extracted from each mammogram. The ability of feature set in differentiating normal, benign and malign tissue is investigated using a Support Vector Machine (SVM) classifier, Naive Bayes classifier and K-Nearest Neighbor (k-NN) classifier. The efficiency of classification is provided using cross-validation technique. Support Vector Machine was originally designed for binary classification. We constructed a three-class SVM classifier by combining two binary classifiers and then compared his performance with classifiers intended for multi-class classification. To evaluate the classification performance, confusion matrix and Receiver Operating Characteristic (ROC) analysis were performed.\n\n\nRESULTS\nObtained results indicate that SVM classification results are better than the k-NN and Naive Bayes classification results, with accuracy ratio of 65% according to 51.6% and 38.1%, respectively.The unbalanced classification that occurs in all three classification tests is reason for unsatisfactory accuracy.\n\n\nCONCLUSIONS\nObtained experimental results indicate that the proposed three-class SVM classifier is more suitable for practical use than the other two methods.",
        "references": [
            "f247c2ef036dc00c72af2d10e767f18e48396d4e",
            "a051841aba1941df6cea66e23c9040a6c6599283",
            "f8b061df30fa1eb1ca444cedf2e9be9d091120f2",
            "e837a09bf1e7447feac78fc81f8332a432812de8",
            "4286ad40053345fdb083856bbe17312c23d7460e",
            "11c335bf079f34576bc742f003ce3c70d3de95e1",
            "7d585ecf2875a19f811924f4d5371e682d8b54fd",
            "031efc355e9a6b0804d5e2111ff81a88e22f831d",
            "4479cdb58e14ad61a616bd1bb8068d595aca203e",
            "d553bc9d6088ee7395a4a7db674c757376e3557d"
        ],
        "related_topics": [
            "Support Vector Machines",
            "Mammograms",
            "Classification",
            "Cylindrical Algebraic Decomposition",
            "Multi-class Classification",
            "Binary Classification",
            "Texture Feature",
            "K-nearest Neighbors"
        ],
        "reference_count": "18",
        "citation_count": "9"
    },
    {
        "Id": "241b3094b2cc3682df765ed3ed70f8a616358729",
        "title": "Mammogram classification using two dimensional discrete wavelet transform and gray-level co-occurrence matrix for detection of breast cancer",
        "authors": [
            "Shradhananda Beura",
            "Banshidhar Majhi",
            "Ratnakar Dash"
        ],
        "date": "22 April 2015",
        "abstract": "Semantic Scholar extracted view of \"Mammogram classification using two dimensional discrete wavelet transform and gray-level co-occurrence matrix for detection of breast cancer\" by S. Beura et al.",
        "references": [
            "42c96b89162413be0f14d5005618a979625994b8",
            "09c937b7a1d4defa5c73ed6ed3c9a467a81e95c4",
            "9ceb5334ead06800781f04bbe6843f2d78564558",
            "c4489eadf3148702292ea59a2d935d3948a1c0b2",
            "c50adb3ac7151aee41f2d7a12694af50ad944c97",
            "3653ba5ff4146a7be5066ef8d6b0530d6ac62a87",
            "001ec101a06cf8ce254db94a9c6130b5fe43aabe",
            "e8b3abe234bd9a61aa950388236e86d629b9136e",
            "34c44883a6152c5298f2c452670c1127072400e6",
            "e2ad29cca43ef504569365ab6484fe26488983ee"
        ],
        "related_topics": [
            "Mammogram Classification",
            "Malignant",
            "Mammographic Image Analysis Society",
            "Feature Matrix",
            "Digital Database For Screening Mammography",
            "Receiver Operating Characteristics",
            "Region Of Interest",
            "Area Under The ROC Curve"
        ],
        "reference_count": "26",
        "citation_count": "201"
    },
    {
        "Id": "1d22adf2080a7a5e98739e11419a9e837de21432",
        "title": "Textural Feature Based Classification of Mammogram Images Using ANN",
        "authors": [
            "Ivarani Routray",
            "Nrusingha Prasad Rath"
        ],
        "date": "1 July 2018",
        "abstract": "A method using Laws Texture Energy Measure (LTEM) as an approach for breast cancer detection using the energy maps of the feature matrix for the calculation of feature vector is proposed. Breast cancer is the most common cancer in women worldwide which leads to death of the patient if not detected early. Breast cancer can be detected early with the help of mammogram images. Mammography is a standard imaging modality for the diagnosis and screening of breast cancer. However, suitable image processing techniques are required to detect such ambiguities. Textural analysis of such image is used to detect the cancer tissues. In this paper, we propose a method using Laws Texture Energy Measure (LTEM) as an approach for breast cancer detection. The LTEM method uses the energy maps of the feature matrix for the calculation of feature vector. A Back-propagation method using Artificial Neural Network (ANN) is used to classify the normal, benign and malignant tissue region. The mammography images are obtained from Mammographic Image Analysis Society (MIAS) database for experimentation and analysis. The proposed method is validated in comparison with different back-propagation algorithms and the results show the superiority of the proposed model.",
        "references": [
            "2f27a7b64c2223ebd922edb0b9644e76ffae0d14",
            "ebbd243374cb63886ab2fed559cf2c27a90ce7a5",
            "24befaf17c9e6dcc8a003ab261147c8d31602b1a",
            "87062858d1b1d59a563be3ea53cd06685eba79b7",
            "c8c81a85bb3387c897e233173e1a0264e8d3e654",
            "9692d71185d79281e82ffa25d0bf8f3c1dbf1659",
            "ac671a9ec9d2f1a3c487cb70c152193b7659fc80",
            "7a9bc54b6cd9a65fcd40c5acec4c706e6c5574b3",
            "3f88a7b95f8796d9fc7fb5b19b4cad2b310736c1",
            "69ea958bbc4025b32a644be25a0d0e87d816feef"
        ],
        "related_topics": [
            "Mammographic Image Analysis Society",
            "Approximate Nearest Neighbor",
            "Feature Vector",
            "Laws Texture Energy Measures",
            "Feature Matrix"
        ],
        "reference_count": "12",
        "citation_count": "6"
    },
    {
        "Id": "82b09f56d68eefdb3a3442cfe29433cd72480d64",
        "title": "Automatic classification of masses from digital mammograms",
        "authors": [
            "Basma A. Mohamed",
            "Nancy M. Salem"
        ],
        "date": "1 March 2018",
        "abstract": "An automatic method for mass classification from mammograms is introduced to distinguish among benign and malignant masses with the aid of the usage of the three classifiers; artificial neural network, support vector machines, k-nearest neighbor. Cancer of the breast remains to be one among the foremost factors behind death between women. The early detection and recognition of breast cancer increases the probabilities of complete and successful recovery; therefore, the mortality rate can be reduced. Currently, screening mammography is the most successful technique for detecting masses or abnormalities which are related to breast cancer in its earliest stages. In this paper, an automatic method for mass classification from mammograms is introduced. The proposed algorithm includes three main steps. First, three different types of features are separated from the mass. Then the most relevant features are selected using the T-test algorithm. Finally, the classification step is done to distinguish among benign and malignant masses with the aid of the usage of the three classifiers; artificial neural network, support vector machines, k-nearest neighbor. The system is rated using 250 mammograms from the Digital Database for Screening Mammography (DDSM). The artificial neural network occupies the best results of 98.9 % for accuracy, 100 % for sensitivity, and 97.8 % for specificity.",
        "references": [
            "b133e642a979d29975c3cc882d2e68c5321c0957",
            "98304d3f02517f90e85d889b9ed188dc5412dcaf",
            "2534385a42333202e1f608d5cfbfcd21b26ced03",
            "34c44883a6152c5298f2c452670c1127072400e6",
            "2ec58f806ff4d2b42209c557ae5eaf452fb0a04d",
            "01da0f8ef03129f2fcb33cdbffc3cfa4babdf2c4",
            "17e835c97356ef662ed3de5eb4114ae15a308f2c",
            "185a5045602e2b72c38d1e1ddedb5994f0878d41",
            "bceaf7ca770996b6c113226521d9afead15b769f",
            "0aa99ff5152b31c7c86e97d0dda82bcd866fa511"
        ],
        "related_topics": [
            "Mammograms",
            "SCREENING MAMMOGRAPHY",
            "Digital Database For Screening Mammography",
            "Mass Classification",
            "K-nearest Neighbors"
        ],
        "reference_count": "22",
        "citation_count": "10"
    },
    {
        "Id": "9f8034a9f7448f59d0ddd4485d6e01dff2b270f9",
        "title": "The Analysis of Digital Mammograms Using HOG and GLCM Features",
        "authors": [
            "Krishna Chaitanya Tatikonda",
            "Chandra Mohan Bhuma",
            "Samayamantula Srinivas Kumar"
        ],
        "date": "1 July 2018",
        "abstract": "An algorithm for early detection of breast cancer is proposed in this work, an algorithm using the concatenation of simple Histogram of Oriented Gradients and Grey Level Co-Occurrence Matrix features are used for classification. An algorithm for early detection of breast cancer is proposed in this paper. Breast cancer is one disease if detected early, can be cured effectively. Failure of early detection is causing many deaths among woman worldwide. Early detection requires proper screening. Digital mammograms are useful for this purpose as they are non-invasive. But data available from the mammograms is often difficult to interpret and analyse. Many algorithms are available in the literature for analysis of digital mammograms. In this work, an algorithm using the concatenation of simple Histogram of Oriented Gradients (HOG) and Grey Level Co-Occurrence Matrix (GLCM) features are used for classification. The database used is a benchmark database MIAS. All the abnormal images of the database are chosen for the experimentation. The images in the database are of poor contrast and are having some noisy pixels. ROIs are extracted from the images. All the ROI (Region of Interest) images are pre-processed with median filtering and Contrast Limited Adaptive Histogram Equalization (CLAHE). Several classifiers are used to assess the performance of the proposed algorithm i.e., KNN, CT, SVM (RBF kernel), ANN, LDA, and Naive Bayes classifier. Performance metrics used in this work are sensitivity, specificity, precision and accuracy. Results are compared with the existing algorithms available in the literature and superiority of the proposed method is presented as the proposed method obtained an accuracy of 99.11%.",
        "references": [
            "d660065106eebbae8824d2ab644afe18ff64cd8e",
            "083c262c4fede6e477326671086611ae53ec9223",
            "28ecaba9ac93017f62798314999feba355b03811",
            "51b3beab5d8b31f7e2771b1ae64aeff3dd12c36d",
            "74019c54322dea3c5d14e5761f23885290c998f4",
            "8271f755d7cea799600e25662dd3f2ac8d23aeb1",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "2dfb1fd3adfa58a3448251e03b1a5a78239958b3",
            "f7e9887b21d4b10a3054e73326529cff32c18091",
            "afb2f5fb9602035b8982c854e6009b7e34cd715c"
        ],
        "related_topics": [
            "Contrast Limited Adaptive Histogram Equalization",
            "Classifier",
            "Support Vector Machines",
            "Histograms Of Oriented Gradient",
            "Mammographic Image Analysis Society",
            "Abnormal Images",
            "Latent Dirichlet Allocation",
            "Approximate Nearest Neighbor",
            "Region Of Interest",
            "K-nearest Neighbors"
        ],
        "reference_count": "19",
        "citation_count": "17"
    },
    {
        "Id": "db06e9623dbe034a8e5d351011148be70bc83dab",
        "title": "Mammogram Density Classification using Double Support Vector Machines",
        "authors": [
            "Yi-Chong Zeng"
        ],
        "date": "1 October 2018",
        "abstract": "A scheme to classify breast densities based on support vector machine (SVM) and voting system and results demonstrate that the proposed scheme is capable of classifying breast density and is better than the compared approaches. Dense breast increases risk of breast cancer. There were of lesion diagnosis difficulties on dense breast for doctors, not least computer aided detection. In this paper, we propose a scheme to classify breast densities based on support vector machine (SVM) and voting system. First, 41 kinds of features are extracted from mammogram images. The first SVM is applied to the features for segmentation of mammary gland and fatty. Subsequently, one-dimension histogram is derived from score map of the first SVM. The second SVM is applied to the histogram to train multiple classifiers, and then classifies mammogram density. The experiment results demonstrate that the proposed scheme is capable of classifying breast density and is better than the compared approaches.",
        "references": [
            "26b8feb26bd320ed98d7a5d811acc2b376280c5b",
            "a47b4939945e9139ccbf37e2f232d5c57583b385",
            "13950c4bf8d997c955956435075bc7e02755caab",
            "da81519c82d3bfd22111b0ff0cd54e6a01712b9c",
            "b6754bc41f7af3875f119cb7b7590d3e42fe5340",
            "47c18899d0d264ff72ca2376ed8bb2d821f13a98",
            "ff2218b349f89026ffaaccdf807228fa497c04bd"
        ],
        "related_topics": [
            "Support Vector Machines",
            "Density Classification",
            "Score Maps",
            "Mammograms"
        ],
        "reference_count": "7",
        "citation_count": "6"
    },
    {
        "Id": "cabeb7c149cfc54fac61431ed3f6a2d0e74269f7",
        "title": "A Breast Cancer Diagnosis Method Based on VIM Feature Selection and Hierarchical Clustering Random Forest Algorithm",
        "authors": [
            "Zexian Huang",
            "Daqi Chen"
        ],
        "date": "2022",
        "abstract": "The HCRF algorithm with VIM as a feature selection method reaches the best accuracy of 97.05% and 97.76% compared to Decision Tree, Adaboost and Random Forest on both the WDBC and WBC datasets. Breast cancer is a neoplastic disease which seriously threatens women\u2019s health. It is regard as the most common cause of cancer death in women. Accurate detection and effective treatment are of vital significance to lower the death rate of breast cancer. In recent years, machine learning technique has been considered to be an effective method for accurate diagnosis of various diseases, among which Random Forest (RF) has been widely applied. However, decision trees with poor classification performance and high similarity may be generated during the training process, which affects the overall classification performance of the model. In this paper, a Hierarchical Clustering Random Forest (HCRF) model is developed. By measuring the similarity among all the decision trees, the hierarchical clustering technique is used to carry out clustering analysis on decision trees. The representative trees are selected from divided clusters to construct the hierarchical clustering random forest with low similarity and high accuracy. In addition, we use Variable Importance Measure (VIM) method to optimize the selected feature number for the breast cancer prediction. Wisconsin Diagnosis Breast Cancer (WDBC) database and Wisconsin Breast Cancer (WBC) database from the UCI (University of California Irvine) Machine Learning repository are employed in this study. The performance of the proposed method is evaluated by utilizing accuracy, precision, sensitivity, specificity and AUC (Area Under ROC Curve). Experimental results indicate that the classification based on HCRF algorithm with VIM as a feature selection method reaches the best accuracy of 97.05% and 97.76% compared to Decision Tree, Adaboost and Random Forest on both the WDBC and WBC datasets. The method proposed in this study is an effective tool for diagnosing breast cancer.",
        "references": [
            "ee6a5ba83222c14c2fb384aa4ca7a74eb7b48e3b",
            "db52d8fcaee339552cbcebd80825e3ded12e87cb",
            "961b8dcd987155d0305aef21f431f89f4aa26020",
            "3221aaf0fe55dbdbd0812c36bcbca5b9f790d5d4",
            "ebc8e668175705ad7d4374173bfda05afd4527d9",
            "f69718839ee7de17cfd77e05fb2b25d22bbfd7ce",
            "7b14c2f742e2ea5f40e32e980ba3877d0b453fe1",
            "a2935c59a706fac24dca02ff284d77008dccb89d",
            "b4706bf5ce6bb764d6406dea2faa5913afe2d619",
            "e88d29116caa03e98757dba0ec4e5c1b45a7d8fe"
        ],
        "related_topics": [
            "Random Forests",
            "Variable Importance Measures",
            "AdaBoost",
            "Wisconsin Diagnosis Breast Cancer",
            "Unused Circuit Identification",
            "Area Under The ROC Curve",
            "Women's Health"
        ],
        "reference_count": "55",
        "citation_count": "23"
    },
    {
        "Id": "2a937dc9c5e24728bfee5e60045ca6d710930aa4",
        "title": "Analysis of tissue abnormality and breast density in mammographic images using a uniform local directional pattern",
        "authors": [
            "Mohamed Abdel-Nasser",
            "Hatem A. Rashwan",
            "Domenec Puig",
            "Antonio Moreno"
        ],
        "date": "30 December 2015",
        "abstract": "Semantic Scholar extracted view of \"Analysis of tissue abnormality and breast density in mammographic images using a uniform local directional pattern\" by M. Abdel-Nasser et al.",
        "references": [
            "26b8feb26bd320ed98d7a5d811acc2b376280c5b",
            "dda0dc40bc5cb79d5638d8cac955337362e6473e",
            "da81519c82d3bfd22111b0ff0cd54e6a01712b9c",
            "af0ee28605ce3f1e30d12adf232f717dbd544fba",
            "6d8c60d47e2be4de763bb2f8044e09981016396e",
            "8a28f08dff029fac4e92818fb56c949a72f41cfa",
            "13950c4bf8d997c955956435075bc7e02755caab",
            "93c25e81ead4525bda8ce7c15420b3472490778b",
            "20ee298c8ca7eefb73d0ee5be9cffcb5c4604b73",
            "19a85922bcc7c932934a79a85c59e15655d88b2e"
        ],
        "related_topics": [
            "Uniform Local Directional Pattern",
            "Mammograms",
            "Breast Density Classification",
            "Breast Density",
            "Breast Tissue Density Classification",
            "Tissue Classification",
            "Classification",
            "Mass Detection",
            "Region Of Interest",
            "Local Directional Pattern"
        ],
        "reference_count": "55",
        "citation_count": "55"
    },
    {
        "Id": "bb1beb57009f356e300ed2c83bb2fc4f8198daf3",
        "title": "A Novel CNN-Inception-V4-Based Hybrid Approach for Classification of Breast Cancer in Mammogram Images",
        "authors": [
            "Muhammad Saquib Nazir",
            "Usman Ghani Khan",
            "Aqsa Mohiyuddin",
            "Mana Saleh Al Reshan",
            "Asadullah Shaikh",
            "Muhammad Rizwan",
            "Monika D{\\&#x27;a}videkov{\\&#x27;a}"
        ],
        "date": "6 July 2022",
        "abstract": "This study proposes a novel-based hybrid approach, CNN-Inception-V4, based on the fusing of these two networks to develop a unique hybrid technique to identify breast cancer mass pictures as benign or malignant abnormalities. Breast cancer is the most frequent disease in women, with one in every 19 women at risk. Breast cancer is the fifth leading cause of cancer death in women around the world. The most effective and efficient technique of controlling cancer development is early identification. Mammography helps in the early detection of cancer, which saves lives. Many studies conducted various tests to categorize the tumor and obtained positive findings. However, there are certain limits. Mass categorization in mammography is still a problem, although it is critical in aiding radiologists in establishing correct diagnoses. The purpose of this study is to develop a unique hybrid technique to identify breast cancer mass pictures as benign or malignant. The combination of two networks helps accelerate the categorization process. This study proposes a novel-based hybrid approach, CNN-Inception-V4, based on the fusing of these two networks. Mass images are used in this research from the CBIS-DDSM dataset. 450 images are taken for benign, and 450 images are used for malignant. The images are first cleaned by removing pectoral muscles, labels, and white borders. Then, CLAHE is used to these images to improve their quality in order to produce promising classification results. Following preprocessing, our model classifies cancer in mammography pictures as benign or malignant abnormalities. Our proposed model\u2019s accuracy is 99.2%, with sensitivity of 99.8%, specificity of 96.3%, and F1-score of 97%. We also compared our proposed model to CNN, Inception-V4, and ResNet-50. Our proposed model outperforms existing classification models, according to the results.",
        "references": [
            "0bac967d54ecbca93cadeca63edf273e9ee9b6e6",
            "ce93c9b3ce2b3e2c51260a3a0d98b5f0848e8c27",
            "b4efe64814bb38a0f7bcc5ebcec2b978e71b8472",
            "1a88ddff5eec7c40ec7d1205277871664fa7c68c",
            "0677d83fbc116d6a9eb71ab3e73d45350720033d",
            "f174e9f2ac85407353bf1ea9cde1ee7a769b37fe",
            "ebc0a2c2433ff2c89794882c6c2afd005e24ef3d",
            "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "75b2813ee47c9c13dd48a634363dedc91640c093",
            "d6edaac8f8fc254e2c66470bb5e6d4c14f4dfad2"
        ],
        "related_topics": [],
        "reference_count": "55",
        "citation_count": "7"
    },
    {
        "Id": "f792c9fec8b7cfef3afed76a1e177349194780a2",
        "title": "IAES International Journal of Artificial Intelligence (IJ-AI)",
        "authors": [
            "Erlinda Ratnasari Putri",
            "Ahmad Zarkasi",
            "Prawito Prajitno",
            "Djarwani Soeharso Soejoko"
        ],
        "date": "2022",
        "abstract": "A novel methodology for the contour detection round the cervical organ classified with artificial neural network (ANN) which was employed to categorize the image data and Experimental results show that ANN model has better receiver operating characteristic (ROC) parameter values than SVM model\u2019s and existing approach's regarding 96.2% of sensitivity, 95.32% of specificity, and 95.75% of accuracy. Received Feb 16, 2022 Revised Jul 4, 2022 Accepted Aug 2, 2022 Cervical cancer is the second deadliest after breast cancer in Indonesia. Sundry diagnostic imaging modalities had been used to decide the location and severity of cervical cancer, one among those is computed tomography (CT) Scan. This study handles a CT image dataset consisting of two categories, abnormal cervical images of cervical cancer patients and normal cervix images of patients with other diseases. It focuses on the ability of segmentation and classification programs to localize cervical cancer areas and classify images into normal and abnormal categories based on the features contained in them. We conferred a novel methodology for the contour detection round the cervical organ classified with artificial neural network (ANN) which was employed to categorize the image data. The segmentation algorithm used was a region-based snake model. The texture features of the cervical image area were arranged in the form of gray level co-occurrence matrix (GLCM). Support vector machine (SVM) had been added to determine which algorithm was better for comparison. Experimental results show that ANN model has better receiver operating characteristic (ROC) parameter values than SVM model\u2019s and existing approach\u2019s regarding 96.2% of sensitivity, 95.32% of specificity, and 95.75% of accuracy.",
        "references": [
            "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "1bf47910c57b080fa56f226301d4e665c8f7cc27",
            "11e608a0f1246eb68a7c1c53f91f89bb6c9b0176",
            "28226ca4204425dfec5c3de0642b27716685050f",
            "89a46303708162be17a6154cea4edb11305f967f",
            "0b8f84ac274ddab5a3a6b3dbd599643b7d01c07c",
            "3e73ced82928a3d4c98bcc5fe3b8e9b448bda131",
            "e27d4e91374b7ddd3c9d5dc3d456821f0e3c7bac",
            "346dad56ca8207ffc41d9e0dc9cb5a3a7723f9eb",
            "372467fc4debb4a9a47baaba2d3b6d5f4af0e8da"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "29"
    },
    {
        "Id": "ad957ef4c7e2d3933f3ad129e82490441e6505fa",
        "title": "Bulletin of Electrical Engineering and Informatics",
        "authors": [
            "Novianti Puspitasari",
            "Anindita Septiarini",
            "Ummul Hairah",
            "Andi Tejawati",
            "Heni Sulastri"
        ],
        "date": "2023",
        "abstract": "This study involved several processes: image acquisition, region of interest (ROI) detection, pre-processing, feature extraction, and classification, which produced the highest accuracy value of 100% using the color and texture features with the SVM classifier. Received Oct 28, 2022 Revised Dec 16, 2022 Accepted Jan 29, 2023 The existence of machine learning has been exploited to solve difficulties in various fields, including the classification of leaf species in agriculture. Betel leaf is one of the plants that provide health advantages. The objective of using a machine learning approach is to classify the betel leaf species. This study involved several processes: image acquisition, region of interest (ROI) detection, pre-processing, feature extraction, and classification. The feature extraction used the combination features of color and texture. Furthermore, the classification applied four classifiers, including artificial neural network (ANN), K-nearest neighbors (KNN), Naive Bayes, and support vector machine (SVM). The evaluation in this study implemented cross-validation with a K-fold value of 5. The method performance produced the highest accuracy value of 100% using the color and texture features with the SVM classifier.",
        "references": [
            "dadcca70bba40a49b42116079f4ccd5c6f3c9588",
            "c38526e1f301aa23c68b7a25b1bfe5b71c2f668c",
            "a48e71a3bd5ea3ad87826403d36dcb9bba53fdaf",
            "26c5705831634f46f26099f6dd57347265736587",
            "da65a5849cdd06f7c8ae8d50f2d80397d1d9ee64",
            "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "b6556cf0f83046778503c06c014f75c559b09bbc",
            "ab8fdd1bc5ca4fda416aa7f8e33688dd1380f893",
            "295c946f3c2eefb6ee7ab7cf082c1bcf8d85ca9a",
            "67ad9f6b272c3b24b4ac649f4f344aa8a7b5bfe7"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "30"
    },
    {
        "Id": "95b1fce2fbe3b8a6fca7ea83825a0bc7945773e7",
        "title": "Brain stroke computed tomography images analysis using image processing: A Review",
        "authors": [
            "Nur Hasanah Ali",
            "Abdul Rahim Abdullah",
            "Norhashimah Mohd Saad",
            "Ahmad Sobri Muda",
            "Tole Sutikno",
            "Mohd Hatta Jopri"
        ],
        "date": "1 December 2021",
        "abstract": "This review is impactful in identifying the best automated segmentation technique to evaluate brain stroke and is expected to contribute new information in the area of stroke research. Stroke is the second-leading cause of death globally; therefore, it needs immediate treatment to prevent the brain from damage. Neuroimaging technique for stroke detection such as computed tomography (CT) has been widely used for emergency setting that can provide precise information on an obvious difference between white and gray matter. CT is the comprehensively utilized medical imaging technology for bone, soft tissue, and blood vessels imaging. A fully automatic segmentation became a significant contribution to help neuroradiologists achieve fast and accurate interpretation based on the region of interest (ROI). This review paper aims to identify, critically appraise, and summarize the evidence of the relevant studies needed by researchers. Systematic literature review (SLR) is the most efficient way to obtain reliable and valid conclusions as well as to reduce mistakes. Throughout the entire review process, it has been observed that the segmentation techniques such as fuzzy C-mean, thresholding, region growing, k-means, and watershed segmentation techniques were regularly used by researchers to segment CT scan images. This review is also impactful in identifying the best automated segmentation technique to evaluate brain stroke and is expected to contribute new information in the area of stroke research.",
        "references": [
            "8616eb1beb47070bb27904bece7a59d31f62a464",
            "a386eb5458eaf568e7d9a3a38916e459ac647f28",
            "83d0fbf72692b78945a3619b6a65848122ba85bd",
            "788ed6e8393baa81ee08b23422fac0a3c10b4e84",
            "43087caea64501a1135571f45b390fcca1faaa8e",
            "f72ed07670225e378aab2313aed6eade277c8289",
            "8c11f4787a8370c4f389f9dd15dbc1d719403854",
            "17bbf150b9819c56a8f155430bfbe222d06c43ef",
            "a3533b506166ac678727119c4748ec562a3b7f46",
            "ec8a0c31aa345707fcabe382e5bdef5c91fddf6b"
        ],
        "related_topics": [],
        "reference_count": "122",
        "citation_count": "12"
    },
    {
        "Id": "732ab2c6404f8322d0067f39503bdfd7f3577825",
        "title": "Expert System of Dengue Disease Using Artificial Neural Network Classifier",
        "authors": [
            "Hamdani Hamdani",
            "Zainal Arifin",
            "Anindita Septiarini"
        ],
        "date": "27 May 2022",
        "abstract": "This study proposes implementing an Artificial Neural Network with the Backpropagation (BPNN) algorithm as the classifier to categorize dengue types, divided into three categories: DF, DHF, and DSS. Abstract \u2013 Expert systems can be applied to the classification of dengue fever. Dengue is a serious disease that can be fatal if not diagnosed and treated properly. Headache, muscle aches, fever, and rash are some of the most prevalent symptoms. Dengue fever is a disease that is endemic in various South Asian and Southeast Asian nations. Dengue fever (DF), dengue hemorrhagic fever (DHF), and dengue shock syndrome are the three types of dengue (DSS). Currently, these diseases may be classified using a machine learning approach with dengue symptoms as the input data. This study proposes implementing an Artificial Neural Network (ANN) with the Backpropagation (BPNN) algorithm as the classifier to categorize dengue types, divided into three categories: DF, DHF, and DSS. The dengue symptoms are represented by 21 attributes in the dataset. It was gathered from 110 patients. Cross-validation with k-fold 3, 5, and 10 were applied as the evaluation method. Three parameters were obtained to evaluate the ANN classification method: precision, recall, and accuracy. These were used to justify the most optimal performance. Cross-validation using k-fold 3 produced the best evaluation results, with precision, recall, and accuracy values of 97.3%, 97.3%, and 97.27%, respectively.",
        "references": [
            "765f6b71ac944422b27283711cbd10418e2623cf",
            "1fd4280ef4e01b1293724ec39e28605047219a5d",
            "65744a1ae4c921b801cf7c4280fd4161491fde45",
            "ac7200123294565ecfc4bd4983b03007de5df0a8",
            "3e1f5d06ef26fce053a19f5c38a13f12610d3a59",
            "25072fb2dd4ee80eda6b857e341bb3adc24c49ce",
            "04e813c862697d6b075f003f64b47d6bd56432ba",
            "6ab4d16036e2b3b0dd35c9168e712efda65507a1",
            "cb6e98bd8ed27aa9f31c1734382bc7d69c54f48c",
            "7e9e795b8047f2a58ff3c312d4ae7aba87dafaae"
        ],
        "related_topics": [],
        "reference_count": "23",
        "citation_count": "One"
    },
    {
        "Id": "2c7ec1dc4d474ee8c36e0f8e092e4da6917ce2d2",
        "title": "IAES International Journal of Artificial Intelligence (IJ-AI)",
        "authors": [
            "Hamdani Hamdani",
            "Heliza Rahmania Hatta",
            "Novianti Puspitasari",
            "Anindita Septiarini",
            "Henderi"
        ],
        "date": "2022",
        "abstract": "This study aims to classify dengue types consisting of three classes: DF, DHF, and DSS using five classification methods including C.45, decision tree (DT), k-nearest neighbor (KNN), random forest (RF), and support vector machine (SVM). Received Oct 16, 2021 Revised May 20, 2022 Accepted Jun 18, 2022 Dengue is a dangerous disease that can lead to death if the diagnosis and treatment are inappropriate. The common symptoms that occur, including headache, muscle aches, fever, and rash. Dengue is a disease that causes endemics in several countries in South Asia and Southeast Asia. There are three varieties of dengue, such as dengue fever (DF), dengue hemorrhagic fever (DHF), and dengue shock syndrome (DSS). This disease can currently be classified using a machine learning approach with the input data being the dengue symptoms. This study aims to classify dengue types consisting of three classes: DF, DHF, and DSS using five classification methods including C.45, decision tree (DT), k-nearest neighbor (KNN), random forest (RF), and support vector machine (SVM). The dataset used consists of 21 attributes, which are the dengue symptoms. It was collected from 110 patients. The evaluation method was conducted using cross-validation with k-folds of 3, 5, and 10. The dengue classification method was evaluated using three parameters: precision, recall, and accuracy, which were most optimally achieved. The most optimal evaluation results were obtained using SVM with k-fold 3 and 10 with precision, recall, and accuracy values reaching 99.1%, 99.1%, and 99.1%, respectively.",
        "references": [
            "765f6b71ac944422b27283711cbd10418e2623cf",
            "80fc56882a549a453659613a066089da723f26c3",
            "9790d1bbee901ab80125b94d29e47fb507067b3e",
            "c83f07063ae93d0cd6b604f81e099b0b759e809f",
            "65744a1ae4c921b801cf7c4280fd4161491fde45",
            "4a18be86816222465d51ed3e903369df754f4005",
            "25072fb2dd4ee80eda6b857e341bb3adc24c49ce",
            "3e1f5d06ef26fce053a19f5c38a13f12610d3a59",
            "a48d3af9720b2c416a1cc52b20557a6568b072c7",
            "15609743f54d1d1539e241bab0fd79a12fef9946"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "29"
    },
    {
        "Id": "d225a689a37a20ef5e9cc6cfa405886ca3b5c622",
        "title": "Indonesian Journal of Electrical Engineering and Computer Science",
        "authors": [
            "Md Ferdouse Hossain Bhuiya",
            "Rohaiza Hamdan",
            "Dur Mohammad Soomro",
            "Abdelrehman Omer Idris",
            "Hussain Sharif"
        ],
        "date": "2021",
        "abstract": "A state-estimation based effective compensation strategy for enhancing performance of employed APF scheme is presented and a novel control technique for PQ enhancement is proposed where Kalman filtering approach has been applied for generating the reference signals. Received Sep 25, 2021 Revised Jun 21, 2022 Accepted Jul 7, 2022 With the photovoltaic (PV) system becoming highly popular in distribution network, there is also a growing concern regarding its adverse impacts over the power quality (PQ) of integrated system. This deterioration in PQ has sparked a new interest in the filtering techniques used in the power system for mitigating these problems. In contrast to conventional passive filters now active power filters (APFs) are being looked as the predominant solution. This paper presents a state-estimation based effective compensation strategy for enhancing performance of employed APF scheme. A novel control technique for PQ enhancement is proposed where Kalman filtering approach has been applied for generating the reference signals. Also, an adaptive hysteresis band technique has been applied here in the switching strategy. Thus an enhanced filter performance is achieved with reduced complexity and better elimination of distortions. Simulated results obtained in MATLAB/Simulink platform have been analyzed completely, where the superiority of filtering algorithm has been also demonstrated through least total harmonic distortion (THD) achieved in source current among all the existing Kalman filters.",
        "references": [
            "f007be1484cd3a47c1c855113b430836f8e8b5bb",
            "c8368438fd6654e8d3ac93b46b0013f505899b09",
            "609c5388924e477a2edf9519520d9bfc53f7bf95",
            "3c4dc7183a9f05e80a729c08871c65bd6ed3bd76",
            "b5d64e9e913b6357ffc39c3514dd793cfcfe10d5",
            "1b3d72c739c9721755ed6c89e86e11bded9c5a40",
            "e7bca3f3c05ebcda335ee05526d32b00e80dc3e6",
            "b4631710f66cd963249aaed7d165aad42ba5d511",
            "1b640d8ea2be89e8c43528c98ca2e063efcac4b5",
            "1d31fb092f9b21d3d8398a87e3a704738ae97e01"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "1,128"
    },
    {
        "Id": "d48c19bd3d4ed5a0a5e0318f225fb16d9525d9b3",
        "title": "IAES International Journal of Artificial Intelligence (IJ-AI)",
        "authors": [
            "Nur Azieta Mohamad Aseri",
            "Mohd Arfian Ismail",
            "Abdul Sahli Fakharudin",
            "Ashraf Osman",
            "Ibrahim",
            "Shahreen Kasim",
            "Noor Hidayah Zakaria",
            "Tole Sutikno"
        ],
        "date": "2021",
        "abstract": "This thesis aims to provide a history of computing and information technology in the United Arab Emirates from 1989 to 2000, a period chosen in order to explore its roots as well as specific applications in medicine and health sciences. Nur Farahaina Idris, Mohd Arfian Ismail, Mohd Saberi Mohamad, Shahreen Kasim, Zalmiyah Zakaria, Tole Sutikno Faculty of Computing, College of Computing and Applied Sciences, Universiti Malaysia Pahang, Malaysia Health Data Science Lab, Department of Genetics and Genomics, College of Medicine and Health Sciences, United Arab Emirates University, Abu Dhabi, United Arab Emirates Faculty of Computing Science and Information Technology, Universiti Tun Hussein Onn, Johor, Malaysia School of Computing, Faculty of Engineering, Universiti Teknologi Malaysia, Johor, Malaysia Department of Electrical Engineering, Faculty of Industrial Technology, Universitas Ahmad Dahlan, Yogyakarta, Indonesia",
        "references": [
            "04ae41326648f7637a506830ad759ef38ecd84a5",
            "84d5ff5c7e1b870649eefc2f7add4430ed09adc5",
            "f0882426143877b9f752fbe814772cbc62d83647",
            "337e2e9e9bd91954c85937ba63254d7810bc28d8",
            "62b64c73af00c29b36c1ae8ccbe9ca4a5ff39e34",
            "27a8b96229af65cdcb02afbc8dbfdb984162a58d",
            "f8196876bdf8f6a768ec55f8fb1d6c1804b6a19b",
            "0604ec3d77933cd0012467ddcb2b5d37912f758e",
            "f15d0f5066539ac131b1bf4d025248871a3a07ac",
            "2f101eb72eb1e9f168c5b79d4ce98a54d3713c50"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "300"
    },
    {
        "Id": "cb80a697996f2693e432a8ad3185dd72774bfbea",
        "title": "A Review on CLAHE Based Enhancement Techniques",
        "authors": [
            "Richa Sharma",
            "Amit Kamra"
        ],
        "date": "14 September 2023",
        "abstract": "Implementation of CLAHE based techniques in medical images is reviewed extensively in this paper along with its usage in other fields and its usage in other fields is also highlighted. Machine intelligence concepts have revolutionized the way to solve/address the multifaceted problems in many domains such as network security, 3-D modelling, etc. One domain which profoundly benefits from these algorithms is Medical Science. To solve the noise and contrast issues in medical images, we have reviewed different implementation and variants on CLAHE based enhancement techniques. Implementation of CLAHE based techniques in medical images is reviewed extensively in this paper along with its usage in other fields is also highlighted. Different algorithms are measured on performance parameters like Entropy, SNR, PSNR etc.",
        "references": [
            "793e5c1bd2fcec7d05375c9a92a6a917e32ca6b5",
            "e17467064acd4187cb387c48c4c9449811c6ea63",
            "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "53fceb33baa404a61d79f47bb184cb6aaaae534f",
            "805663cc851325bef18e1c113eabee69df241157",
            "d8802374614be9d93758d64e26415aea0d38a957",
            "e32a6d4bd0f266615e2e359cc3899de4b427d5ee",
            "6fb196100999e4ae362b7bd1d9931126695b8ed1",
            "fa13b10ef664763f1a5c0ea0bdb8603d30b17be4",
            "a3ff45784663b9f7b3f6149b65b5ad071a63fc74"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "29"
    },
    {
        "Id": "32320a5c9a49868b851ef6727cc3d0bd80f9f011",
        "title": "Segmentation of malignant tumours in mammogram images: A hybrid approach using convolutional neural networks and connected component analysis",
        "authors": [
            "Abhijit Roy",
            "Bikesh Kumar Singh",
            "Sumit Kumar Banchhor",
            "Kesari Verma"
        ],
        "date": "24 September 2021",
        "abstract": "A novel hybrid approach is proposed by combining a convolution neural network (CNN) with connected component analysis (CCA) to segment malignant breast lesions without any pre\u2010processing to avoid any distortion in image sharpness at the initial stages. The segmentation of breast lesions is an important step in the computer\u2010aided analysis of the mammogram. The presence of noise in mammograms makes lesion detection challenging particularly for complex malignant lesions. Pre\u2010processing techniques can deal with the noise issue but distorts the important shape features. This motivates us to propose a novel hybrid approach by combining a convolution neural network (CNN) with connected component analysis (CCA) to segment malignant breast lesions without any pre\u2010processing to avoid any distortion in image sharpness at the initial stages. Two well\u2010known segmentation techniques namely, K\u2010means (KM) and Fuzzy c\u2010means (FCM) are also used to compare the results. From a pool of 1045 mammographic cancer images acquired from the Digital Database for Screening Mammography (DDSM), 1016 are used for training and validation, and 29 are used for testing. All three results (Hybrid, KM and FCM) are compared against the results by the expert Radiologist. The results indicate that, among various segmentation techniques, the proposed hybrid approach achieves the highest accuracy (90%), Matthew's correlation coefficient (0.79), Jaccard index (0.73) and the Dice similarity coefficient (0.84). Other performance evaluation techniques such as; precision, sensitivity, specificity, false\u2010positive rate, false discovery rate, negative predictive value and false\u2010negative rate also show the superior performance of the proposed hybrid approach. Statistical analysis (Mann\u2013Whitney U test, T\u2010test, Chi\u2010square test, Kolmogorov\u2013Smirnov test and Wilcoxon test), graphical analysis (Regression and Bland\u2013Altman plots) and receiver operating characteristic curve further demonstrate the stability and consistency of the results.",
        "references": [
            "0e0de80f4ff66cd3aef5ff05fd30406fb85da0ea",
            "a4360f362168a6107e1014b7fc61bed32038fc70",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "1220fbc61ec47d5978d42c8b5a48b7a33b3c8374",
            "7b3a3e1bd771b7f1ecf0ad0be49f0aed1a358049",
            "0650a2e55a41e154bb43f75ac2cf68d99fefcdda",
            "21b694ae64f12b5a372e9a48ce0896772037574e",
            "fc2ee21b2c625b2f480dcc6bc54bbc60c9a48d54",
            "f08551744f15848eecd645717c0071cd3839db13",
            "46ad856e44cd3b5314ebb785789a9d1b898c137c"
        ],
        "related_topics": [
            "Breast Lesions",
            "Mammograms",
            "Lesion Detection",
            "Digital Database For Screening Mammography",
            "Radiologist",
            "Convolutional Neural Network",
            "Regression"
        ],
        "reference_count": "63",
        "citation_count": "3"
    },
    {
        "Id": "5ad977a94976b8b5d5572639e17b5d0fb73cb589",
        "title": "Automated diagnosis of breast cancer using parameter optimized kernel extreme learning machine",
        "authors": [
            "Figlu Mohanty",
            "Suvendu Rup",
            "Bodhisattva Dash"
        ],
        "date": "1 September 2020",
        "abstract": "Semantic Scholar extracted view of \"Automated diagnosis of breast cancer using parameter optimized kernel extreme learning machine\" by Figlu Mohanty et al.",
        "references": [
            "82cc7ee3831c3a6b8a75942bd791b0454ba3d503",
            "adba7174e88772a558e168baa7e2764b722d0dc5",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "d47177c238f633b9092d719438e372eb75acbfa7",
            "4773f0d5d0ba10301f9d71a95ff7ce1f459b4d0f",
            "e309ddf4eb133f3df4709ac5d5c6709179349fde",
            "bb070c70c3bae998e950c72aa0a20e3eb27e228e",
            "c3abf167a1e99113823647c651520f55bfab7b98",
            "8adfe3dd0cbc6eaa1688dfbc8d69301b70ff7193",
            "e5a1ab9e4d11c55dc66f9e78f48232ce19139008"
        ],
        "related_topics": [
            "Cylindrical Algebraic Decomposition",
            "Mammographic Image Analysis Society",
            "Digital Database For Screening Mammography",
            "Multi-class Classification",
            "Malignant",
            "Classification Accuracy",
            "Kernel Principle Component Analysis",
            "Extreme Learning Machine"
        ],
        "reference_count": "35",
        "citation_count": "26"
    },
    {
        "Id": "f24b31da5bb6d71869dff1d210eaf761265ba9dd",
        "title": "Breast cancer: Three\u2010class masses classification in mammograms using Apriori dynamic selection",
        "authors": [
            "Khaoula Belhaj Soulami",
            "Naima Kaabouch",
            "Mohamed Nabil Saidi"
        ],
        "date": "22 July 2022",
        "abstract": "A machine learning\u2010based method is proposed for the classification of breast masses using the shape and texture features extracted from the suspicious mammogram patches and the Apriori dynamic selection method is applied for the final test predictions using the appropriate classifier for each test sample. Mammography is a commonly used screening technique for early diagnosis of breast cancer. However, the early detection of abnormalities remains challenging, particularly for dense breast categories. In this context, the automated classification of breast masses assists radiologists in their diagnosis and give them a second opinion. In this paper, we propose a machine learning\u2010based method for the classification of breast masses. First, the shape and texture features are extracted from the suspicious mammogram patches. These features are then fed to the Principal Component Analysis (PCA) to keep the relevant features only and are classified using the Support Vector Machine (SVM) and Random Forest (RF). Lastly, the Apriori dynamic selection method is applied for the final test predictions using the appropriate classifier for each test sample. The classification of breast masses patches into normal and abnormal attains accuracy of 96.43%, F1\u2010score of 95.76%, precision of 96.29%, recall of 95.27%, specificity of 96.43%, and AUC of 0.963. Whereas the one\u2010stage multi\u2010classification of breast masses into normal, benign, and malignant achieves accuracy of 75.81%, F1\u2010score of 76.47%, precision of 76.85%, recall of 78.77%, specificity of 87.91%, and AUC of 0.829.",
        "references": [
            "5384355c1dcb8b6325b351e2317cf16eb8a2aba6",
            "13fbed3445f7b3a66e649314037558bcde14b11e",
            "7c66be06a11210b10afe5dddde51f7c355b98b14",
            "be99a592c2f5e89ef12732d11e04461e435719d0",
            "34e8d5e7e19693f804a25cd5a869beb765a68c7f",
            "ba4a787088f7e1fc278e45bc1663d402c5922d4e",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "bb070c70c3bae998e950c72aa0a20e3eb27e228e",
            "662d927da3c17bfde61da5dbc24c037dce30ce25",
            "f975a0b5adc95b9c2e0bd99ab6c866a02fe4727b"
        ],
        "related_topics": [
            "Area Under The ROC Curve",
            "Malignant",
            "Radiologist",
            "Random Forests",
            "Classifier",
            "Texture Feature",
            "Mammograms",
            "Machine Learning"
        ],
        "reference_count": "0",
        "citation_count": "46"
    },
    {
        "Id": "a0bb41d54673d2e9c514620bbc8967f8d0d867be",
        "title": "Identification of Breast Malignancy by Marker-Controlled Watershed Transformation and Hybrid Feature Set for Healthcare",
        "authors": [
            "Tariq Sadad",
            "Ayyaz Hussain",
            "Asim Munir",
            "Muhammad Habib",
            "Sajid Ali Khan",
            "Shariq Hussain",
            "Shunkun Yang",
            "Mohammed Alawairdhi"
        ],
        "date": "11 March 2020",
        "abstract": "A hybrid feature set is developed after the extraction of shape-based and texture features from the breast lesion to detect breast malignancy through computer-aided diagnosis (CADx). Breast cancer is a highly prevalent disease in females that may lead to mortality in severe cases. The mortality can be subsided if breast cancer is diagnosed at an early stage. The focus of this study is to detect breast malignancy through computer-aided diagnosis (CADx). In the first phase of this work, Hilbert transform is employed to reconstruct B-mode images from the raw data followed by the marker-controlled watershed transformation to segment the lesion. The methods based only on texture analysis are quite sensitive to speckle noise and other artifacts. Therefore, a hybrid feature set is developed after the extraction of shape-based and texture features from the breast lesion. Decision tree, k-nearest neighbor (KNN), and ensemble decision tree model via random under-sampling with Boost (RUSBoost) are utilized to segregate the cancerous lesions from the benign ones. The proposed technique is tested on OASBUD (Open Access Series of Breast Ultrasonic Data) and breast ultrasound (BUS) images collected at Baheya Hospital Egypt (BHE). The OASBUD dataset contains raw ultrasound data obtained from 100 patients containing 52 malignant and 48 benign lesions. The dataset collected at BHE contains 210 malignant and 437 benign images. The proposed system achieved promising accuracy of 97% with confidence interval (CI) of 91.48% to 99.38% for OASBUD and 96.6% accuracy with CI of 94.90% to 97.86% for the BHE dataset using ensemble method.",
        "references": [
            "e90fe4688d07f58810aea31124035c629a46fae0",
            "9499e0322d1e50dd65a5d8511ce253531685f0b2",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "951fbb632fd02fd57fb1d864bbd183ebb93172e0",
            "ae1db27518b5edfe36c65b23f31aef9584bf74e8",
            "d708ce13a35c29139e798d1779bf3629219b754b",
            "19ece439cbee920a7514b26487beb398bacc95e3",
            "1d0eb3fe75f2f343b753212e26351706b1a95c6f",
            "6842744e5309605489d2b5fa4bda0ce53cb72794",
            "28328289926fb23f9b687d2366141ea195691e63"
        ],
        "related_topics": [
            "Malignant",
            "Breast Ultrasound",
            "Random Under-sampling",
            "Texture Analysis",
            "K-nearest Neighbors",
            "Decision Trees",
            "Ensemble Methods"
        ],
        "reference_count": "48",
        "citation_count": "36"
    },
    {
        "Id": "6931ca52d7504ae72ea3ddc0707e8231529665e9",
        "title": "Detection of breast cancer using the infinite feature selection with genetic algorithm and deep neural network",
        "authors": [
            "S. S. Ittannavar",
            "R. H. Havaldar"
        ],
        "date": "7 August 2021",
        "abstract": "A new model is proposed in this research using mammogram images which is an effective technique used for screening and detecting the breast cancer and showed 0.10\u20137% improvement in accuracy related to the existing models such as an extreme learning machine, conditional generative adversarial network with convolutional neural network, etc. The breast cancer is a major health issue worldwide, so the early detection of abnormalities decreases the mortality rate. For the early detection of breast cancer, a new model is proposed in this research using mammogram images which is an effective technique used for screening and detecting the breast cancer. At first, the images are acquired from the digital database for screening mammography and mammographic image analysis society datasets. Then, the visual quality of the images is improved by using normalization, contrast limited adaptive histogram equalization and median filter. Further, multi-level multi objective electro magnetism like optimization algorithm is proposed to segment the non-cancer and cancer regions from the enhanced images. Additionally, feature vectors are extracted from the segmented regions using local directional ternary pattern, histogram of oriented gradients and Haralick texture features. Next, infinite feature selection with genetic algorithm is applied to select the active or relevant features for breast cancer classification. The genetic algorithm is applied with entropy value to measure the homogeneity to find the mutual information between the extracted features that helps in provide prominent feature values. The genetic algorithm has the advantage of probabilistic transition rule that helps to analysis the associate neighbour features. The selected feature vectors are fed to deep neural network to classify the mammogram images as malignant and benign classes. From the simulation result, the proposed infinite feature selection with genetic algorithm and deep neural network showed 0. 10\u20137% improvement in accuracy related to the existing models such as an extreme learning machine, conditional generative adversarial network with convolutional neural network, etc.",
        "references": [
            "34599ec5ffe911a4350f6b89fcb6b11c0f9c2b79",
            "7c66be06a11210b10afe5dddde51f7c355b98b14",
            "68e98184ac29cb50df57b4cce8462debea8722ad",
            "09a81f37797ee12713b5ddec259bf5be1a1cc226",
            "b9e840bab3facd93db9bdf9f3872b987fe7b0e11",
            "635b1caa8371fdb9bc09d610fbe5ff2eefa3383b",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "526ca602fb79bf5df20741567114fe55c2a52548",
            "2c5db1c44dddf0d8e6ea667f3f7afb13032ea386",
            "9960c18b3154090644c536905224ae90e521df64"
        ],
        "related_topics": [
            "Infinite Feature Selection",
            "Deep Neural Networks",
            "Feature Vector",
            "Mammographic Image Analysis",
            "Malignant",
            "Contrast Limited Adaptive Histogram Equalization",
            "Normalization",
            "Mutual Information",
            "Convolutional Neural Network",
            "Extreme Learning Machine"
        ],
        "reference_count": "41",
        "citation_count": "2"
    },
    {
        "Id": "4ed705bc9b8fdaf6a2cd4e0d6e35e9ef15569de5",
        "title": "TTCNN: A Breast Cancer Detection and Classification towards Computer-Aided Diagnosis Using Digital Mammography in Early Stages",
        "authors": [
            "Sarmad Maqsood",
            "Robertas Dama{\\vs}evi{\\vc}ius",
            "Rytis Maskeli\u016bnas"
        ],
        "date": "23 March 2022",
        "abstract": "The proposed transferable texture CNN-based method for classifying screening mammograms has outperformed prior methods and demonstrates that automatic deep learning algorithms can be easily trained to achieve high accuracy in diverse mammography images, and can offer great potential to improve clinical tools to minimize false positive and false negative screening mammography results. Breast cancer is a major research area in the medical image analysis field; it is a dangerous disease and a major cause of death among women. Early and accurate diagnosis of breast cancer based on digital mammograms can enhance disease detection accuracy. Medical imagery must be detected, segmented, and classified for computer-aided diagnosis (CAD) systems to help the radiologists for accurate diagnosis of breast lesions. Therefore, an accurate breast cancer detection and classification approach is proposed for screening of mammograms. In this paper, we present a deep learning system that can identify breast cancer in mammogram screening images using an \u201cend-to-end\u201d training strategy that efficiently uses mammography images for computer-aided breast cancer recognition in the early stages. First, the proposed approach implements the modified contrast enhancement method in order to refine the detail of edges from the source mammogram images. Next, the transferable texture convolutional neural network (TTCNN) is presented to enhance the performance of classification and the energy layer is integrated in this work to extract the texture features from the convolutional layer. The proposed approach consists of only three layers of convolution and one energy layer, rather than the pooling layer. In the third stage, we analyzed the performance of TTCNN based on deep features of convolutional neural network models (InceptionResNet-V2, Inception-V3, VGG-16, VGG-19, GoogLeNet, ResNet-18, ResNet-50, and ResNet-101). The deep features are extracted by determining the best layers which enhance the classification accuracy. In the fourth stage, by using the convolutional sparse image decomposition approach, all the extracted feature vectors are fused and, finally, the best features are selected by using the entropy controlled firefly method. The proposed approach employed on DDSM, INbreast, and MIAS datasets and attained the average accuracy of 97.49%. Our proposed transferable texture CNN-based method for classifying screening mammograms has outperformed prior methods. These findings demonstrate that automatic deep learning algorithms can be easily trained to achieve high accuracy in diverse mammography images, and can offer great potential to improve clinical tools to minimize false positive and false negative screening mammography results.",
        "references": [
            "f36b3ee8d83ec14808a268183e28f05f4b7b3bc3",
            "4130fbcf26a46cfdbddae77a11b3f781aa25124d",
            "defb443060f269e0f0eb3630afe42e7928c6ee1d",
            "3f02ff7afca151f2abfe14a863ef0d4b5a8b1393",
            "34a91f535f841db560062e7266ac87069f282615",
            "0f3ca7895bebbad7a0f4b7ddcc3febeeff9811ef",
            "7c8c21129cccf09508de82dd9ea785163f3e7834",
            "f8408e114795d5195679c5dbfc46e8a587bcc2d4",
            "4cfea5ab26c5824d616fa8e2562c0666ecee55aa",
            "257438fca36f8e1bcf1ca7aedc70364603496818"
        ],
        "related_topics": [],
        "reference_count": "75",
        "citation_count": "54"
    },
    {
        "Id": "bf9fab73a955c7bea893092b6e963da0052ddf83",
        "title": "Hybrid computer aided diagnostic system designs for screen film mammograms using DL\u2010based feature extraction and ML\u2010based classifiers",
        "authors": [
            "Jyoti Rani",
            "Jaswinder Singh",
            "Jitendra Virmani"
        ],
        "date": "3 May 2023",
        "abstract": "Mammography is most popular imaging method used often in predicting breast cancer within women above age of 38\u2009years. Various computer\u2010assisted algorithms have been employed for classifying breast masses as normal or malignant using screen film mammographic (SFM) images. In present research work, exhaustive experimentations have been carried out with nine deep learning\u2010based convolutional neural networks (CNNs) belonging to the three different categories of CNN architectures including (a) simple convolution\u2010based series models, that is VGG16 and VGG19 (b) simple convolution\u2010based directed acyclic graph (DAG) model, that is GoogleNet and (c) dilated convolution\u2010based DAG models, that is ResNet18, ResNet50, Inceptionv3, XceptionNet, ShuffleNet and MobileNet\u2010V2, for binary classification of the mammographic masses with SFM images. The experimental work has been carried out using 518 mammographic images taken from DDSM dataset with 208 images \u03f5 benign class and 310 images \u03f5 malignant class, respectively. The encoder\u2010decoder based semantic segmentation network model, that is ResNet50 has been used for the segmentation of mammographic masses from SFM images. The segmented masses images obtained from the ResNet50 model are subjected to classification experiments. To design a robust hybrid classifier design, that is, deep learning (DL)\u2010based feature extraction and machine learning (ML)\u2010based classification, the first step is to obtain an optimal DL\u2010based feature extractor for classification task. The optimal feature set obtained by the best performing CNN Model, that is, VGG 19 has been subjected to correlation\u2010based feature extraction and ML\u2010based classifiers including (i) adaptive neuro fuzzy classifier\u2010linguistic hedges (ii) principal component analysis\u2010 support vector machine classifier and (iii) GA\u2010SVM classifier to yield an optimal hybrid computer aided diagnostic (CAD) system design. It is found that hybrid CAD system using VGG19 as feature extractor and ANFC\u2010LH as classifier yields 96% the highest classification accuracy. The other performance parameters yield values, that is 96% sensitivity, 96% specificity, 96% F\u2010score, 96% precision and 92% MCC, which indicates a best prediction of binary classification. The test images which were misclassified by these hybrid CAD system designs were analysed subjectively by experienced participating radiologist. The results obtained by present work suggests that proposed hybrid CAD system with VGG19 Network model acting as feature extractor and ANFC\u2010LH classifier can be employed for the differential diagnosis of benign as well as malignant mammographic masses using SFM images in a routine clinical setting.",
        "references": [
            "8b8d30acb191667504462578615dda741f15cc75",
            "6c26a46ce3a3793ae47ff48b9725c72dfa8cc06a",
            "715d7b34d2b3f58a257e43aa9c1a5122a3718915",
            "257438fca36f8e1bcf1ca7aedc70364603496818",
            "f0741c59f750fcfbe1288654c328bc06408d2b11",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741",
            "fc3e53a00559dc17a6a35fd8c2e9e9df94fb9232",
            "cc91294404953dc555edd2de2156c88754654a0d",
            "d0f0f7fdb13fcbe77dcc28c6662a0981ef02e80e",
            "14a0fe917e65cd08695e7438eec254a3e7538225"
        ],
        "related_topics": [
            "Screen Film Mammography",
            "Social Force Model",
            "ResNet-50",
            "Deep Learning",
            "Binary Classification",
            "VGG16",
            "Malignant",
            "GoogLeNet",
            "Encoder-decoders",
            "Classification"
        ],
        "reference_count": "105",
        "citation_count": "One"
    },
    {
        "Id": "1c91ca81e779df5a5180e722f0fc0acd184a1ffc",
        "title": "Automatic Detection of Malignant Masses in Digital Mammograms Based on a MCET-HHO Approach",
        "authors": [
            "Erick Rodr{\\&#x27;i}\u00adguez-Esparza",
            "Laura A. Zanella-Calzada",
            "Daniel Zald{\\&#x27;i}var",
            "Carlos Eric Galv{\\&#x27;a}n-Tejada"
        ],
        "date": "28 March 2020",
        "abstract": "In this work, it is proposed the implementation of a multilevel threshold segmentation technique applied to mammography images, based on the Harris Hawks Optimization (HHO) algorithm, in order to identify regions of interest (ROIs) that contain malignant masses. Digital image processing techniques have become an important process within medical images. These techniques allow the improvement of the images in order to facilitate their interpretation for specialists. Within these are the segmentation methods, which help to divide the images by regions based on different approaches, in order to identify details that may be complex to distinguish initially. In this work, it is proposed the implementation of a multilevel threshold segmentation technique applied to mammography images, based on the Harris Hawks Optimization (HHO) algorithm, in order to identify regions of interest (ROIs) that contain malignant masses. The method of minimum cross entropy thresholding (MCET) is used to select the optimal threshold values for the segmentation. For the development of this work, four mammography images were used (all with presence of a malignant tumor), in their two views, craniocaudal (CC) and mediolateral oblique (MLO), obtained from the Digital Database for Screening Mammography (DDSM). Finally, the ROIs calculated were compared with the original ROIs of the database through a series of metrics, to evaluate the behavior of the algorithm. According to the results obtained, where it is shown that the agreement between the original ROIs and the calculated ROIs is significantly high, it is possible to conclude that the proposal of the MCET-HHO algorithm allows the automatic identification of ROIs containing malignant tumors in mammography images with significant accuracy.",
        "references": [
            "922f40dcad618dccffb2d42bfbcd58febcd0ad91",
            "4bddd3d031bf524202df3f83eb76ef5121f6c9a4",
            "5f0f6207844c6ef3df53dcfa21591d6359fbffe0",
            "5467b8e460ba7a44f3cd92c609899bd4f64c0035",
            "301fee6989cc75f84343836cab5a25691d58dc5c",
            "87062858d1b1d59a563be3ea53cd06685eba79b7",
            "fd7f3234a847f0ce8be891af5b2cd098764356a7",
            "93300ce12a55b616dfd539ffb934ccdb0d061059",
            "ba285c33c4e7f5634291f3c795606b8bec197692",
            "a260a11aa3de8092796b31f6d028be63b4c2fdf5"
        ],
        "related_topics": [
            "Region Of Interest",
            "Minimum Cross Entropy Thresholding",
            "Digital Image Processing",
            "Mediolateral Oblique",
            "Digital Database For Screening Mammography",
            "Craniocaudal",
            "Medical Images",
            "Harris Hawks Optimizer"
        ],
        "reference_count": "46",
        "citation_count": "4"
    },
    {
        "Id": "9ee9eeed8fc56f600eb3ff07f6e38dff81957ff8",
        "title": "Systematic Review of Computing Approaches for Breast Cancer Detection Based Computer Aided Diagnosis Using Mammogram Images",
        "authors": [
            "Dilovan Asaad Zebari",
            "Dheyaa Ahmed Ibrahim",
            "Diyar Qader Zeebaree",
            "Habibollah Haron",
            "Merdin Shamal Salih",
            "Robertas Dama{\\vs}evi{\\vc}ius",
            "Mazin Abed Mohammed"
        ],
        "date": "2 December 2021",
        "abstract": "A systematic review of existing CAD systems that use machine learning methods as well as their current state based on mammogram image modalities and classification methods to identify research gaps and outline recommendations for future research. ABSTRACT Breast cancer is one of the most prevalent types of cancer that plagues females. Mortality from breast cancer could be reduced by diagnosing and identifying it at an early stage. To detect breast cancer, various imaging modalities can be used, such as mammography. Computer-Aided Detection/Diagnosis (CAD) systems can assist an expert radiologist to diagnose breast cancer at an early stage. This paper introduces the findings of a systematic review that seeks to examine the state-of-the-art CAD systems for breast cancer detection. This review is based on 118 publications published in 2018\u20132021 and retrieved from major scientific publication databases while using a rigorous methodology of a systematic review. We provide a general description and analysis of existing CAD systems that use machine learning methods as well as their current state based on mammogram image modalities and classification methods. This systematic review presents all stages of CAD including pre-processing, segmentation, feature extraction, feature selection, and classification. We identify research gaps and outline recommendations for future research. This systematic review may be helpful for both clinicians, who use CAD systems for early diagnosis of breast cancer, as well as for researchers to find knowledge gaps and create more contributions for breast cancer diagnostics.",
        "references": [
            "c6b2b7704f937ae5f967c0efc80771bb2a1b4127",
            "5a6f4d1742997c512f35155fbf8dd94331db5496",
            "44eb9aafb0d32bf9c8dfd2a4f0f4a418be779a08",
            "a2e059919db1dae0969c25c0c9de4847b15fd083",
            "8a7409d4b756f4f9bab9c52eba54bca66e3bbbe3",
            "8967ea71b194a28744c15a3be93fa6e32ddac1e9",
            "3ba8e598181ac75984f27e235c928a4d32cb3bd3",
            "b2b6030734286daf3ba7e6d921717fb763c900ed",
            "8621d05c23e1a4d7bc64caf663538c0f502079ab",
            "1c2555d1e13580bd809f058e12ac7dfa8490946a"
        ],
        "related_topics": [
            "Systematic Review",
            "Cylindrical Algebraic Decomposition",
            "Radiologist",
            "Recommendation",
            "Feature Selection",
            "Classification"
        ],
        "reference_count": "167",
        "citation_count": "45"
    },
    {
        "Id": "06e3c3353bf177212f3d968b36ca3ac2e13b2cd9",
        "title": "Efficient Breast Cancer Diagnosis from Complex Mammographic Images Using Deep Convolutional Neural Network",
        "authors": [
            "Hameedur Rahman",
            "Tanvir Fatima Naik Bukht",
            "Rozilawati Ahmad",
            "Ahmad S. Almadhor",
            "Abdul Rehman Javed"
        ],
        "date": "2 March 2023",
        "abstract": "Results highlight that deep convolutional neural network algorithms can be trained to achieve highly accurate results in various mammograms, along with the capacity to enhance medical tools by reducing the error rate in screening mammograms. Medical image analysis places a significant focus on breast cancer, which poses a significant threat to women's health and contributes to many fatalities. An early and precise diagnosis of breast cancer through digital mammograms can significantly improve the accuracy of disease detection. Computer-aided diagnosis (CAD) systems must analyze the medical imagery and perform detection, segmentation, and classification processes to assist radiologists with accurately detecting breast lesions. However, early-stage mammography cancer detection is certainly difficult. The deep convolutional neural network has demonstrated exceptional results and is considered a highly effective tool in the field. This study proposes a computational framework for diagnosing breast cancer using a ResNet-50 convolutional neural network to classify mammogram images. To train and classify the INbreast dataset into benign or malignant categories, the framework utilizes transfer learning from the pretrained ResNet-50 CNN on ImageNet. The results revealed that the proposed framework achieved an outstanding classification accuracy of 93%, surpassing other models trained on the same dataset. This novel approach facilitates early diagnosis and classification of malignant and benign breast cancer, potentially saving lives and resources. These outcomes highlight that deep convolutional neural network algorithms can be trained to achieve highly accurate results in various mammograms, along with the capacity to enhance medical tools by reducing the error rate in screening mammograms.",
        "references": [
            "9e4c5c8ace304341f30a774b0c71a34987adc2a5",
            "939749fe9a71d4bc80ba7ea062278857bca1f76c",
            "1a4a4e54df80764a9f79e40363d1f10e5a1f2609",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741",
            "34a91f535f841db560062e7266ac87069f282615",
            "f8408e114795d5195679c5dbfc46e8a587bcc2d4",
            "3f02ff7afca151f2abfe14a863ef0d4b5a8b1393",
            "f36b3ee8d83ec14808a268183e28f05f4b7b3bc3",
            "b14e81042a3d41dfcac1046149fae4bb1f64947d",
            "2a863a1ab6df3ca9a55573befcb89e1ed7b7df74"
        ],
        "related_topics": [],
        "reference_count": "80",
        "citation_count": "11"
    },
    {
        "Id": "d29cfbc42f0e77f546ae3a437432c9234aa48aab",
        "title": "Improving Breast Cancer Detection and Diagnosis through Semantic Segmentation Using the Unet3+ Deep Learning Framework",
        "authors": [
            "Taukir Alam",
            "Wei-Chung Shia",
            "Fang Rong Hsu",
            "Taimoor Hassan"
        ],
        "date": "25 May 2023",
        "abstract": "An advanced semantic segmentation method and a deep convolutional neural network are used to identify the Breast Imaging Reporting and Data System (BI-RADS) lexicon for breast ultrasound images to provide a more accurate and objective diagnosis of breast cancer, leading to improved patient outcomes. We present an analysis and evaluation of breast cancer detection and diagnosis using segmentation models. We used an advanced semantic segmentation method and a deep convolutional neural network to identify the Breast Imaging Reporting and Data System (BI-RADS) lexicon for breast ultrasound images. To improve the segmentation results, we used six models to analyse 309 patients, including 151 benign and 158 malignant tumour images. We compared the Unet3+ architecture with several other models, such as FCN, Unet, SegNet, DeeplabV3+ and pspNet. The Unet3+ model is a state-of-the-art, semantic segmentation architecture that showed optimal performance with an average accuracy of 82.53% and an average intersection over union (IU) of 52.57%. The weighted IU was found to be 89.14% with a global accuracy of 90.99%. The application of these types of segmentation models to the detection and diagnosis of breast cancer provides remarkable results. Our proposed method has the potential to provide a more accurate and objective diagnosis of breast cancer, leading to improved patient outcomes.",
        "references": [
            "e941511c42e749153fdec4c738483915c0967906",
            "ba4a787088f7e1fc278e45bc1663d402c5922d4e",
            "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "6a12d6f8f7b7f29ffd8abbe442ec86fc7a96f4c8",
            "d9ce59102f97cca7826bd5ea4dc066ae9da57116",
            "084ad84604d1afb914a92986bbaf8db326433112",
            "198d308169e7b95aced6e6b65918a548be20235d",
            "00a4e3f469ff6e1e90d87e4c6aca7210266f6f84",
            "8133e66c9c03095ef605090e6a72b752dc774d92",
            "0c3b4750353a73c2d72dd636c20aa01d1ee9d05e"
        ],
        "related_topics": [],
        "reference_count": "38",
        "citation_count": "4"
    },
    {
        "Id": "c09abdfd52f2ca7c742eb76efe97dd1fdd6cb82e",
        "title": "Effective Tumor Diagnosis based on Shape and Size of Tumor",
        "authors": [
            "Meghavi Rana",
            "Megha Bhushan"
        ],
        "date": "6 July 2023",
        "abstract": "The aim of this study is to generate imposed images for future classification by fusing the tumor mask with image pixels that will improve various aspects of tumor segmentation such as varied size and shape. The key aspects of cancer diagnosis include accurate detection of tumor size and shape, as well as efficient classification. Given study provides a brief overview of the recent developments in tumor classification approaches, tumor shape and size detection techniques. In this study, a tumor mask was imposed on the original medical image(s) using a logical AND operator. It also highlights the value of automated tumor size evaluation, which is crucial for classification, prediction, and future treatment. Further, it explores how to quantify tumor size more precisely and consistently using computer-aided methods (machine learning, deep learning, and image analysis). The aim of this study is to generate imposed images for future classification by fusing the tumor mask with image pixels. Then, the imposed images were classified into three categories namely benign, malignant, and malignant metastasis for effective tumor diagnosis. It will improve various aspects of tumor segmentation such as varied size and shape.",
        "references": [
            "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "e3070fd72490cb057410cedc65db8330af4389b0",
            "210ba0e0351476c2a75030bb4c261a70e514954d",
            "1a9f4a6d207fddda007c05fd02ca3682de27f3e4",
            "802797c2650a8d277b368fc193e626d0e1b2bf0a",
            "3043d8b8e03c9f8c1f3879fae6191ed9eb100255",
            "2e3d1ed66a67b424329df6bf3cb2ff089c8e0758",
            "9293ea4ddb3460c71e12a545f23b36db8a78abf1",
            "8ed7e8ef653270ff0a0c2e1d2cb1a752329ae68e",
            "c70426071de2e4a2b155c4f43dba6a2dde2db84b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "30"
    },
    {
        "Id": "f713858353cd4cb1273d4f7773e40ef069f3e5e8",
        "title": "Attention U-Net: A Deep Learning Approach for Breast Cancer Segmentation",
        "authors": [
            "Krishna Mridha",
            "Tasnim Sarker",
            "Suborno Deb Bappon",
            "Shahriar Mahmud Sabuj"
        ],
        "date": "15 September 2023",
        "abstract": "It is indicated that the Attention U-Net is a potential technique for segmenting breast cancer pictures and can assist to increase the precision of diagnosis and treatment planning. One of the main causes of death for women globally is breast cancer. Early discovery can reduce the frequency of early deaths. Breast ultrasound images can help in diagnosis and treatment planning by helping to categories different types of breast cancer. To segment breast cancer pictures in this study, we employed an Attention U-Net. A deep learning model for picture segmentation is called the Attention U-Net. We used a dataset of 780 breast ultrasound pictures divided into three classes-normal, benign, and malignant-to train the Attention U-Net. The mean IoU, precision, recall, and F1 scores for the Attention U-Net were 0.840,0.815,0.848, and 0.831 respectively. These outcomes are comparable to those of other cutting-edge techniques for segmenting breast cancer. The findings of this study indicate that the Attention U-Net is a potential technique for segmenting breast cancer. The great accuracy that the Attention U-Net can attain can assist to increase the precision of diagnosis and treatment planning",
        "references": [
            "1f5bdb1b3499a9522a1ae606d3e4e7315470f2c1",
            "5269e5dac2c63932f9793b6ae36909c02bb6c65b",
            "d29cfbc42f0e77f546ae3a437432c9234aa48aab",
            "a40ca08bbabd1232934357cc9bea6c4c3caf50d3",
            "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "afc4900dc69e9af396a7ff1267947ae5d5729d66",
            "10bf5ebb9137f19e4b6fb2e77573bc78e3447cfa",
            "c8765d92fb5c99e70249905bd6658632cd9f8bd2",
            "7b6d49cf5e969287abf123e07fb04ae7b8910c26",
            "1e097c70bfbf5b22affb9840067c4dd91812d3a4"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "18"
    },
    {
        "Id": "77a7d887057ef632cfe566c7af23f1c204b43b09",
        "title": "Improved Breast Cancer Classification through Combining Transfer Learning and Attention Mechanism",
        "authors": [
            "Asadulla Ashurov",
            "Samia Allaoua Chelloug",
            "Alexey Tselykh",
            "Mohammed Saleh Ali Muthanna",
            "Ammar Muthanna",
            "Mehdhar S. A. M. Al-gaashani"
        ],
        "date": "1 September 2023",
        "abstract": "A novel approach to breast cancer histopathological image classification is introduced that leverages modified pre-trained CNN models and attention mechanisms to enhance model interpretability and robustness, emphasizing localized features and enabling accurate discrimination of complex cases. Breast cancer, a leading cause of female mortality worldwide, poses a significant health challenge. Recent advancements in deep learning techniques have revolutionized breast cancer pathology by enabling accurate image classification. Various imaging methods, such as mammography, CT, MRI, ultrasound, and biopsies, aid in breast cancer detection. Computer-assisted pathological image classification is of paramount importance for breast cancer diagnosis. This study introduces a novel approach to breast cancer histopathological image classification. It leverages modified pre-trained CNN models and attention mechanisms to enhance model interpretability and robustness, emphasizing localized features and enabling accurate discrimination of complex cases. Our method involves transfer learning with deep CNN models\u2014Xception, VGG16, ResNet50, MobileNet, and DenseNet121\u2014augmented with the convolutional block attention module (CBAM). The pre-trained models are finetuned, and the two CBAM models are incorporated at the end of the pre-trained models. The models are compared to state-of-the-art breast cancer diagnosis approaches and tested for accuracy, precision, recall, and F1 score. The confusion matrices are used to evaluate and visualize the results of the compared models. They help in assessing the models\u2019 performance. The test accuracy rates for the attention mechanism (AM) using the Xception model on the \u201cBreakHis\u201d breast cancer dataset are encouraging at 99.2% and 99.5%. The test accuracy for DenseNet121 with AMs is 99.6%. The proposed approaches also performed better than previous approaches examined in the related studies.",
        "references": [
            "57675cb01c5884f434e6ae0afe278e5da06f14b0",
            "79b3263d2b5ddebf079b9e7342344074e5aea042",
            "53dbfdb4d3bf8d498586b309a98c048b6da3e422",
            "3014d7614c3b5332dd6f40b22cf7c8c539595d93",
            "f2731860fba3b19f208a980a157dbb779bf76dc6",
            "7cbb2af0640372e2668407a1e870220aee162a0d",
            "335a5ae305f35420f30262b31ec7668ab0d7e79c",
            "ea5b27748ee154c978534505a177f280d87d52d4",
            "2e224826abb30096b477e5e02803c7c3b30b8aa9",
            "4bca68319e56b97744dc188e8389c7c0a30cb6ac"
        ],
        "related_topics": [],
        "reference_count": "31",
        "citation_count": "2"
    },
    {
        "Id": "27e1139126bdbcce1d998295dbf5eeafbda95fe9",
        "title": "Breast cancer detection and classification using metaheuristic optimized ensemble extreme learning machine",
        "authors": [
            "Raj Kumar Pattnaik",
            "Mohammad Siddique",
            "Satyasis Mishra",
            "Demissie Jobir Gelmecha",
            "Ram Sewak Singh",
            "Sunita Satapathy"
        ],
        "date": "29 September 2023",
        "abstract": "The proposed IWCA-APSO-based EELM model performs better than the traditional models at classifying breast cancer, and three benchmark functions are taken for optimization to demonstrate the proposed hybrid IWCA-APSO algorithm's uniqueness. Breast cancer deaths are increasing rapidly due to the abnormal growth of breast cells in the women's milk duct. Manual cancer diagnosis from mammogram images is also difficult for radiologists and medical practitioners. This paper proposes a novel metaheuristic algorithm-based machine learning model and Fuzzy C Means-based segmentation technique for the classification and detection of breast cancer from mammogram images. At first instance, the fuzzy factor improved fast and robust fuzzy c means (FFI-FRFCM) segmentation is proposed for the segmentation by modifying the member partition matrix of the FRFCM technique. Secondly, a hybrid improved water cycle algorithm-Accelerated particle swarm optimization (IWCA-APSO) optimization, is proposed for weight optimization of the ensemble extreme learning machine (EELM) model. Three benchmark functions are taken for optimization to demonstrate the proposed hybrid IWCA-APSO algorithm's uniqueness. With the INbreast dataset, the IWCA-APSO-based EELM classification shown the sensitivity, specificity, accuracy, and computational time as 99.67%, 99.71%, 99.36%, and 23.8751\u00a0s respectively. The proposed IWCA-APSO-based EELM model performs better than the traditional models at classifying breast cancer.",
        "references": [
            "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "34a91f535f841db560062e7266ac87069f282615",
            "254472f8888b3788b9c8196d92331b7acdef6418",
            "78fae2930fca4742a34966f891542ffdd9b76866",
            "cf5ee7a762dcaa735ce527700748fab63dee68d7",
            "6e5c1be5d62f534114c70170a53c787a9e118cbb",
            "5030c65fd3099507f2a2268091f6be262a8f8c54",
            "4ed705bc9b8fdaf6a2cd4e0d6e35e9ef15569de5",
            "3cb2d7ce3779879077d3ab2584436b9de2defc2c",
            "14f52a48954bdc3fd7ab872a05c49d60da1fdb8d"
        ],
        "related_topics": [],
        "reference_count": "39",
        "citation_count": "One"
    },
    {
        "Id": "7d04abd3fcfd4adbaf2738e996d4352826145575",
        "title": "Recent Advances in Selection Techniques for Image Processing",
        "authors": [
            "Sathiyaraj Chinnasamy",
            "M Ramachandran",
            "Vidhya Prasanth"
        ],
        "date": "1 August 2022",
        "abstract": "The parameters and modifying the code, the library allows students in image processing to learn practical methods, in addition to teaching programming in the \"turtle graphics\" paradigm, such as color and dimension and to introduce users to image ideas. The parameters and modifying the code, the library allows students in image processing to learn practical methods. In addition, in addition to teaching programming in the \"turtle graphics\" paradigm, such as color and dimension and to introduce users to image ideas A new module is provided. Online gallery of examples, in addition to providing an overview of the available activity, commonly used in image processing Introduces several algorithms. These usually include an introduction to the package and an insight, for image processing ideas Provides introductions. Well documented application programming interface (API) contributes to the learning experience with tools that facilitate visualization, It also makes it easier to explore the effect of various algorithms and parameters. So, it is not surprising that there are so many Image processing algorithms for margin extraction, upgrade, rearrangement; data compression, etc. are unambiguous. Artifacts can also be introduced through digital image processing such as margin enhancement. Since artifacts can prevent diagnosis or provide incorrect measurements, it is important to avoid them or at least understand their appearance. It is clear that a pattern independent of the spatial size or scale of image features is required and only emphasizes the range of less-contrasting features. Diversified image processing has been extensively studied not only by computer scientists but also by neurophysiologists, and the approach to improving this image is currently being used in a clinical way. It is well known that the human visual system uses a multifaceted approach.",
        "references": [
            "43fe930c477d597d1a0aa4b4e044354c507a8473",
            "b0d2ec75395f99c4b4c19c62af1b572384701be6",
            "cfb2be89c40a29d37210614afffa6cdd3a2e4ea3",
            "63d0c248ec64da5464521527a989acac39e25919",
            "ed7038b052af85886bf96c3511910ad654e1811a",
            "9fd37391b675db8b7751d00780abfbed7596fedf",
            "67b5929fb812f31c68846c37347b17726283b8a5",
            "1b439468ac15a07438a01d95b091691ca210b25e",
            "ff796fe1acc28f98ee44da7802135d1747083f6d",
            "4bae19a633ad5912ad400c449f2db0df9bb8a6d2"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "60"
    },
    {
        "Id": "42097772228e070b44f0af7c520a88e8656b9c53",
        "title": "Exploring the recent trends in Big Data Analysis",
        "authors": [
            "Supreeth Suresh",
            "M. Ramachandran",
            "Chinnasami Sivaji"
        ],
        "date": "1 July 2022",
        "abstract": "People, companies and more and more machines are now developing technologies to analyze more complex and large databases that cannot be handled by traditional management tools. Big Data is a collection of technologies developed to store, analyze and manage this data. It is a macro tool. Today it is used in fields as diverse as medicine, agriculture, gambling and environmental protection. Machine learning, forecasting Companies use big data to streamline their marketing campaigns and techniques. Modeling and other advanced analytics applications enable big data organizations to generate valuable insights. Companies use it in machine learning. Programs cannot balance large data in any particular database. When datasets are large in size, velocity, and volume, the following three distinct dimensions constitute \"big data.\" Examples of big data analytics include stock markets, social media platforms, and jet engines. But it is growing exponentially with time. Traditional data management tools cannot store or process it efficiently. It is a large scale technology developed by Story, Analysis and Manoj, Macro-Tool. Find patterns of exploding confusion in information about smart design, however, implying that various big data structures are structured, whether this includes the amount of information, the speed with which it is generated and collected, or the variety or scope of related data points. In the past, data was collected only from spreadsheets and databases of large, disparate information that grew at an ever-increasing rate. Very large, complex data refers to large amounts of data for sets that are impossible to analyze with traditional data processing applications. Is software configuration used to handle the problem? Apache is an advanced application for storing and processing large, complex data sets and analyzing big data. Analytical techniques against very large, heterogeneous databases containing varying amounts of data, big data analytics help businesses gain insights from today's massive data sources. Defined as software tools for processing and extraction. People, companies and more and more machines are now developing technologies to analyze more complex and large databases that cannot be handled by traditional management tools. It is designed to discover patterns of chaos that explode in information in order to design smart solutions.",
        "references": [
            "f7961fcfa4cf0d21c1fed4d827779770c26e2bf2",
            "17ed6e8d8b35cb4422d0f959e4ccb808f7bf5a3a",
            "cdc60c030cf44e920334cfaee947213de31fa294",
            "18b441dce3c40e6bff61e88c345752500f864a7f",
            "5ddbe916e203ed85d99828677133dbc6e29dd678",
            "b0150dd118ebedbc3ece68726e065f9afaaf3b18",
            "a8a40298b05466adf011b6f7f842427e6a1e5d02",
            "2467c80223843583462e8d587fa8fef39fff6074",
            "4413c60a668caade1c4c3bc6749f0f6dbb443673",
            "50a5d84937819cf68594082b577799c32e24c7ed"
        ],
        "related_topics": [],
        "reference_count": "57",
        "citation_count": "One"
    },
    {
        "Id": "8fcc6d8576911d34d4b2454a41685eea5408a93d",
        "title": "Artificial Intelligence of Things for Smarter Healthcare: A Survey of Advancements, Challenges, and Opportunities",
        "authors": [
            "Stephanie B. Baker",
            "Wei Xiang"
        ],
        "date": "2023",
        "abstract": "This work begins by briefly establishing a unified architecture of AIoT in a healthcare context, including sensors and devices, novel communication technologies, and cross-layer AI, and outlines promising directions for future research inAIoT for healthcare. Healthcare systems are under increasing strain due to a myriad of factors, from a steadily ageing global population to the current COVID-19 pandemic. In a world where we have needed to be connected but apart, the need for enhanced remote and at-home healthcare has become clear. The Internet of Things (IoT) offers a promising solution. The IoT has created a highly connected world, with billions of devices collecting and communicating data from a range of applications, including healthcare. Due to these high volumes of data, a natural synergy with Artificial Intelligence (AI) has become apparent - big data both enables and requires AI to interpret, understand, and make decisions that provide optimal outcomes. In this extensive survey, we thoroughly explore this synergy through an examination of the field of the Artificial Intelligence of Things (AIoT) for healthcare. This work begins by briefly establishing a unified architecture of AIoT in a healthcare context, including sensors and devices, novel communication technologies, and cross-layer AI. We then examine recent research pertaining to each component of the AIoT architecture from several key perspectives, identifying promising technologies, challenges, and opportunities that are unique to healthcare. Several examples of real-world AIoT healthcare use cases are then presented to illustrate the potential of these technologies. Lastly, this work outlines promising directions for future research in AIoT for healthcare.",
        "references": [
            "efcc84bc5542453001483bc9c81c7d4c6c409b80",
            "a1bbaf9f1994b6dbfd509abe05358c0051b0afd5",
            "1afa1751e0e4670399f6004ef519d311132ba44d",
            "439bc1325a31a46bb8073993b8ba301f3d9a7818",
            "5bea4972e7b7562a4ed5ad9bd539323f9076c439",
            "32ed6e8770f643251aa93b8e457b22773221a666",
            "04f117ff8881378c1912e2c2102b5c0cfb3d79f7",
            "79b84a85a5efbdd6da8cb4d6d58000b96aa0cbe8",
            "bab96d024441faf4560d6b1c9a25847103f0caad",
            "f1d92f97c01d49ef0ccc422d7a0f6eb4eeb73b20"
        ],
        "related_topics": [],
        "reference_count": "270",
        "citation_count": "18"
    },
    {
        "Id": "0ec81d2dd816c3ed4fa4b001de4a0bbbad380b03",
        "title": "Sustainable Transportation Systems Analysis using WASPAS MCDM Method",
        "authors": [
            "C. Venkateswaran",
            "M. Ramachandran",
            "Manjula Selvam"
        ],
        "date": "1 August 2022",
        "abstract": "In this form of analysis the WASPAS method is the most ideal solution Short-distance and negative-best The solution with the longest distance from the solution Determines, but the comparison of these distances Does not consider importance. From the result it is seen that Reliability is the first rank whereas is the Safety is having the lowest rank. Sustainable transportation systems are the need of modern times. There has been an unexpected growth in the number of transportation activities over years and the trend is expected to continue in the coming years. This has obviously associated environmental costs like air pollution, noise, etc. which is degrading the quality of life in modern cities. To cope with this crisis, municipal administrations are investing in sustainable transportation systems that are not only efficient, robust and economical but also friendly towards the environment. Sustainability has become an overarching concern for transportation policy and planning around the world. This article presents an approach for urban transport sustainability performance evaluation using fuzzy logic. This article presents a model for transport sustainability performance evaluation. Appropriate transport sustainability indicators were identified based on literature. Recently, sustainability has become a very important research area in transportation because of the dependencies between transportation, economic, and environmental systems. Alternative: Safety, Security, Reliability, Air pollutants, Noise. .Evaluation Preference: Cost benefits analysis optimization models, Life cycle analysis, Data analysis. WASPAS-Weighted Aggregated Sum Product Assessment. In this method from WASPAS in Sustainable Transportation Systems. From the result it is seen that Reliability and is the first rank whereas the Safety got is having the lowest rank.",
        "references": [
            "a29921604aba6e4a60848386109e58b60a16a7e8",
            "5b0cb042056810ea207922e386b553b3f05b1a90",
            "22ca79c93ad326923b3685cad2657f453048f96e",
            "d897382d826fcbf1f8412a6c5a75905bf4aba1ea",
            "1a20218246e19af739c4253aaeffc43efd319fe9",
            "94fa81c2884da66e8d2a8a97ab34bca026f0b323",
            "b9fc14af4c4012969f27a69f076594fe0d2e41af",
            "9f87b6c606fc5d9b65269400b6a6d2cf19bb55e2",
            "d276f808422fcfbffdb31f14f76bafb2daec23e5",
            "f5d22dacf89a89f96a9f0691f209ef2f125e4a38"
        ],
        "related_topics": [],
        "reference_count": "75",
        "citation_count": "2"
    },
    {
        "Id": "dfe3b46cf91ecb7e92c2f2da0b07f87c367d472f",
        "title": "Microcontroller Based Sensor Interface and Its Investigation",
        "authors": [
            "Kurinjimalar Ramu",
            "M Ramachandran",
            "Manjula Selvam"
        ],
        "date": "1 July 2022",
        "abstract": "A microcontroller (sometimes called an MCU or microcontroller unit) is an integrated circuit (IS) commonly used for a specific application designed to do certain tasks, often designed to govern functions of the portable of private computer systems. A microcontroller (sometimes called an MCU or microcontroller unit) is an integrated circuit (IS) commonly used for a specific application designed to do certain tasks. Devices, power tools, and automobile engine control products must be controlled automatically under certain conditions, such as computers and systems. And best examples of devices, but microcontrollers are beyond these applications. Essentially, the microcontroller collects input, processes this information, and publishes a specific action based on the information collected. Microcontrollers typically operate at low speeds of 1 MHz to 200 MHz and must be designed to use less power as they are embedded in other devices with higher power consumption in other areas. The microcontroller is a particular feature inside the embedded system a small included designed to manage Round. A common microcontroller chip processor, Memory and input output (I / O) gadgets Contains. Microcontrollers embedded Controller or microcontroller unit (MCU),Vehicles, robots, every now and then the workplace Machinery, Medical Devices, Mobile Radio Transceivers, vending machines and family Found in consumables. Unnecessarily small a small aspect designed to govern functions of the portable of private computer systems. For a complex pre-very last running system (OS).",
        "references": [
            "cab2644efd880480230a83ac017bb870fbc0d36e",
            "563da01a2a67840845434284a0bb653b3dceece7",
            "20c19c47114d44588c6562ce7479e40e077578ec",
            "e0dbe18a0f9b37049db39114d197b9751615a00f",
            "97a310663dec349b757510e830e5e0dbd90daf96",
            "5fb7c5741363e14df90c4124883c908f80c977e0",
            "f8106a9a8b9ebe2265b933e44fc9ff3229fb6271",
            "08ee39e68161b2518e6105590b6c545f4048c11b",
            "d0242ae19eb490166c13fbb6442f1bae6c11c069",
            "23b80a1bbf7ddf7a73b473f40328b5cfacd033a4"
        ],
        "related_topics": [],
        "reference_count": "54",
        "citation_count": "One"
    },
    {
        "Id": "2af1310dd611857d8bd1b1374695fc9c8913a4c9",
        "title": "Classifier Based Breast Cancer Segmentation",
        "authors": [
            "Samuel Rahimeto Kebede",
            "Taye Girma Debelee",
            "Friedhelm Schwenker",
            "Dereje Yohannes"
        ],
        "date": "1 November 2020",
        "abstract": "The ROIs obtained using the proposed classifier-based segmentation algorithm was compared with the ground truth annotated by the radiologists and the performance of the proposed algorithm was evaluated. Breast cancer occurs as a result of erratic growth and proliferation cells that originate in the breast. In this paper, the classifiers were used to identify the abnormalities on mammograms to get the region of interest (ROI). Before classifier based segmentation, noise, pectoral muscles, and tags were removed for a successful segmentation process. Then the proposed approach extracted the brightest regions using modified k-means. From the extracted brightest regions, shape and texture features were extracted and given to classifiers (KNN and SVM) and marked as ROI only those non-overlapping abnormal regions. The ROIs obtained using the proposed classifier-based segmentation algorithm was compared with the ground truth annotated by the radiologists. The datasets used to evaluate the performance of the proposed algorithm was public (MIAS) and local datasets (BGH and DADC).",
        "references": [
            "156733fbf757d4e8a333537518c8961163c4fbf7",
            "4cf3911b8613a47c82bd58d2597822d5de70606d",
            "fa967eac78f47b283de80399a28f5d7c3ca09f20",
            "0837f79a0efa2180c7bd35a6792c0e3a3e8cd258",
            "c55550ff224ff080d95d93200aea8838267fc21c",
            "3978804d0824c3bf1e17a1d0cd2e9734060fa058",
            "6325f450237d437e2072ef4939da023b08a6f2f1",
            "b046bdd97715dfcd7eeaadb709fe081d4d72d8a3",
            "63b6557f0e49ae79574e6a62715d64264e2f8013",
            "ea5b27748ee154c978534505a177f280d87d52d4"
        ],
        "related_topics": [
            "Classifier",
            "Support Vector Machines",
            "Radiologist",
            "Mammographic Image Analysis Society",
            "Pectoral Muscle",
            "Mammograms",
            "Texture Feature",
            "Local Dataset",
            "K-nearest Neighbors",
            "Region Of Interest"
        ],
        "reference_count": "34",
        "citation_count": "12"
    },
    {
        "Id": "4cf3911b8613a47c82bd58d2597822d5de70606d",
        "title": "Classification of Mammograms Using Texture and CNN Based Extracted Features",
        "authors": [
            "Taye Girma Debelee",
            "Abrham Kahsay Gebreselasie",
            "Friedhelm Schwenker",
            "Mohammadreza Amirian",
            "Dereje Yohannes"
        ],
        "date": "1 July 2019",
        "abstract": "A modified adaptive K-means (MAKM) method is proposed to extract the region of interest (ROI) from the local and public datasets and indicates that GLCM features from BGH dataset outperformed that of MIAS dataset in all five classifiers. In this paper, a modified adaptive K-means (MAKM) method is proposed to extract the region of interest (ROI) from the local and public datasets. The local image datasets are collected from Bethezata General Hospital (BGH) and the public datasets are from Mammographic Image Analysis Society (MIAS). The same image number is used for both datasets, 112 are abnormal and 208 are normal. Two texture features (GLCM and Gabor) from ROIs and one CNN based extracted features are considered in the experiment. CNN features are extracted using Inception-V3 pre-trained model after simple preprocessing and cropping. The quality of the features are evaluated individually and by fusing features to one another and five classifiers (SVM, KNN, MLP, RF, and NB) are used to measure the descriptive power of the features using cross-validation. The proposed approach was first evaluated on the local dataset and then applied to the public dataset. The results of the classifiers are measured using accuracy, sensitivity, specificity, kappa, computation time and AUC. The experimental analysis made using GLCM features from the two datasets indicates that GLCM features from BGH dataset outperformed that of MIAS dataset in all five classifiers. However, Gabor features from the two datasets scored the best result with two classifiers (SVM and MLP). For BGH and MIAS, SVM scored an accuracy of 99%, 97.46%, the sensitivity of 99.48%, 96.26% and specificity of 98.16%, 100% respectively. And MLP achieved an accuracy of 97%, 87.64%, the sensitivity of 97.40%, 96.65% and specificity of 96.26%, 75.73% respectively. Relatively maximum performance is achieved for feature fusion between Gabor and CNN based extracted features using MLP classifier. However, KNN, MLP, RF, and NB classifiers achieved almost 100% performance for GLCM texture features and SVM scored an accuracy of 96.88%, the sensitivity of 97.14% and specificity of 96.36%. As compared to other classifiers, NB has scored the least computation time in all experiments.",
        "references": [
            "156733fbf757d4e8a333537518c8961163c4fbf7",
            "11d9eff32a71ed852a8508a4c849903fbd4e7cce",
            "76825f26cadef0c2e80d19a0764dac94ccecff0b",
            "f247c2ef036dc00c72af2d10e767f18e48396d4e",
            "870d52e025216445bbc51434527f450c7be630fd",
            "eb8f6f8c48e6b61ea35e7294f26f7cfbbc3dd833",
            "95eb88e78a6e54fb31fbcbd7f9eda66ddef9e6b8",
            "6d8c60d47e2be4de763bb2f8044e09981016396e",
            "dfff650e5fc09244abf114827e18434d5bddd89e",
            "2f27a7b64c2223ebd922edb0b9644e76ffae0d14"
        ],
        "related_topics": [
            "Classifier",
            "Multi-layer Perceptron",
            "Support Vector Machines",
            "Mammographic Image Analysis Society",
            "Convolutional Neural Network",
            "Region Of Interest",
            "K-nearest Neighbors",
            "Texture Feature",
            "Cross-validation",
            "Area Under The ROC Curve"
        ],
        "reference_count": "36",
        "citation_count": "26"
    },
    {
        "Id": "577bfaf8c1797a1ce8c55f1bdb620eaa3b15f9db",
        "title": "Computer Aided Breast Cancer Detection Using Ensembling of Texture and Statistical Image Features",
        "authors": [
            "Soumya Deep Roy",
            "Soham Das",
            "Devroop Kar",
            "Friedhelm Schwenker",
            "Ram Sarkar"
        ],
        "date": "23 May 2021",
        "abstract": "An IDC Breast Cancer dataset that contains 277,524 images is used to classify the images into IDC(+) and IDC(-, which finds that CatBoost yielded the highest accuracy, which is at par with other state-of-the-art results\u2014most of which employ Deep Learning architectures. Breast cancer, like most forms of cancer, is a fatal disease that claims more than half a million lives every year. In 2020, breast cancer overtook lung cancer as the most commonly diagnosed form of cancer. Though extremely deadly, the survival rate and longevity increase substantially with early detection and diagnosis. The treatment protocol also varies with the stage of breast cancer. Diagnosis is typically done using histopathological slides from which it is possible to determine whether the tissue is in the Ductal Carcinoma In Situ (DCIS) stage, in which the cancerous cells have not spread into the encompassing breast tissue, or in the Invasive Ductal Carcinoma (IDC) stage, wherein the cells have penetrated into the neighboring tissues. IDC detection is extremely time-consuming and challenging for physicians. Hence, this can be modeled as an image classification task where pattern recognition and machine learning can be used to aid doctors and medical practitioners in making such crucial decisions. In the present paper, we use an IDC Breast Cancer dataset that contains 277,524 images (with 78,786 IDC positive images and 198,738 IDC negative images) to classify the images into IDC(+) and IDC(-). To that end, we use feature extractors, including textural features, such as SIFT, SURF and ORB, and statistical features, such as Haralick texture features. These features are then combined to yield a dataset of 782 features. These features are ensembled by stacking using various Machine Learning classifiers, such as Random Forest, Extra Trees, XGBoost, AdaBoost, CatBoost and Multi Layer Perceptron followed by feature selection using Pearson Correlation Coefficient to yield a dataset with four features that are then used for classification. From our experimental results, we found that CatBoost yielded the highest accuracy (92.55%), which is at par with other state-of-the-art results\u2014most of which employ Deep Learning architectures. The source code is available in the GitHub repository.",
        "references": [
            "d3d333de14acd1e07ee1ab9ffdbee65ae3509d23",
            "72ed83174a77c2cae15c91f4078daf8c422bd24e",
            "a74065b90c16130bc1a1d7ea82fdb0a2aed6cfe9",
            "156733fbf757d4e8a333537518c8961163c4fbf7",
            "2f11f86fd805807076b22317738c819484a8e21b",
            "37f65d98c5b5466b63c94a5c83c504b82786f23b",
            "4978d9ab6b70849b28dc44cf960111e6f851e9ad",
            "97c09bdd919fc139215c26366117697417b1006c",
            "d84bafd1fa5e28f41a1e6b0e7d7a36a61efc2fd8",
            "ea5b27748ee154c978534505a177f280d87d52d4"
        ],
        "related_topics": [
            "Invasive Ductal Carcinoma",
            "Internet Data Center",
            "CatBoost",
            "Machine Learning",
            "Multi-layer Perceptron",
            "Feature Extractor",
            "Extra-Trees",
            "XGBoost",
            "AdaBoost",
            "Textural Features"
        ],
        "reference_count": "34",
        "citation_count": "21"
    },
    {
        "Id": "bf9fab73a955c7bea893092b6e963da0052ddf83",
        "title": "Hybrid computer aided diagnostic system designs for screen film mammograms using DL\u2010based feature extraction and ML\u2010based classifiers",
        "authors": [
            "Jyoti Rani",
            "Jaswinder Singh",
            "Jitendra Virmani"
        ],
        "date": "3 May 2023",
        "abstract": "Mammography is most popular imaging method used often in predicting breast cancer within women above age of 38\u2009years. Various computer\u2010assisted algorithms have been employed for classifying breast masses as normal or malignant using screen film mammographic (SFM) images. In present research work, exhaustive experimentations have been carried out with nine deep learning\u2010based convolutional neural networks (CNNs) belonging to the three different categories of CNN architectures including (a) simple convolution\u2010based series models, that is VGG16 and VGG19 (b) simple convolution\u2010based directed acyclic graph (DAG) model, that is GoogleNet and (c) dilated convolution\u2010based DAG models, that is ResNet18, ResNet50, Inceptionv3, XceptionNet, ShuffleNet and MobileNet\u2010V2, for binary classification of the mammographic masses with SFM images. The experimental work has been carried out using 518 mammographic images taken from DDSM dataset with 208 images \u03f5 benign class and 310 images \u03f5 malignant class, respectively. The encoder\u2010decoder based semantic segmentation network model, that is ResNet50 has been used for the segmentation of mammographic masses from SFM images. The segmented masses images obtained from the ResNet50 model are subjected to classification experiments. To design a robust hybrid classifier design, that is, deep learning (DL)\u2010based feature extraction and machine learning (ML)\u2010based classification, the first step is to obtain an optimal DL\u2010based feature extractor for classification task. The optimal feature set obtained by the best performing CNN Model, that is, VGG 19 has been subjected to correlation\u2010based feature extraction and ML\u2010based classifiers including (i) adaptive neuro fuzzy classifier\u2010linguistic hedges (ii) principal component analysis\u2010 support vector machine classifier and (iii) GA\u2010SVM classifier to yield an optimal hybrid computer aided diagnostic (CAD) system design. It is found that hybrid CAD system using VGG19 as feature extractor and ANFC\u2010LH as classifier yields 96% the highest classification accuracy. The other performance parameters yield values, that is 96% sensitivity, 96% specificity, 96% F\u2010score, 96% precision and 92% MCC, which indicates a best prediction of binary classification. The test images which were misclassified by these hybrid CAD system designs were analysed subjectively by experienced participating radiologist. The results obtained by present work suggests that proposed hybrid CAD system with VGG19 Network model acting as feature extractor and ANFC\u2010LH classifier can be employed for the differential diagnosis of benign as well as malignant mammographic masses using SFM images in a routine clinical setting.",
        "references": [
            "8b8d30acb191667504462578615dda741f15cc75",
            "6c26a46ce3a3793ae47ff48b9725c72dfa8cc06a",
            "715d7b34d2b3f58a257e43aa9c1a5122a3718915",
            "257438fca36f8e1bcf1ca7aedc70364603496818",
            "f0741c59f750fcfbe1288654c328bc06408d2b11",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741",
            "fc3e53a00559dc17a6a35fd8c2e9e9df94fb9232",
            "cc91294404953dc555edd2de2156c88754654a0d",
            "d0f0f7fdb13fcbe77dcc28c6662a0981ef02e80e",
            "14a0fe917e65cd08695e7438eec254a3e7538225"
        ],
        "related_topics": [
            "Screen Film Mammography",
            "Social Force Model",
            "ResNet-50",
            "Deep Learning",
            "Binary Classification",
            "VGG16",
            "Malignant",
            "GoogLeNet",
            "Encoder-decoders",
            "Classification"
        ],
        "reference_count": "105",
        "citation_count": "One"
    },
    {
        "Id": "8d77a44b3dc85186a08a1b132ca73bc08aad7583",
        "title": "Feature fused breast cancer detection",
        "authors": [
            "Adila K.P.",
            "Sheeba K"
        ],
        "date": "23 September 2020",
        "abstract": "AlexNet architecture shows an improved accuracy in classifying in mammogram and late multimodal fusion technique implemented combines the output from individual classifiers and differentiates benign and malignant growth. Breast cancer is dreadful, fatal and a widespread disease now. Mammogram is a commonly used imaging technique for screening. Deep learning is applied in clinical dataset for medical imaging. Images are affected by quality factors. To diagnose more accurately pre-processing methods are required. Here denoising is done by wiener filter and image enhancement is performed by contrast limited adaptive histogram equalization. As medical image segmentation has an important role in computer aided diagnosis, a hybrid segmentation technique, thresholding and morphological operation is done. Image feature analysis is done by a convolutional neural network and geometric and texture feature analysis is done by multi-layer perceptron. A multi-modal feature fusion technique which merge low-level features with high-level features is applied. Late multimodal fusion technique implemented combines the output from individual classifiers and differentiates benign and malignant growth. In the performed work AlexNet architecture shows an improved accuracy in classifying.",
        "references": [
            "d526d4998ce24c5b3360262a870b34da2e735edd",
            "8967ea71b194a28744c15a3be93fa6e32ddac1e9",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741",
            "1500a27efd9832beb0b92c34c5bb4e12009d9197",
            "e7e401f77802461d0b6a2b25ed3eacd50fc11e1f",
            "0c0ab9edfaa8c1f6cad563e41dc882c8de5514c7",
            "03c8484c9632d03c9fd37d9ec665f7fe57d20b4e",
            "5f830d8af712a0c8cf23ac5c4faa6906b435c4d7",
            "8a74c98896b57626324520741cb172013421d6f6",
            "156733fbf757d4e8a333537518c8961163c4fbf7"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "0295f85e0db918b26ff7e8d1d547f8d98eea7feb",
        "title": "A Survey of Convolutional Neural Network in Breast Cancer",
        "authors": [
            "Ziquan Zhu",
            "Shuihua Wang",
            "Yudong Zhang"
        ],
        "date": "9 March 2023",
        "abstract": "A comprehensive review of the diagnosis of breast cancer based on the convolutional neural network (CNN) after reviewing a sea of recent papers and dividing the diagnosis into three different tasks. Problems For people all over the world, cancer is one of the most feared diseases. Cancer is one of the major obstacles to improving life expectancy in countries around the world and one of the biggest causes of death before the age of 70 in 112 countries. Among all kinds of cancers, breast cancer is the most common cancer for women. The data showed that female breast cancer had become one of the most common cancers. Aims A large number of clinical trials have proved that if breast cancer is diagnosed at an early stage, it could give patients more treatment options and improve the treatment effect and survival ability. Based on this situation, there are many diagnostic methods for breast cancer, such as computer-aided diagnosis (CAD). Methods We complete a comprehensive review of the diagnosis of breast cancer based on the convolutional neural network (CNN) after reviewing a sea of recent papers. Firstly, we introduce several different imaging modalities. The structure of CNN is given in the second part. After that, we introduce some public breast cancer data sets. Then, we divide the diagnosis of breast cancer into three different tasks: 1. classification; 2. detection; 3. segmentation. Conclusion Although this diagnosis with CNN has achieved great success, there are still some limitations. (i) There are too few good data sets. A good public breast cancer dataset needs to involve many aspects, such as professional medical knowledge, privacy issues, financial issues, dataset size, and so on. (ii) When the data set is too large, the CNN-based model needs a sea of computation and time to complete the diagnosis. (iii) It is easy to cause overfitting when using small data sets.",
        "references": [
            "00b2c1394bb6195c00fd0fc895aeac26b65e9d5f",
            "1500a27efd9832beb0b92c34c5bb4e12009d9197",
            "a812ebb6761ff453a02d866e97295932153537e2",
            "8eb5f9cf359790bc9b3e00985544c77237ada9c7",
            "1a19f55bc7f5d3d90bd5cb239c0202b3428b6aa2",
            "f4008deb11f6a2ec3d52dde72be48cd53c898a45",
            "df8dc5ebc7a0abadfba095da292ec9c9c27c783c",
            "7d4b1e7c47c02cc62705dce132e187cc53207ff3",
            "c4fc87dd6c5b311e585a45e85cc4122b9986ad9d",
            "1a88ddff5eec7c40ec7d1205277871664fa7c68c"
        ],
        "related_topics": [],
        "reference_count": "202",
        "citation_count": "9"
    },
    {
        "Id": "abfb457caba5314466652825260c12da01a41e0f",
        "title": "Deep Learning in Selected Cancers\u2019 Image Analysis\u2014A Survey",
        "authors": [
            "Taye Girma Debelee",
            "Samuel Rahimeto Kebede",
            "Friedhelm Schwenker",
            "Zemene Matewos Shewarega"
        ],
        "date": "1 November 2020",
        "abstract": "The result of the review process indicated that deep learning methods have achieved state-of-the-art in tumor detection, segmentation, feature extraction and classification. Deep learning algorithms have become the first choice as an approach to medical image analysis, face recognition, and emotion recognition. In this survey, several deep-learning-based approaches applied to breast cancer, cervical cancer, brain tumor, colon and lung cancers are studied and reviewed. Deep learning has been applied in almost all of the imaging modalities used for cervical and breast cancers and MRIs for the brain tumor. The result of the review process indicated that deep learning methods have achieved state-of-the-art in tumor detection, segmentation, feature extraction and classification. As presented in this paper, the deep learning approaches were used in three different modes that include training from scratch, transfer learning through freezing some layers of the deep learning network and modifying the architecture to reduce the number of parameters existing in the network. Moreover, the application of deep learning to imaging devices for the detection of various cancer cases has been studied by researchers affiliated to academic and medical institutes in economically developed countries; while, the study has not had much attention in Africa despite the dramatic soar of cancer risks in the continent.",
        "references": [
            "ea5b27748ee154c978534505a177f280d87d52d4",
            "d8c6761cb923a33170a3f5a1be30a09070e174d4",
            "2b18e8af1b9c007f1c04d93c0f0b1642c1c4e4f9",
            "70167247fe82408b27f34188c58a27cca698c690",
            "6005102d275c09b5d5ccd3495c55ae7f7c9aad54",
            "abb9b66bc548dbbf25ae242e0996f8a61027cf23",
            "63199437cf214b46fd4a9f6e43784d7d118065b8",
            "b8761df8782f81076930c00c97f5a9aeaa83116f",
            "66794ec56f19b226cb1b0ec7dca77c90ec8694ef",
            "d89c815eb2c79471bcc087d85856d0fcec7d9590"
        ],
        "related_topics": [
            "Deep Learning",
            "Brain Tumors",
            "Medical Image Analysis",
            "Emotion Recognition",
            "Training From Scratch",
            "Face Recognition",
            "Parameters",
            "Classification",
            "Transfer Learning",
            "Architecture"
        ],
        "reference_count": "153",
        "citation_count": "42"
    },
    {
        "Id": "ea5b27748ee154c978534505a177f280d87d52d4",
        "title": "Survey of deep learning in breast cancer image analysis",
        "authors": [
            "Taye Girma Debelee",
            "Friedhelm Schwenker",
            "Achim Ibenthal",
            "Dereje Yohannes"
        ],
        "date": "1 March 2020",
        "abstract": "The most common breast cancer imaging modalities, public, most cited and recently updated breast cancer databases, histopathological based breast cancer image analysis, and DL application types in medical image analysis are reviewed. Computer-aided image analysis for better understanding of images has been time-honored approaches in the medical computing field. In the conventional machine learning approach, the domain experts in medical images are mandatory for image annotation that subsequently to be used for feature engineering. However, in deep learning, a big jump has been made to help the researchers do segmentation, feature extraction, classification, and detection from raw medical images obtained using digital breast tomosynthesis, digital mammography, magnetic resonance imaging, and ultrasound imaging modalities. As a result, deep learning (DL) has gained a state-of-the-art in many application areas, for example, breast cancer image analysis. In this survey paper, we reviewed the most common breast cancer imaging modalities, public, most cited and recently updated breast cancer databases, histopathological based breast cancer image analysis, and DL application types in medical image analysis. We finally conclude by pointing out the research gaps to be addressed in the future.",
        "references": [
            "8866a3f71098be5eccab69903cc248e75e8727be",
            "7ab0f0da686cd4094fd96f5a30e0b6072525fd09",
            "011763e5b4009bc9c8adc894af22af9902a6d4bc",
            "907ac4d30e4fc9fc5059b35647b3a853ad5085ea",
            "96b2bc59a36fea81681d782cffe00d942a481249",
            "198d308169e7b95aced6e6b65918a548be20235d",
            "002551e8eff3a53e004a69897937d75b7a046281",
            "216cc6fccb3e239350564213d36830d3ca2bbc28",
            "e121a1e9c0b37c91beaf9f96af1ff71954faa7fc",
            "0ef50960ebe7d6760d4ace0d369bc348f8314969"
        ],
        "related_topics": [
            "Deep Learning",
            "Image Annotation",
            "Machine Learning",
            "Classification"
        ],
        "reference_count": "156",
        "citation_count": "90"
    },
    {
        "Id": "58dcb43c4909ca0178bbd5d4a4199731ad16c404",
        "title": "Survey of deep learning in breast cancer image analysis",
        "authors": [
            "Taye Girma Debelee",
            "Friedhelm Schwenker",
            "Achim Ibenthal",
            "Dereje Yohannes"
        ],
        "date": "24 August 2019",
        "abstract": "The most common breast cancer imaging modalities, public, most cited and recently updated breast cancer databases, histopathological based breast cancer image analysis, and DL application types in medical image analysis are reviewed. Computer-aided image analysis for better understanding of images has been time-honored approaches in the medical computing field. In the conventional machine learning approach, the domain experts in medical images are mandatory for image annotation that subsequently to be used for feature engineering. However, in deep learning, a big jump has been made to help the researchers do segmentation, feature extraction, classification, and detection from raw medical images obtained using digital breast tomosynthesis, digital mammography, magnetic resonance imaging, and ultrasound imaging modalities. As a result, deep learning (DL) has gained a state-of-the-art in many application areas, for example, breast cancer image analysis. In this survey paper, we reviewed the most common breast cancer imaging modalities, public, most cited and recently updated breast cancer databases, histopathological based breast cancer image analysis, and DL application types in medical image analysis. We finally conclude by pointing out the research gaps to be addressed in the future.",
        "references": [
            "8866a3f71098be5eccab69903cc248e75e8727be",
            "907ac4d30e4fc9fc5059b35647b3a853ad5085ea",
            "96b2bc59a36fea81681d782cffe00d942a481249",
            "198d308169e7b95aced6e6b65918a548be20235d",
            "002551e8eff3a53e004a69897937d75b7a046281",
            "5e0b34c8a371dcd7d623169c079f56ac75ffcd1e",
            "216cc6fccb3e239350564213d36830d3ca2bbc28",
            "e121a1e9c0b37c91beaf9f96af1ff71954faa7fc",
            "0ef50960ebe7d6760d4ace0d369bc348f8314969",
            "41841138f0844ee9259cd738b8c9227c0e76160c"
        ],
        "related_topics": [],
        "reference_count": "161",
        "citation_count": "7"
    },
    {
        "Id": "44eb9aafb0d32bf9c8dfd2a4f0f4a418be779a08",
        "title": "Deep Learning Based Computer-Aided Systems for Breast Cancer Imaging : A Critical Review",
        "authors": [
            "Yuliana Jim&#x27;enez-Gaona",
            "Mar&#x27;ia Jos&#x27;e Rodr&#x27;iguez-&#x27;Alvarez",
            "Vasudevan Lakshminarayanan"
        ],
        "date": "30 September 2020",
        "abstract": "The main findings in the classification process reveal that new DL-CAD methods are useful and effective screening tools for breast cancer, thus reducing the need for manual feature extraction. This paper provides a critical review of the literature on deep learning applications in breast tumor diagnosis using ultrasound and mammography images. It also summarizes recent advances in computer-aided diagnosis/detection (CAD) systems, which make use of new deep learning methods to automatically recognize breast images and improve the accuracy of diagnoses made by radiologists. This review is based upon published literature in the past decade (January 2010\u2013January 2020), where we obtained around 250 research articles, and after an eligibility process, 59 articles were presented in more detail. The main findings in the classification process revealed that new DL-CAD methods are useful and effective screening tools for breast cancer, thus reducing the need for manual feature extraction. The breast tumor research community can utilize this survey as a basis for their current and future studies.",
        "references": [
            "ea5b27748ee154c978534505a177f280d87d52d4",
            "0210a41a44fdbf13ff2e7b546f94d84b99f8265b",
            "4102d50123e3eba572fdcd310acefa551e191b4f",
            "951fbb632fd02fd57fb1d864bbd183ebb93172e0",
            "571f0c4b3a82cdf130effd545e5f17edac522a00",
            "7d8d6d30ce3bd383f947d6d88e22e00e809065f7",
            "831a1e8106571d82f22cd8dac7725964852b0154",
            "7d4b1e7c47c02cc62705dce132e187cc53207ff3",
            "e65e3aaa1d49b4f2cb1aac49230e6339a77290cc",
            "3f09e4926b6a437849f1372bbd91a4cf3435c741"
        ],
        "related_topics": [
            "Deep Learning",
            "Radiologist",
            "Cylindrical Algebraic Decomposition"
        ],
        "reference_count": "211",
        "citation_count": "47"
    },
    {
        "Id": "426573514d4d64451ca34aa5e794d56fb1d98d69",
        "title": "Preparation and characterization of hafnium-zirconium oxide ceramics as a CMOS compatible material for non-volatile memories",
        "authors": [
            "Urvashi Sharma",
            "Charanjeet Singh",
            "Vishnu M Varma",
            "G. Santhosh Kumar",
            "Sachin Mishra",
            "Ajay Kumar",
            "Reji Thomas"
        ],
        "date": "27 March 2023",
        "abstract": "HfxZr1\u2212xO2 (HZO) ceramics with x\u2009=\u20090.25, 0.50 and 0.75 were prepared by conventional solid-state reaction technique. Structural studies of the synthesized stoichiometric compounds were done by X-ray diffraction. The microstructure/morphology and composition of ceramics were obtained with field emission-scanning electron microscopy and energy dispersive X-ray, respectively. Impedance spectroscopic studies were done to study the temperature-dependent and frequency-dependent dielectric properties. The dielectric constants at 10 kHz of HfxZr1\u2212xO2 ceramics were increased from 17 to 40 with Hf varying from 0.25 to 0.75 at ambient conditions. Ferroelectric studies were also done on these compositions with the help of a polarization-electric field plot and are discussed.",
        "references": [
            "ba2f7afcf11015cccd094771ea5015a3405788c0",
            "c8f1a55222c3bb6d8a1038c5e837ef7bb07e8239",
            "2d9ab64500030e78c249f329855d3418b100459c",
            "126d6c47da0b3bc4c07b5715b786e84441deaa19",
            "febc0ff35caa44a3674f17825d21bc8b40f84abe",
            "c4b81dbbe80da09b7779d5fce6f267b1be2b8376",
            "9fb3cb299e1349bf9b18d4dd6d07cf58ad647338",
            "bacdc5c73886ea571276fdfed9fdbd6e08a783a2",
            "8206eeea1b71e6d8afb633082d953aed55d27e36",
            "6be7188841668e38a1a000e2db92bea3f9eacd9d"
        ],
        "related_topics": [],
        "reference_count": "33",
        "citation_count": "One"
    },
    {
        "Id": "c0cdec8e8149ab2e7d185893e3f18eb0089a0f2f",
        "title": "Classification of breast masses in mammograms using genetic programming and feature selection",
        "authors": [
            "Robin J. Nandi",
            "Asoke Kumar Nandi",
            "Rangaraj M. Rangayyan",
            "D. Scutt"
        ],
        "date": "21 July 2006",
        "abstract": "A shape feature known as fractional concavity was found to be the most important among those tested, since it was automatically selected by the GP classifier in almost every experiment. Mammography is a widely used screening tool and is the gold standard for the early detection of breast cancer. The classification of breast masses into the benign and malignant categories is an important problem in the area of computer-aided diagnosis of breast cancer. A small dataset of 57 breast mass images, each with 22 features computed, was used in this investigation; the same dataset has been previously used in other studies. The extracted features relate to edge-sharpness, shape, and texture. The novelty of this paper is the adaptation and application of the classification technique called genetic programming (GP), which possesses feature selection implicitly. To refine the pool of features available to the GP classifier, we used feature-selection methods, including the introduction of three statistical measures\u2014Student\u2019s t test, Kolmogorov\u2013Smirnov test, and Kullback\u2013Leibler divergence. Both the training and test accuracies obtained were high: above 99.5% for training and typically above 98% for test experiments. A leave-one-out experiment showed 97.3% success in the classification of benign masses and 95.0% success in the classification of malignant tumors. A shape feature known as fractional concavity was found to be the most important among those tested, since it was automatically selected by the GP classifier in almost every experiment.",
        "references": [
            "5fac84ef49bc6dcc4aeaf6c5623173f8c0e2a531",
            "f8b061df30fa1eb1ca444cedf2e9be9d091120f2",
            "ac0ca4295b0622e370c19716071614b6d2d723f9",
            "ba721e81d2ab8114bc9afcf78f8335c1f6475364",
            "f2f53333761f6badfa8cbc3aaf90c82455c4745e",
            "1060c5b534e53e213b9e3f9b8a75b54e3639cee8",
            "16f3cb52eb1f374aa037c4080ccbf68d2d7b8cd1",
            "66cb958c5fe045d9037724f38c5ca7b4728a13f1",
            "0ffefa7d286b24f7c20bedf779cd7bc7d1f64ceb",
            "13c5a81b319445f3e6bba16f4e76fcf0efd87354"
        ],
        "related_topics": [
            "Classification",
            "Feature Selection",
            "Malignant",
            "Genetic Programming",
            "Fractional Concavity",
            "Test Accuracy",
            "Mammograms"
        ],
        "reference_count": "36",
        "citation_count": "90"
    },
    {
        "Id": "76389ebb7c1496239e66fd663b0e7e43d391bca9",
        "title": "Performance evaluation of a region growing procedure for mammographic breast lesion identification",
        "authors": [
            "Giulia Rabottino",
            "Arianna Mencattini",
            "Marcello Salmeri",
            "Federica Caselli",
            "Roberto Lojacono"
        ],
        "date": "1 February 2011",
        "abstract": "Semantic Scholar extracted view of \"Performance evaluation of a region growing procedure for mammographic breast lesion identification\" by G. Rabottino et al.",
        "references": [
            "5a568905f56d5c424c671757552afc03e70f73fd",
            "e652b81c4aba8e104c8aee064cc90923293cdb82",
            "a496b4de4b92d905ec2daaecd9af154ea47481f3",
            "baac72cbca8efd95e76ea17577e8e90e49fc7275",
            "37ed2eba228d850b3b34472c216d4ed20e33e151",
            "e83fdb20beb9908fa6a8b52df03197ba84ef2188",
            "8d46682acbe8ac27ab8e29163e64f1b3cbb70bb0",
            "e4488ebd82674a981852c66c3138cb0bb9fe6b26",
            "04fc0c398508822f3b22a8065a48f17ed69baeb0",
            "295f223238012947e5ffc1e5a0e7c9bd52807977"
        ],
        "related_topics": [
            "Tumors",
            "Radiologist",
            "Mammograms"
        ],
        "reference_count": "25",
        "citation_count": "25"
    },
    {
        "Id": "662d927da3c17bfde61da5dbc24c037dce30ce25",
        "title": "Computer-Aided Diagnosis of Malignant Mammograms using Zernike Moments and SVM",
        "authors": [
            "Shubhi Sharma",
            "Pritee Khanna"
        ],
        "date": "1 February 2015",
        "abstract": "This work is directed toward the development of a computer-aided diagnosis (CAD) system to detect abnormalities or suspicious areas in digital mammograms and classify them as malignant or nonmalignant, and proves the applicability of Zernike moments as a fitting texture descriptor. This work is directed toward the development of a computer-aided diagnosis (CAD) system to detect abnormalities or suspicious areas in digital mammograms and classify them as malignant or nonmalignant. Original mammogram is preprocessed to separate the breast region from its background. To work on the suspicious area of the breast, region of interest (ROI) patches of a fixed size of 128\u00d7128 are extracted from the original large-sized digital mammograms. For training, patches are extracted manually from a preprocessed mammogram. For testing, patches are extracted from a highly dense area identified by clustering technique. For all extracted patches corresponding to a mammogram, Zernike moments of different orders are computed and stored as a feature vector. A support vector machine (SVM) is used to classify extracted ROI patches. The experimental study shows that the use of Zernike moments with order 20 and SVM classifier gives better results among other studies. The proposed system is tested on Image Retrieval In Medical Application (IRMA) reference dataset and Digital Database for Screening Mammography (DDSM) mammogram database. On IRMA reference dataset, it attains 99 % sensitivity and 99 % specificity, and on DDSM mammogram database, it obtained 97 % sensitivity and 96 % specificity. To verify the applicability of Zernike moments as a fitting texture descriptor, the performance of the proposed CAD system is compared with the other well-known texture descriptors namely gray-level co-occurrence matrix (GLCM) and discrete cosine transform (DCT).",
        "references": [
            "5c84051b348beb94f07cec39f389b5c88146dec5",
            "7a16973606f2f34e06a6446f22105cc342576202",
            "e01f9f37c0b096990025b9b2861cd05d84ffa665",
            "46c409dd878e643271ef63f1817ded8b57abc01e",
            "db3101c7057d71ca42edc17e2c30353ff1b70120",
            "dda0dc40bc5cb79d5638d8cac955337362e6473e",
            "047947cfa21a45d9beea07cc2a2f579bb1c8a0da",
            "54ceeab87535d4048eb262dcc8cdbc46ae735ea4",
            "c5ac9c778de5186dc42e6052bb7f0d28e6b279e5",
            "c1ec2c9f9d2e8914cf6de51e4cef0b15e061b944"
        ],
        "related_topics": [
            "Zernike Moments",
            "Support Vector Machines",
            "Image Retrieval In Medical Application",
            "Texture Descriptors",
            "Digital Database For Screening Mammography",
            "Discrete Cosine Transform",
            "Cylindrical Algebraic Decomposition",
            "Feature Vector",
            "Malignant",
            "Patches"
        ],
        "reference_count": "52",
        "citation_count": "102"
    },
    {
        "Id": "9bf32a68edfea8b8c072bcd3ee0d696687bab403",
        "title": "A review of computer-aided diagnosis of breast cancer: Toward the detection of subtle signs",
        "authors": [
            "Rangaraj M. Rangayyan",
            "F{\\&#x27;a}bio J. Ayres",
            "J. E. Leo Desautels"
        ],
        "date": "1 May 2007",
        "abstract": "Semantic Scholar extracted view of \"A review of computer-aided diagnosis of breast cancer: Toward the detection of subtle signs\" by R. Rangayyan et al.",
        "references": [
            "ad62b724df12fd40169ae8115f5e156adda99283",
            "e4a65a02428a6c82c6034f194ee894afa324890f",
            "55f1596a7c8073d8dc675a3e86519fd5f9e1f5d8",
            "8706fa1f5aa5721a6c36c8ec088122e08dda20e7",
            "acc3f5d417d18ad994466940e6cfa7289fc9d947",
            "968157d1264ea1ac6fb33f861c54ed1765ecd2af",
            "b072abd46ff6abc6c527710c7eef2b00055e4733",
            "a88d12a88e711702041bedeb5dfe482af96cc959",
            "52e66cbe0bc8ce5fbdcd8e0d2c309d35f64ed848",
            "f8ff28e4c2e415cef87cd77254ea03b788f901da"
        ],
        "related_topics": [
            "Radiologist",
            "Cylindrical Algebraic Decomposition",
            "Visual Prompts",
            "Digital Image Processing",
            "Tumors",
            "Architectural Distortions",
            "Contrast Enhancement",
            "Mammograms"
        ],
        "reference_count": "118",
        "citation_count": "356"
    },
    {
        "Id": "da4237818523ea3e3d44dbe18c261afc6d6d404f",
        "title": "Optimization of breast mass classification using sequential forward floating selection (SFFS) and a support vector machine (SVM) model",
        "authors": [
            "Maxine Tan",
            "Jiantao Pu",
            "Bin Zheng"
        ],
        "date": "25 March 2014",
        "abstract": "The results showed that the most frequently selected features by the SFFS-based algorithm in tenfold iterations were those related to mass shape, isodensity, and presence of fat, which are consistent with the image features used by radiologists in the clinical environment for mass classification. PurposeImproving radiologists\u2019 performance in classification between malignant and benign breast lesions is important to increase cancer detection sensitivity and reduce false-positive recalls. For this purpose, developing computer-aided diagnosis schemes has been attracting research interest in recent years. In this study, we investigated a new feature selection method for the task of breast mass classification.MethodsWe initially computed 181 image features based on mass shape, spiculation, contrast, presence of fat or calcifications, texture, isodensity, and other morphological features. From this large image feature pool, we used a sequential forward floating selection (SFFS)-based feature selection method to select relevant features and analyzed their performance using a support vector machine (SVM) model trained for the classification task. On a database of 600 benign and 600 malignant mass regions of interest, we performed the study using a tenfold cross-validation method. Feature selection and optimization of the SVM parameters were conducted on the training subsets only.ResultsThe area under the receiver operating characteristic curve $$(\\hbox {AUC}) = 0.805\\pm 0.012$$(AUC)=0.805\u00b10.012 was obtained for the classification task. The results also showed that the most frequently selected features by the SFFS-based algorithm in tenfold iterations were those related to mass shape, isodensity, and presence of fat, which are consistent with the image features frequently used by radiologists in the clinical environment for mass classification. The study also indicated that accurately computing mass spiculation features from the projection mammograms was difficult, and failed to perform well for the mass classification task due to tissue overlap within the benign mass regions.ConclusionIn conclusion, this comprehensive feature analysis study provided new and valuable information for optimizing computerized mass classification schemes that may have potential to be useful as a \u201csecond reader\u201d in future clinical practice.",
        "references": [
            "c0cdec8e8149ab2e7d185893e3f18eb0089a0f2f",
            "c1b8abebc968d811d1121b11145ee8bb360b9351",
            "1448eded35e963c7316f64f9c1b54abe519665fa",
            "5341997de2d5e90a07145f731b4edc282c2d3d23",
            "a3825cafd02659229fb6a65b2c8c4be970f13726",
            "5fac84ef49bc6dcc4aeaf6c5623173f8c0e2a531",
            "f8b061df30fa1eb1ca444cedf2e9be9d091120f2",
            "841e4c4843c6fb70b1b631f471e24bbf1f52f7ae",
            "da6f9a25ebc31ae4572e4ec9b8bab37c7952af7a",
            "a277fe4df26e09a6c375e32d77472ae2c9a7632d"
        ],
        "related_topics": [
            "Isodensity",
            "Classification",
            "Sequential Forward Floating Selection",
            "Support Vector Machines",
            "Malignant",
            "Breast Mass Classification",
            "Radiologist",
            "Feature Selection",
            "Optimization",
            "Breast Lesions"
        ],
        "reference_count": "75",
        "citation_count": "77"
    },
    {
        "Id": "f0a4d96b5d7f6668d607512f8744e07dbe1c9b27",
        "title": "Mammogram Classification using Law's Texture Energy Measure and Neural Networks",
        "authors": [
            "Arden Sagiterry Setiawan",
            "Elysia",
            "Julian Wesley",
            "Yudy Purnama"
        ],
        "date": "2015",
        "abstract": "Semantic Scholar extracted view of \"Mammogram Classification using Law's Texture Energy Measure and Neural Networks\" by Arden Sagiterry Setiawan et al.",
        "references": [
            "c8c81a85bb3387c897e233173e1a0264e8d3e654",
            "13cb8d2252c5578ebc8f034e07feb1d0336966c2",
            "6065785bda135acf1ab0d95d7ec4624cbd148342",
            "20e9ccf0fa762f6afb3f51e8fb62a128e3235063",
            "1ca488cbbc84138e12e405661a2db7fcc9f2e4f1",
            "964ca23551a3a0d863d572449eb2a208374f9a27",
            "fbf8dfccd3bf1db32f9822e6b1cb5ec40488e8e5",
            "ea5f35acafa5a1d60cbfa970d0c8f44eec05dcc8"
        ],
        "related_topics": [
            "Law's Texture Energy Measure",
            "Classification",
            "Classifier",
            "Mammogram Classification",
            "Approximate Nearest Neighbor",
            "Neural Network",
            "Mammograms"
        ],
        "reference_count": "8",
        "citation_count": "78"
    },
    {
        "Id": "db480b3f22244151cfeaa4beedc406a5137d1a4e",
        "title": "MammoSys: A content-based image retrieval system using breast density patterns",
        "authors": [
            "J{\\&#x27;u}lia Epischina Engr{\\&#x27;a}cia de Oliveira",
            "Alexei Manso Corr{\\^e}a Machado",
            "Guillermo C{\\&#x27;a}mara Ch{\\&#x27;a}vez",
            "Ana Paula Brand{\\~a}o Lopes",
            "Thomas Martin Deserno",
            "Arnaldo de Albuquerque Ara{\\&#x27;u}jo"
        ],
        "date": "1 September 2010",
        "abstract": "Semantic Scholar extracted view of \"MammoSys: A content-based image retrieval system using breast density patterns\" by J. E. E. D. Oliveira et al.",
        "references": [
            "1be08ae74ca6e82b57ea0e1253a83debb5bf45c6",
            "26b8feb26bd320ed98d7a5d811acc2b376280c5b",
            "b143668d86bb4cb6a607aaa31f86918ee27598bf",
            "b4a70cc8645eb17baa39d23b5c94e1c86ec67d32",
            "fe421481c6995703def460bee45e68e6060d5485",
            "aca8e029d8526ad5033ab4479abc0edde99c0084",
            "a53cfcd7f1b1742945e7b1997541f3edac2bdaae",
            "fb8704210358d0cbf5113c97e1f9f9f03f67e6fc",
            "efd031e2baeb837943997708a30dce737ff06445",
            "4aa6aaeb14e5f881100c97cd5d06306f16ab80d0"
        ],
        "related_topics": [
            "Content-based Image Retrieval Systems",
            "Image Retrieval In Medical Application",
            "Breast Density",
            "Two-dimensional Principal Component Analysis",
            "Dimensionality Reduction"
        ],
        "reference_count": "28",
        "citation_count": "93"
    },
    {
        "Id": "037587b28682886bb02d87a150028ed931f967a4",
        "title": "Texture features for classification of ultrasonic liver images",
        "authors": [
            "Chung-Ming Wu",
            "Yung-Chang Chen",
            "Kai-Sheng Hsieh"
        ],
        "date": "1992",
        "abstract": "A new texture feature set (multiresolution fractal features) based on multiple resolution imagery and the fractional Brownian motion model is proposed to detect diffuse liver diseases quickly and accurately. The classification of ultrasonic liver images is studied, making use of the spatial gray-level dependence matrices, the Fourier power spectrum, the gray-level difference statistics, and the Laws texture energy measures. Features of these types are used to classify three sets of ultrasonic liver images-normal liver, hepatoma, and cirrhosis (30 samples each). The Bayes classifier and the Hotelling trace criterion are employed to evaluate the performance of these features. From the viewpoint of speed and accuracy of classification, it is found that these features do not perform well enough. Hence, a new texture feature set (multiresolution fractal features) based on multiple resolution imagery and the fractional Brownian motion model is proposed to detect diffuse liver diseases quickly and accurately. Fractal dimensions estimated at various resolutions of the image are gathered to form the feature vector. Texture information contained in the proposed feature vector is discussed. A real-time implementation of the algorithm produces about 90% correct classification for the three sets of ultrasonic liver images.",
        "references": [
            "2218c7e9c5d28a7e78bfdd28b77b31d6dbc4c824",
            "5265070d473215e690115e1cd39480d0bda3ca9b",
            "4ebd291ecd93bab330138d5791c0061a44dc769e",
            "45666cef42b1a6423a030cfc0581062a45641434",
            "7a8b47ca09ff2182fa905504dba4a72d0bec4808",
            "4c8accd5d2986e1991ddbc952531ee033e358ff2",
            "1fdb62555eb650662dbe2a6f3985d390861597c2",
            "02fc7d20332217f84f6e2217c0966f1adccac027",
            "9e8e2cd95bd8d879a608f20a133abd92fa988c31",
            "bd988ef4ac18856528bd12bc924bb83dcd5efdb6"
        ],
        "related_topics": [
            "Ultrasonic Liver Images",
            "Gray-level Difference Statistics",
            "Spatial Gray-level Dependence Matrices",
            "Classification",
            "Feature Vector",
            "Fourier Power Spectrum",
            "Multiresolution Fractal Features",
            "Laws Texture Energy Measures",
            "Hotelling Trace Criterion",
            "Texture Feature"
        ],
        "reference_count": "32",
        "citation_count": "524"
    },
    {
        "Id": "fd883740e089e041b1931c1521a597490434c518",
        "title": "Breast Cancer Diagnosis from Perspective of Class Imbalance",
        "authors": [
            "Jue Zhang",
            "Li Chen"
        ],
        "date": "1 May 2019",
        "abstract": "Experimental results showed that the proposed hybrid method based on improved Laplacian score and K-nearest neighbor algorithms worked well with breast cancer datasets and could be a good alternative to the well-known machine learning methods. Introduction: Breast cancer is the second cause of mortality among women. Early detection is the only rescue to reduce the risk of breast cancer mortality. Traditional methods cannot effectively diagnose tumor since they are based on the assumption of well-balanced dataset.. However, a hybrid method can help to alleviate the two-class imbalance problem existing in the diagnosis of breast cancer and establish a more accurate diagnosis. Material and Methods: The proposed hybrid approach was based on improved Laplacian score (LS) andK-nearest neighbor (KNN) algorithms called LS-KNN. An improved LS algorithm was used for obtaining the optimal feature subset. The KNN with automatic K was utilized for classifying the data which guaranteed the effectiveness of the proposed method by reducing the computational effort and making the classification more faster. The effectiveness of LS-KNN was also examined on two biased-representative breast cancer datasets using classification accuracy, sensitivity, specificity, G-mean, and Matthews correlation coefficient. Results: Applying the proposed algorithm on two breast cancer datasets indicated that the efficiency of the new method was higher than the previously introduced methods. The obtained values of accuracy, sensitivity, specificity, G-mean, and Matthews correlation coefficient were 99.27%, 99.12%, 99.51%, 99.42%, respectively. Conclusion: Experimental results showed that the proposed approach worked well with breast cancer datasets and could be a good alternative to the well-known machine learning methods",
        "references": [
            "4d7ef4f116a535750529e2853c181d5d3b678646",
            "fe83150bc326fd62d352cb2993ac91344f195e10",
            "0dfb7b7b58920b6a212caa284243bb1b4b8a1d7c",
            "8d555e26111604297caeaf5bc90b32b9656f7511",
            "c2dbfdb4e55fd15b4e79a332f307bbe1b4db6b43",
            "51598b2f25a5c660877305401a226d7ab65b477c",
            "bd1d0d43e0dbdc191d0acb84acf7eed910e24f81",
            "d4eb78de55ea28540b5692810e6fd7747847ae9a",
            "129adc554e55b70b358674f87230e4ce992ca62e",
            "d543ff291a96584586ad36d1209a8b1b0cafd1f1"
        ],
        "related_topics": [
            "G-mean",
            "Matthews Correlation Coefficient",
            "Tumors",
            "Two-class Imbalance Problems",
            "Laplacian Score",
            "Feature Subsets",
            "Classification Accuracy",
            "Class Imbalance"
        ],
        "reference_count": "0",
        "citation_count": "31"
    }
]