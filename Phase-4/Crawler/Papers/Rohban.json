[
    {
        "Id": "0b5b33b7ea1dc12f3e9252ac1852170a6a6775bf",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
        "title": "Multiresolution Knowledge Distillation for Anomaly Detection",
        "authors": [
            "Mohammadreza Salehi",
            "Niousha Sadjadi",
            "Soroosh Baselizadeh",
            "Mohammad Hossein Rohban",
            "Hamid R. Rabiee"
        ],
        "date": "22 November 2020",
        "abstract": "This work proposes to use the \"distillation\" of features at various layers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle anomaly detection and localization. Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the \"distillation\" of features at various layers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks\u2019 intermediate activation values given an input sample. We show that considering multiple intermediate hints in distillation leads to better exploitation of the expert\u2019s knowledge and a more distinctive discrepancy between the two networks, compared to utilizing only the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework to localize anomalous regions. Despite the striking difference between some test datasets and ImageNet, we achieve competitive or significantly superior results compared to SOTA on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets on both anomaly detection and localization.",
        "references": [
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "2b75ba7f75170b73d913c515cc0deefef6c88f5f",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "d9d7ab13ce305ccee309c989a2341d72b1252070",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d"
        ],
        "related_topics": [
            "Multiresolution Knowledge Distillation",
            "Anomaly Localization",
            "One-class Setting",
            "Uninformed Students",
            "f-AnoGAN",
            "Anomalous Regions",
            "Anomalous Images",
            "Anomaly Detection",
            "ImageNet",
            "Region-based Training"
        ],
        "reference_count": "62",
        "citation_count": "218"
    },
    {
        "Id": "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
        "title": "Data-analysis strategies for image-based cell profiling",
        "authors": [
            "Juan C. Caicedo",
            "Sam Cooper",
            "Florian Heigwer",
            "Scott Warchal",
            "Peng Qiu",
            "Csaba Molnar",
            "Aliaksei S. Vasilevich",
            "Joseph D Barry",
            "Harmanjit Singh Bansal",
            "Oren Z. Kraus",
            "Mathias Wawer",
            "Lassi Paavolainen",
            "Markus D. Herrmann",
            "Mohammad Hossein Rohban",
            "Jane Hung",
            "Holger Hennig",
            "John B. Concannon",
            "Ian Smith",
            "Paul A. Clemons",
            "Shantanu Singh",
            "Paul Rees",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th",
            "Roger G. Linington",
            "Anne E Carpenter"
        ],
        "date": "31 August 2017",
        "abstract": "The steps required to create high-quality image-based (i.e., morphological) profiles from a collection of microscopy images are introduced and techniques that have proven useful in each stage of the data analysis process are recommended on the basis of the experience of 20 laboratories worldwide that are refining their image- based cell-profiling methodologies. Image-based cell profiling is a high-throughput strategy for the quantification of phenotypic differences among a variety of cell populations. It paves the way to studying biological systems on a large scale by using chemical and genetic perturbations. The general workflow for this technology involves image acquisition with high-throughput microscopy systems and subsequent image processing and analysis. Here, we introduce the steps required to create high-quality image-based (i.e., morphological) profiles from a collection of microscopy images. We recommend techniques that have proven useful in each stage of the data analysis process, on the basis of the experience of 20 laboratories worldwide that are refining their image-based cell-profiling methodologies in pursuit of biological discovery. The recommended techniques cover alternatives that may suit various biological goals, experimental designs, and laboratories' preferences.",
        "references": [
            "929a490198770abcb8c123d68a59384879b69adb",
            "29c736eb38861ecf346ce49eedf163c03974566b",
            "36748de338909976f72ffbadaf097470ec040da0",
            "6bfc1c0c7d3fc3cd9ebb9517cc0de73e841b7f74",
            "aa0875ccc516862edc0b6bd2181ee27ce882933d",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "8db554d7e597000f5a0e13712ef0cb3299c05187",
            "bfc8a8724b36cd2b1d068d1f997400e74791a68d",
            "8a7ce466e8f119f033aeace4210cc348ed62520b",
            "669832b732a20ccbdbb81c22393f4bc8f9371dbc"
        ],
        "related_topics": [
            "Image-based Cell Profiling",
            "Image-based Profiling",
            "Morphological Profiling",
            "Cell Painting",
            "Cell Profiling",
            "CellProfiler"
        ],
        "reference_count": "154",
        "citation_count": "483"
    },
    {
        "Id": "0e16a10560b4f7d5f891b932c79d1b2005407acf",
        "title": "Minimax Optimal Sparse Signal Recovery With Poisson Statistics",
        "authors": [
            "Mohammad Hossein Rohban",
            "Venkatesh Saligrama",
            "Delaram Motamed Vaziri"
        ],
        "date": "21 January 2015",
        "abstract": "This work derives a minimax matching lower bound on the mean-squared error of the maximum likelihood decoder and shows that the constrained ML decoder is minimax optimal for this regime. We are motivated by problems that arise in a number of applications such as Online Marketing and Explosives detection, where the observations are usually modeled using Poisson statistics. We model each observation as a Poisson random variable whose mean is a sparse linear superposition of known patterns. Unlike many conventional problems observations here are not identically distributed since they are associated with different sensing modalities. We analyze the performance of a maximum likelihood (ML) decoder, which for our Poisson setting involves a non-linear optimization but yet is computationally tractable. We derive fundamental sample complexity bounds for sparse recovery when the measurements are contaminated with Poisson noise. In contrast to the least-squares linear regression setting with Gaussian noise, we observe that in addition to sparsity, the scale of the parameters also fundamentally impacts l2 error in the Poisson setting. We show that our upper bounds are tight under suitable regularity conditions. Specifically, we derive a minimax matching lower bound on the mean-squared error and show that our constrained ML decoder is minimax optimal for this regime.",
        "references": [
            "3ecba59e99a4cd5f146270debc5ef8d214697c77",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "1ed22f981940b25d3ceac8846153b3521e8cae7b",
            "46dfa751eade5e60ab16b51496a9c764bced2322",
            "e4d56ec1be5603ddb2cd5b9264e365a19dbd5217",
            "8f4f9faa26027f8eae4474d90f6d31c0749acd49",
            "5f56320c5979faeab78dbd9ddb7db755ba4550f3",
            "e716688ddc25dcc8871ef04c7f864063949aa8b9",
            "fe09efb519b26d59c64c715c4efcbe752dc933be",
            "046d550f95db0dae9aa26b34c31cb502b5b72983"
        ],
        "related_topics": [
            "Mean Squared Error",
            "Sparse Recovery",
            "Sparse Signal Recovery",
            "Gaussian Noise"
        ],
        "reference_count": "25",
        "citation_count": "14"
    },
    {
        "Id": "10b219619e88931fabb674037bbb633682775136",
        "title": "ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection",
        "authors": [
            "Mohammadreza Salehi",
            "Atrin Arya",
            "Barbod Pajoum",
            "Mohammad Otoofi",
            "Amirreza Shaeiri",
            "Mohammad Hossein Rohban",
            "Hamid R. Rabiee"
        ],
        "date": "12 March 2020",
        "abstract": "Semantic Scholar extracted view of \"ARAE: Adversarially Robust Training of Autoencoders Improves Novelty Detection\" by Mohammadreza Salehi et al.",
        "references": [
            "70f9968a356d840040a1c9207906f60376dc6bd4",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "599fd051c9438011ec5b581983c89e8922b4a5e6",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "2b75ba7f75170b73d913c515cc0deefef6c88f5f",
            "0314e777333a63aca5735ea136c74e113aa8801d"
        ],
        "related_topics": [
            "Adversarially Regularized Autoencoders",
            "Novelty Detection",
            "Autoencoders",
            "Normal Data",
            "Anomalous Data",
            "Bottleneck Layers",
            "Adversarial Perturbations",
            "Adversarial Robustness",
            "Benchmark Dataset",
            "Adversarially Robust Training"
        ],
        "reference_count": "49",
        "citation_count": "42"
    },
    {
        "Id": "0a78085721f70d82c1284c124c3137bb7c2b34e7",
        "title": "nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer",
        "authors": [
            "R{\\&#x27;e}ka Hollandi",
            "{\\&#x27;A}bel Szkalisity",
            "Timea Toth",
            "Ervin A. Tasn{\\&#x27;a}di",
            "Csaba Molnar",
            "Botond M{\\&#x27;a}th{\\&#x27;e}",
            "Istvan Grexa",
            "J{\\&#x27;o}zsef Moln{\\&#x27;a}r",
            "{\\&#x27;A}rp{\\&#x27;a}d B{\\&#x27;a}lind",
            "Mate Gorbe",
            "M{\\&#x27;a}ria Kov{\\&#x27;a}cs",
            "Ede Migh",
            "Allen Goodman",
            "Tam{\\&#x27;a}s Balassa",
            "Krisztian Koos",
            "Wenyu Wang",
            "Juan C. Caicedo",
            "Norbert Bara",
            "Ferenc Kov{\\&#x27;a}cs",
            "Lassi Paavolainen",
            "Tivadar Danka",
            "Andr{\\&#x27;a}s Kriston",
            "Anne E Carpenter",
            "Kevin Smith",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th"
        ],
        "date": "1 May 2020",
        "abstract": "Semantic Scholar extracted view of \"nucleAIzer: A Parameter-free Deep Learning Framework for Nucleus Segmentation Using Image Style Transfer\" by R\u00e9ka Hollandi et al.",
        "references": [
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "5c5be36e3111e42247d78a6d529e4b1d7d2ced12",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852",
            "268241fd604918a86e1b27e1a880d47e0ecc2d5b",
            "318f82a3e593e391cfd0da7964b16d83299aa943",
            "6ff93aa6ca902002d16fb0c2d3fb48aead92c61e"
        ],
        "related_topics": [
            "nucleAIzer",
            "Image Style Transfer",
            "Deep Learning",
            "Nucleus Segmentation",
            "CellProfiler",
            "Single-cell Segmentation",
            "Parameters",
            "Data Science Bowl"
        ],
        "reference_count": "24",
        "citation_count": "142"
    },
    {
        "Id": "c7c1b005a042e52be8523ddf0d26941a242d5402",
        "title": "ASW-Net: A Deep Learning-based Tool for Cell Nucleus Segmentation of Fluorescence Microscopy",
        "authors": [
            "Weihao Pan",
            "Zhe Liu",
            "Guan Ning Lin"
        ],
        "date": "29 October 2021",
        "abstract": "This paper proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used, and showed that this lightweight model could reach remarkable segmentation performance in the testing set. Nucleus segmentation of fluorescence microscopy is a critical step in quantifying measurements in cell biology. Automatic and accurate nucleus segmentation has powerful applications in analyzing intrinsic characterization in nucleus morphology. However, existing methods have limited capacity to perform accurate segmentation in challenging samples, such as noisy images and clumped nuclei. In this paper, inspired by the idea of cascaded U-Net (or W-Net) and its remarkable performance improvement in medical image segmentation, we proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used. Results showed that this lightweight model could reach remarkable segmentation performance in the testing set (aggregated Jaccard index, 0.7981). In addition, our proposed framework performed better than the state-of-the-art methods in terms of segmentation performance. Moreover, we further explored the effectiveness of our designed network by visualizing the deep features from the network. Notably, our proposed framework is open-source.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "0ea3e5215ac2676a15bef2354b2938704a0789a3",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "29fbe4a6c55f8eae8ff40841440e4cb198cd9aec",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668"
        ],
        "related_topics": [
            "Nucleus Segmentation",
            "Segmentation Performance",
            "Deep Features",
            "Cascaded U-Net",
            "Medical Image Segmentation",
            "Noisy Image",
            "W-Net",
            "Deep Learning",
            "Aggregated Jaccard Index"
        ],
        "reference_count": "0",
        "citation_count": "35"
    },
    {
        "Id": "3a7275c5752c9a1f7cbb439b551c41ec73a7bd83",
        "title": "CrossCT: CNN and Transformer cross\u2013teaching for multimodal image cell segmentation",
        "authors": [
            "Sara Joubbi",
            "Giorgio Ciano",
            "Dario Cardamone",
            "Giuseppe Maccari",
            "Duccio Medini"
        ],
        "date": "2022",
        "abstract": "The main idea behind this work was to improve the organizers\u2019 baseline methods and use both labeled and unlabeled data, and develop CrossCT, a framework based on the cross\u2013teaching between a CNN and a Transformer. Segmenting microscopy images is a crucial step for quantitatively analyzing biological imaging data. Classical tools for biological image segmentation need to be adjusted to the cell type and image conditions to get decent results. Another limitation is the lack of high-quality labeled data to train alternative methods like Deep Learning since manual labeling is costly and time-consuming. Weakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Images 1 was organized by NeurIPS to solve this problem. The aim of the challenge was to develop a versatile method that can work with high variability, with few labeled images, a lot of unlabeled images, and with no human interaction. We developed CrossCT, a framework based on the cross\u2013teaching between a CNN and a Transformer. The main idea behind this work was to improve the organizers\u2019 baseline methods and use both labeled and unlabeled data. Experiments show that our method outperforms the baseline methods based on a supervised learning approach. We achieved an F1 score of 0.5988 for the Transformer and 0.5626 for the CNN respecting the time limits imposed for inference. The code is available on GitHub",
        "references": [
            "a195a2c234ef7a72cc448cb9b58404ff83077f65",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "4cc5ac4a27a4c33722173c8b42aab1009b7bb793",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "40af8e077b162551888ea348d16f458a12c147e5",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "a2aa5a08ad6d5e6432f0468e077e523430cf46a1",
            "c616b0fe93dc0b52b04ce6bf68ae3a67701200e8",
            "e16cc2ce5e961447cb7a1c227764231ab406cef8",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56"
        ],
        "related_topics": [
            "Transformer",
            "Convolutional Neural Network",
            "Cross Teaching",
            "Image Segmentation",
            "Deep Learning",
            "Unlabeled Images",
            "Inference"
        ],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "65389e7ab951a21fd0610236ae84f0cd48941860",
        "title": "Application of convolutional neural networks towards nuclei segmentation in localization-based super-resolution fluorescence microscopy images",
        "authors": [
            "Christopher A. Mela",
            "Yang Liu"
        ],
        "date": "19 January 2021",
        "abstract": "It is found that convolutional neural networks are powerful tools capable of accurately and quickly segmenting localization-based super-resolution microscopy images of nuclei. Automated segmentation of nuclei in microscopic images has been conducted to enhance throughput in pathological diagnostics and biological research. Segmentation accuracy and speed has been significantly enhanced with the advent of convolutional neural networks. A barrier in the broad application of neural networks to nuclei segmentation is the necessity to train the network using a set of application specific images and image labels. Previous works have attempted to create broadly trained networks for universal nuclei segmentation; however, such networks do not work on all imaging modalities, and best results are still commonly found when the network is retrained on user specific data. Stochastic optical reconstruction microscopy (STORM) based super-resolution fluorescence microscopy has opened a new avenue to image nuclear architecture at nanoscale resolutions. Due to the large size and discontinuous features typical of super-resolution images, automatic nuclei segmentation can be difficult. In this study, we apply commonly used networks (Mask R-CNN and UNet architectures) towards the task of segmenting super-resolution images of nuclei. First, we assess whether networks broadly trained on conventional fluorescence microscopy datasets can accurately segment super-resolution images. Then, we compare the resultant segmentations with results obtained using networks trained directly on our super-resolution data. We next attempt to optimize and compare segmentation accuracy using three different neural network architectures. Results indicate that super-resolution images are not broadly compatible with neural networks trained on conventional bright-field or fluorescence microscopy images. When the networks were trained on super-resolution data, however, we attained nuclei segmentation accuracies (F1-Score) in excess of 0.8, comparable to past results found when conducting nuclei segmentation on conventional fluorescence microscopy images. Overall, we achieved the best results utilizing the Mask R-CNN architecture. We found that convolutional neural networks are powerful tools capable of accurately and quickly segmenting localization-based super-resolution microscopy images of nuclei. While broadly trained and widely applicable segmentation algorithms are desirable for quick use with minimal input, optimal results are still found when the network is both trained and tested on visually similar images. We provide a set of Colab notebooks to disseminate the software into the broad scientific community (https://github.com/YangLiuLab/Super-Resolution-Nuclei-Segmentation).",
        "references": [
            "6f5fe81c50cd278de70839d6508eca502336a93f",
            "c1f68f43973e9312bd35420d3c25e79375f3b520",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "20c97e24db99e4667813051dc1fa3e8f5b4d4983",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "d349666e48c1e1aeb4ca703adc839882c7c731b4",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5986547c7380f5a8fb6028093f827b3662f838a2"
        ],
        "related_topics": [
            "Nuclei Segmentation",
            "Stochastic Optical Reconstruction Microscopy",
            "Neural Network"
        ],
        "reference_count": "41",
        "citation_count": "5"
    },
    {
        "Id": "d5ba8477631662a68b50ef30e3585585dd8aaf7d",
        "title": "Cell Segmentation in Multi-modality High-Resolution Microscopy Images with Cellpose",
        "authors": [
            "Kwanyoung Lee",
            "Hyung Jung Byun",
            "Hyunjung Shim"
        ],
        "date": "2022",
        "abstract": "The Cellpose model was trained to competently perform instance segmentation on datasets with various characteristics, and the effect and performance of the existing diameter estimation model was investigated to determine the areas where it performs best, using images of various resolutions. Deep learning has achieved significant improvement in cell segmentation of microscopy images in the field of Biology. However, a lack of generalization has been a major bottleneck of segmentation models since the performance is largely degraded with out-of-distribution data or unseen class data. Developing a generalized segmentation model is challenging due to the diversity of modalities, different staining methods, complicated cell shapes, and extremely high image resolution in microscopy images. The dataset for the \u201cWeakly Supervised Cell Segmentation in Multi-modality High-Resolution Microscopy Images\u201d challenge consists of images with these diverse characteristics. To address these challenges, we trained the Cellpose[1] model to competently perform instance segmentation on datasets with various characteristics. For that, we 1) specified the model to only use green and blue channels for all types of cell images, and 2) investigated the effect and performance of the existing diameter estimation model to determine the areas where it performs best, using images of various resolutions. As a result, we achieved an F1 score of 0.7607 for the validation (Tuning) set.",
        "references": [
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "bd316183cad100b12d717d25f2e55676eaa8f701",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "c7edc5bf1d811e42c0c7de542ca6f395b82b3264",
            "268241fd604918a86e1b27e1a880d47e0ecc2d5b",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "c68fc9bb0d611547d0f7cafc0717e1f02a291766",
            "8c04f169203f9e55056a6f7f956695babe622a38",
            "4f8d648c52edf74e41b0996128aa536e13cc7e82"
        ],
        "related_topics": [
            "Cellpose",
            "Overfitting",
            "Unseen Classes",
            "Cell Segmentation",
            "Deep Learning",
            "Instance Segmentation"
        ],
        "reference_count": "11",
        "citation_count": "One"
    },
    {
        "Id": "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
        "title": "NISNet3D: three-dimensional nuclear synthesis and instance segmentation for fluorescence microscopy images",
        "authors": [
            "Liming Wu",
            "Alain Chen",
            "Paul Salama",
            "Kenneth W. Dunn",
            "Edward J. Delp"
        ],
        "date": "22 October 2022",
        "abstract": "3D Nuclei Instance Segmentation Network (NISNet3D) is described, which provides accurate segmentation of even challenging image volumes using a network trained on large amounts of synthetic nuclei derived from relatively few annotated volumes, or on synthetic data obtained without annotated volumes. The primary step in tissue cytometry is the automated distinction of individual cells (segmentation). Since cell borders are seldom labeled, cells are generally segmented by their nuclei. While tools have been developed for segmenting nuclei in two dimensions, segmentation of nuclei in three-dimensional volumes remains a challenging task. The lack of effective methods for three-dimensional segmentation represents a bottleneck in the realization of the potential of tissue cytometry, particularly as methods of tissue clearing present the opportunity to characterize entire organs. Methods based on deep learning have shown enormous promise, but their implementation is hampered by the need for large amounts of manually annotated training data. In this paper, we describe 3D Nuclei Instance Segmentation Network (NISNet3D) that directly segments 3D volumes through the use of a modified 3D U-Net, 3D marker-controlled watershed transform, and a nuclei instance segmentation system for separating touching nuclei. NISNet3D is unique in that it provides accurate segmentation of even challenging image volumes using a network trained on large amounts of synthetic nuclei derived from relatively few annotated volumes, or on synthetic data obtained without annotated volumes. We present a quantitative comparison of results obtained from NISNet3D with results obtained from a variety of existing nuclei segmentation techniques. We also examine the performance of the methods when no ground truth is available and only synthetic volumes were used for training.",
        "references": [
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "86e92a4b836db293a7a63d81e2a400846088648a",
            "5e1256abd795f7cda3711071512c5cdb01a8f808",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "1ec20454b36b1181062af2587831b49d85253245",
            "c9716a73db358852c2250814a8dc8c3646890028",
            "0ea25f1599fd520c1df29d2bde83f09cb53fb2da",
            "ba1a4bd47b815747badd9aa0bfb57eacfdf8d559",
            "99f3d0831e13040e99f67620bdac56bdcf50f49f",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b"
        ],
        "related_topics": [],
        "reference_count": "77",
        "citation_count": "6"
    },
    {
        "Id": "e33f7928a45e395f1bc2a1aa1dd9349bc725ba24",
        "title": "An Integrative Segmentation Framework for Cell Nucleus of Fluorescence Microscopy",
        "authors": [
            "Weihao Pan",
            "Zhe Liu",
            "Weichen Song",
            "Xu Zhen",
            "Kai Yuan",
            "Fei Xu",
            "Guan Ning Lin"
        ],
        "date": "26 February 2022",
        "abstract": "This paper proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used, and showed that this lightweight model could reach remarkable segmentation performance in the BBBC039 testing set. Nucleus segmentation of fluorescence microscopy is a critical step in quantifying measurements in cell biology. Automatic and accurate nucleus segmentation has powerful applications in analyzing intrinsic characterization in nucleus morphology. However, existing methods have limited capacity to perform accurate segmentation in challenging samples, such as noisy images and clumped nuclei. In this paper, inspired by the idea of cascaded U-Net (or W-Net) and its remarkable performance improvement in medical image segmentation, we proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used. Results showed that this lightweight model could reach remarkable segmentation performance in the BBBC039 testing set (aggregated Jaccard index, 0.90). In addition, our proposed framework performed better than the state-of-the-art methods in terms of segmentation performance. Moreover, we further explored the effectiveness of our designed network by visualizing the deep features from the network. Notably, our proposed framework is open source.",
        "references": [
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "0ea3e5215ac2676a15bef2354b2938704a0789a3",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "4a91c15880a788711c0a7e00ba3968580e3052a5"
        ],
        "related_topics": [],
        "reference_count": "35",
        "citation_count": "3"
    },
    {
        "Id": "3a8150de52d52c3d3ac36a7db695b00949409942",
        "title": "Multi-Modality Microscopy Image Style Transfer for Nuclei Segmentation",
        "authors": [
            "Ye Liu",
            "Sophia J. Wagner",
            "Tingying Peng"
        ],
        "date": "23 November 2021",
        "abstract": "With the authors' style augmentation, the segmentation accuracy of the two top-ranked Mask R-CNN-based nuclei segmentation algorithms in the competition increases significantly and helps counteract class imbalance without resampling of minority classes. Annotating microscopy images for nuclei segmentation is laborious and time-consuming. To leverage the few existing annotations, also across multiple modalities, we propose a novel microscopy-style augmentation technique based on a generative adversarial network (GAN). Unlike other style transfer methods, it can not only deal with different cell assay types and lighting conditions, but also with different imaging modalities, such as bright-field and fluorescence microscopy. Using disentangled representations for content and style, we can preserve the structure of the original image while altering its style during augmentation. We evaluate our data augmentation on the 2018 Data Science Bowl dataset consisting of various cell assays, lighting conditions, and imaging modalities. With our style augmentation, the segmentation accuracy of the two top-ranked Mask R-CNN-based nuclei segmentation algorithms in the competition increases significantly. Thus, our augmentation technique renders the downstream task more robust to the test data heterogeneity and helps counteract class imbalance without resampling of minority classes.",
        "references": [
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "7b83fd62774adc795464ca3ad9b3c238c9ce5daf",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "1a0912bb76777469295bb2c059faee907e7f3258",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852",
            "3d5d9d8e74b215609eabba80ef79a35ebf460e49",
            "bd898f483476e3dcacf83cd85efc64e6319da0e1"
        ],
        "related_topics": [
            "Nuclei Segmentation",
            "Generative Adversarial Networks",
            "Style Augmentation",
            "Style Transfer Method",
            "Minority Classes",
            "Image Style Transfer",
            "Disentangled Representations",
            "Class Imbalance"
        ],
        "reference_count": "0",
        "citation_count": "9"
    },
    {
        "Id": "2fad85659ef4644fa7a41f7479efb963d4f32d2d",
        "title": "A comparative study for nuclei segmentation using latest deep learning optimizers",
        "authors": [
            "Engy N Eltayab",
            "Walid Al-Atabany",
            "Nancy M. Salem",
            "Lamees N. Mahmoud"
        ],
        "date": "21 October 2023",
        "abstract": "In this work, the efficiency of the latest deep learning optimizers, such as Ranger and GC, on Attention U-Net and TransUNet models are assessed and their performance with traditional optimizers are compared. Nuclei segmentation is a critical task in biological image analysis, with numerous applications in cancer diagnosis, grading, staging, and treatment planning. However, this task is challenging, particularly when dealing with low-resolution and low signal-to-noise ratio microscopy images. Segmentation problems arise, such as touching and missing cells, which make the process even more challenging. Deep learning models, including Attention U-Net and TransUNet, have demonstrated exceptional performance in medical image segmentation. Nonetheless, the choice of optimizer can significantly impact the model\u2019s performance, especially in nuclei segmentation. In this work, we assessed the efficiency of the latest deep learning optimizers, such as Ranger and GC, on Attention U-Net and TransUNet models and compared their performance with traditional optimizers, i.e., Adam. We conducted experiments on the publicly available Fluo-N2DH-HeLa dataset that was adapted from the cell tracking challenge. The Ranger optimizer, along with the TransUNet model, achieved the highest mean IOU of 95.05% and reasonable recall and precision scores of 94.4% and 96.8%, respectively. The experimental results proved that integrating the TransUNet model with the Ranger optimizer significantly enhanced touching nuclei segmentation performance in fluorescence microscopy images.",
        "references": [
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "a195a2c234ef7a72cc448cb9b58404ff83077f65",
            "bcf7a61d248a29fb647d232c57e69520a1f715c5",
            "60ebb13073843322f2927edcf69ae214710d6448",
            "24b8a0b02bcb7934967757fc59d273a71ba67e30",
            "ae1c89817a3a239e5344293138bdd80293983460",
            "6c783eb64db6016fe80c0682709f4d24350120c3",
            "82367164f016d25d581527cb141f775e016b9c89",
            "5773cf42590f11c51ca4b36c13d5c2cd2bcd9482",
            "7d78b5bfb749eca31a37340e1d6e55360e877914"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "22"
    },
    {
        "Id": "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
        "title": "A Fast and Accurate Algorithm for Nuclei Instance Segmentation in Microscopy Images",
        "authors": [
            "Zhiming Cheng",
            "Aiping Qu"
        ],
        "date": "2020",
        "abstract": "This paper joint the detection and segmentation simultaneously, and proposes a fast and accurate box-based nuclei instance segmentation method that outperforms prior state-of-the-art methods, not only on accuracy but also on speed. Nuclei instance segmentation within microscopy images is a fundamental task in the pathology work-flow, based on that the meaningful nuclear features can be extracted and multiple biological related analysis can be performed. However, this task is still challenging because of the large variability among different types of nuclei. Although deep learning(DL) based methods have achieved state-of-the-art results in nuclei instance segmentation tasks, these methods are usually focus on improving the accuracy and require support of powerful computing resources. In this paper, we joint the detection and segmentation simultaneously, and propose a fast and accurate box-based nuclei instance segmentation method. Mainly, we employ a fusion module based on the feature pyramid network(FPN) to combine the complementary information of the shallow layers with deep layers for detection the nuclear location by bounding boxes. Subsequently, we crop the feature maps according to the bounding boxes and feed the cropped patches into an U-net architecture as a guide to separate clustered nuclei. The experiments show that the proposed approach outperforms prior state-of-the-art methods, not only on accuracy but also on speed. The source code will be released at: https://github.com/QUAPNH/Nucleiseg.",
        "references": [
            "78079ff363b4b7ecc1d03b88be69e526668432ec",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "bbffe75a1b5c5bc02f3a13bbc4f6700b17686102",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "1b184ae321f37415b14db71ceb1dd585b9f39077",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb",
            "114a42b24290829c16550781ec1a2d6c09fc969e",
            "e312652daf82ed144d1696aae7ab412030d4f7eb"
        ],
        "related_topics": [
            "Nuclei Instance Segmentation",
            "Bounding Boxes",
            "State-of-the-art Results",
            "Feature Maps",
            "Clustered Nuclei",
            "U-net Architecture"
        ],
        "reference_count": "40",
        "citation_count": "14"
    },
    {
        "Id": "8488cbb0edb6e0955efacbb6870e8566eac6b833",
        "title": "Mixed Distillation for Unsupervised Anomaly Detection",
        "authors": [
            "Fuzhen Cai",
            "Siyu Xia"
        ],
        "date": "23 July 2023",
        "abstract": "A skip distillation method is proposed, which allows the deep layers of the student network to learn directly from the shallow of the teacher, avoiding a worse deep fit. Anomaly detection is typically a class of unsupervised learning problems in which the model is trained with only normal samples. Knowledge distillation (KD) has shown promising results in the field of image anomaly detection, especially for texture images. However, the knowledge of the classical KD model is step-by-step transferred from the shallow layers to the deep, which causes the deep layers not to be well-fitted due to an incomplete match of the shallow layers of the student network. For this problem, we propose a skip distillation method, which allows the deep layers of the student network to learn directly from the shallow of the teacher, avoiding a worse deep fit. We also design a symmetric path that allows the shallow layers of the student network to learn directly from the deep of the teacher. These two paths encode su\ufb03cient information for the student network. We have done thorough experiments on the anomaly detection benchmark dataset MvtecAD, and the experimental results show that our model exceeds the current state-of-the-art anomaly detection methods in terms of texture classes.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "5fdd9cfb9b22b9e575d36e98e5d6f52a83729e42",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "95a26eafabf06b1fc5dec6c460a927cf5964e97e",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "9277dc70c74bcadf80dab11c28ead83fd085deec"
        ],
        "related_topics": [
            "Student Network",
            "Anomaly Detection",
            "Image Anomaly Detection",
            "Unsupervised Learning",
            "Knowledge Distillation",
            "UnSupervised Anomaly Detection",
            "Benchmark Dataset",
            "Texture Classes"
        ],
        "reference_count": "0",
        "citation_count": "18"
    },
    {
        "Id": "388ed5032dcc5f5e5b59e4cd4ca5a5d3751a1588",
        "title": "Unsupervised anomaly detection via knowledge distillation with non-directly-coupled student block fusion",
        "authors": [
            "Zhiyuan Feng",
            "Ying Chen",
            "Linbo Xie"
        ],
        "date": "13 September 2023",
        "abstract": "A novel distillation network is proposed for unsupervised anomaly detection, consisting of a complete teacher network and a set of non-directly-coupled student blocks, which independently take features of each layer of teacher network as their input and target to recover the multi-scale representation of the teacher. Recently, knowledge distillation has achieved excellent results in unsupervised anomaly detection. The representation difference of anomalies between teacher and student model is an essential basis for unsupervised anomaly detection. To fully exploit the diversity of anomaly representations, a novel distillation network is proposed for unsupervised anomaly detection, consisting of a complete teacher network and a set of non-directly-coupled student blocks. Instead of taking a complete network as a student which sequentially inherits the distilled knowledge from the previous layer, the student blocks are specifically designed, which independently take features of each layer of teacher network as their input and target to recover the multi-scale representation of the teacher. For each block, an adaptive weighted multi-branch feature extraction strategy is presented to enable the blocks to better focus on key messages from the teacher model. In addition, a feature reunion technique is given during distillation to make multi-scale features more robust to noisy input. The experimental results indicate that the proposed method achieves an outstanding performance on MVTec AD dataset. Compared with the baseline method, the proposed method improves by 2.21% at ROC-AUC of image level and improves by 1.00 and 2.22% for both ROC-AUC and PRO-AUC at pixel level.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "4dd78b8d466b4cfe55a1bbdc694291197ce62541",
            "1c0165247ce1d56a9de7be50ca6c4a49f0db4a82",
            "6517f92d519fc126cc18924231bafd8945a554d1",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "39"
    },
    {
        "Id": "0e8446c00ed21c19f62d71ab208a7b3601671766",
        "title": "A Unified Model for Multi-class Anomaly Detection",
        "authors": [
            "Zhiyuan You",
            "Lei Cui",
            "Yujun Shen",
            "Kai Yang",
            "Xin Lu",
            "Yu Zheng",
            "Xinyi Le"
        ],
        "date": "8 June 2022",
        "abstract": "UniAD is presented, that accomplishes anomaly detection for multiple classes with a unified framework and makes three improvements, including a layer-wise query decoder to help model the multi-class distribution and a feature jittering strategy that urges the model to recover the correct message even with noisy inputs. Despite the rapid advance of unsupervised anomaly detection, existing methods require to train separate models for different objects. In this work, we present UniAD that accomplishes anomaly detection for multiple classes with a unified framework. Under such a challenging setting, popular reconstruction networks may fall into an\"identical shortcut\", where both normal and anomalous samples can be well recovered, and hence fail to spot outliers. To tackle this obstacle, we make three improvements. First, we revisit the formulations of fully-connected layer, convolutional layer, as well as attention layer, and confirm the important role of query embedding (i.e., within attention layer) in preventing the network from learning the shortcut. We therefore come up with a layer-wise query decoder to help model the multi-class distribution. Second, we employ a neighbor masked attention module to further avoid the information leak from the input feature to the reconstructed output feature. Third, we propose a feature jittering strategy that urges the model to recover the correct message even with noisy inputs. We evaluate our algorithm on MVTec-AD and CIFAR-10 datasets, where we surpass the state-of-the-art alternatives by a sufficiently large margin. For example, when learning a unified model for 15 categories in MVTec-AD, we surpass the second competitor on the tasks of both anomaly detection (from 88.1% to 96.5%) and anomaly localization (from 89.5% to 96.8%). Code is available at https://github.com/zhiyuanyou/UniAD.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "4758baad6b22c61682e7f7182bb93723046f36f5",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "2e8d62277e40d465343e8dfb32ecc246f320540e",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "8ee35ed698527d9695c872e3b76715fec4ef69ad",
            "93040f8a5d10e8fde279e18d353aa3dca2873900"
        ],
        "related_topics": [
            "UniAD",
            "DRAEM",
            "Pixel-level AUROC",
            "MVTec AD",
            "Anomaly Detection",
            "Anomaly Localization",
            "Input Features",
            "Fully Connected Layers",
            "Layers",
            "Convolutional Layers"
        ],
        "reference_count": "71",
        "citation_count": "56"
    },
    {
        "Id": "787c063a11df7facc43d5be3a49343746f2d27ef",
        "title": "Adapting Generic Features to A Specific Task: A Large Discrepancy Knowledge Distillation for Image Anomaly Detection",
        "authors": [
            "Chenkai Zhang",
            "Tianqi Du",
            "Yueming Wang"
        ],
        "date": "2023",
        "abstract": "A novel angular margin loss is introduced to improve the regular training loss of knowledge distillation and ensure larger discrepancies between T-S models on anomalies and help state-of-the-art KD-based methods achieve better detection performance. Anomaly detection is a challenging task due to the lack of data on unexpected anomalies. Recent approaches using Knowledge Distillation (KD) between Teacher-Student (T-S) models have shown great potential for anomaly detection. These techniques use pre-trained models on natural images as the teacher model. However, for industrial images, defects typically occur in a small region, while the global semantics of the anomaly image remain similar to normal images. This situation results in generic features being unable to capture defects well, leading to a loss of discriminability in detecting anomalies. This paper proposes a way to improve this situation by applying learnable feature mappings to adapt the generic features for the data-specific task. Additionally, a novel angular margin loss is introduced to improve the regular training loss of knowledge distillation and ensure larger discrepancies between T-S models on anomalies. Extensive experiments show that the proposed feature mappings and angular loss can effectively improve the feature discriminability for anomaly detection and help state-of-the-art KD-based methods achieve better detection performance.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "8ee35ed698527d9695c872e3b76715fec4ef69ad",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "7b7087b7452adc2fe8a874678049f591c1342c0f",
            "38ca689c2f916c648ea3ecb1043facbc4bea0d4f"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "26"
    },
    {
        "Id": "3b3aefbbdb64e5812f133f220b3f129a36a30065",
        "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
        "authors": [
            "Hanqiu Deng",
            "Xingyu Li"
        ],
        "date": "26 January 2022",
        "abstract": "A novel T-S model consisting of a teacher encoder and a student decoder is proposed and a simple yet effective \u201creverse distillation\u201d paradigm is introduced, demonstrating the proposed approach's effectiveness and generalizability. Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective \u201creverse distillation\u201d paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model's one-class embedding as input and targets to restore the teacher's multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but aban-dons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach's effectiveness and generalizability.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "599fd051c9438011ec5b581983c89e8922b4a5e6",
            "30895c61bb836f2cae7ef5ba6516886f746a7153",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [
            "Reverse Distillation",
            "Student Decoder",
            "One-Class Embedding",
            "CutPaste",
            "Multiresolution Knowledge Distillation",
            "Large Defects",
            "Anomaly-free Samples",
            "Pseudo Anomalies",
            "WideResnet-50",
            "Localize Anomaly"
        ],
        "reference_count": "52",
        "citation_count": "161"
    },
    {
        "Id": "cec282840ed7992af45400472fa545c94a6e3f7d",
        "title": "ADPS: Asymmetric Distillation Post-Segmentation for Image Anomaly Detection",
        "authors": [
            "Peng Xing",
            "Hao Tang",
            "Jinhui Tang",
            "Zechao Li"
        ],
        "date": "19 October 2022",
        "abstract": "This work proposes an innovative approach called Asymmetric Distillation Post-Segmentation (ADPS), which employs an asymmetric distillation paradigm that takes distinct forms of the same image as the input of the teacher-student networks, driving the student network to learn discriminating representations for anomalous regions. Knowledge Distillation-based Anomaly Detection (KDAD) methods rely on the teacher-student paradigm to detect and segment anomalous regions by contrasting the unique features extracted by both networks. However, existing KDAD methods suffer from two main limitations: 1) the student network can effortlessly replicate the teacher network's representations, and 2) the features of the teacher network serve solely as a ``reference standard\"and are not fully leveraged. Toward this end, we depart from the established paradigm and instead propose an innovative approach called Asymmetric Distillation Post-Segmentation (ADPS). Our ADPS employs an asymmetric distillation paradigm that takes distinct forms of the same image as the input of the teacher-student networks, driving the student network to learn discriminating representations for anomalous regions. Meanwhile, a customized Weight Mask Block (WMB) is proposed to generate a coarse anomaly localization mask that transfers the distilled knowledge acquired from the asymmetric paradigm to the teacher network. Equipped with WMB, the proposed Post-Segmentation Module (PSM) is able to effectively detect and segment abnormal regions with fine structures and clear boundaries. Experimental results demonstrate that the proposed ADPS outperforms the state-of-the-art methods in detecting and segmenting anomalies. Surprisingly, ADPS significantly improves Average Precision (AP) metric by 9% and 20% on the MVTec AD and KolektorSDD2 datasets, respectively.",
        "references": [
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "93040f8a5d10e8fde279e18d353aa3dca2873900"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "66"
    },
    {
        "Id": "2013f18832e0fb11c7aaf1f4f8da453aebda488c",
        "title": "Pull & Push: Leveraging Differential Knowledge Distillation for Efficient Unsupervised Anomaly Detection and Localization",
        "authors": [
            "Qihang Zhou",
            "Shibo He",
            "Haoyu Liu",
            "Tao Chen",
            "Jiming Chen"
        ],
        "date": "1 May 2023",
        "abstract": "This work designs an efficient teacher-student model for anomaly detection and localization, which maximizes pixel-wise discrepancies for anomalous regions approximated by data augmentation and simultaneously minimizes discrepancies for pixel- wise normal regions between these two networks. Recently, much attention has been paid to segmenting subtle unknown defect regions by knowledge distillation in an unsupervised setting. Most previous studies concentrated on guiding the student network to learn the same representations on the normality, neglecting the different behaviors of the abnormality. This leads to a high probability of false detection of subtle defects. To address such an issue, we propose to push representations on abnormal areas of the teacher and student network as far as possible while pulling representations on normal areas as close as possible. Based on this idea, we design an efficient teacher-student model for anomaly detection and localization, which maximizes pixel-wise discrepancies for anomalous regions approximated by data augmentation and simultaneously minimizes discrepancies for pixel-wise normal regions between these two networks. The explicit differential knowledge distillation enlarges the margin between normal representations and abnormal ones in favour of discriminating them. Then, the appropriate small student network is not only efficient, but more importantly, helps inhibit the generalization ability of anomalous patterns when learning normal patterns, facilitating the precise decision boundary. The experimental results on the MVTec AD, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our proposed method achieves better performance than current state-of-the-art (SOTA) approaches. Especially, For the MVTec AD dataset with high resolution images, we achieve 98.1 AUROC% and 93.6 AUPRO% in anomaly localization, outperforming knowledge distillation based SOTA methods by 1.1 AUROC% and 1.5 AUPRO% with a lightweight model.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "4dd78b8d466b4cfe55a1bbdc694291197ce62541",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "7d90243c5a46430a36c5ba88627b5d254450a1e1",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "23ad8fc48530ce366f8192dfb48d0f7df1dba277"
        ],
        "related_topics": [
            "Student Network",
            "Knowledge Distillation",
            "MVTec AD",
            "Self-organizing Tree Algorithm",
            "Area Under The Receiver Operating Characteristic Curve",
            "Margin",
            "Student Model",
            "CIFAR-10",
            "UnSupervised Anomaly Detection",
            "Anomaly Detection"
        ],
        "reference_count": "64",
        "citation_count": "8"
    },
    {
        "Id": "25e72f27ebf69da3171f99724539c1b88f8837f9",
        "title": "Autoencoder-Like Knowledge Distillation Network for Anomaly Detection",
        "authors": [
            "Caie Xu",
            "Bingyan Wang",
            "Dandan Ni",
            "Jin Gan",
            "Mingyang Wu",
            "Wujie Zhou"
        ],
        "date": "2023",
        "abstract": "A novel autoencoder-like KD model based on the attention mechanism for anomaly detection that attains the state-of-the-art (SOTA) performance in anomaly detection on the public dataset MVTec and exhibits superior performance compared to other existing anomaly detection models on specific datasets. Anomaly detection is a crucial research field in computer vision with diverse applications in practical scenarios. The common anomaly detection methods employed currently consist of autoencoders, generative adversarial networks, and knowledge distillation (KD) models. However, the teacher and student models in KD might not always yield distinct representations to signify anomalies due to their similar model structure and data flow. This study proposes a novel autoencoder-like KD model based on the attention mechanism for anomaly detection. The pre-trained teacher model incorporates a dual attention module as the encoder, while the student model integrates the same dual attention module as the decoder. The teacher guides the student to learn the feature knowledge of the input image. To connect the teacher-student model, a BottleNeck module is employed, converting the features extracted from the teacher model into more compact latent codes for precise restoration by the student model, thereby achieving anomaly detection. In general, the proposed model exhibits superior performance compared to other existing anomaly detection models on specific datasets. Experimental results demonstrate that the proposed model attains the state-of-the-art (SOTA) performance in anomaly detection on the public dataset MVTec. It achieves an average AUC of 98.2% and 98.0% at sample and pixel levels, respectively.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "4d8abae45a5492ed2399fd5e25eeade8ac0bfa0f",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "98fa8f7b28f43830a22612be53bb393cf421bbc1",
            "f37bc75aa1833e1330c39c4f04b131baca08d67b"
        ],
        "related_topics": [],
        "reference_count": "37",
        "citation_count": "One"
    },
    {
        "Id": "61840de4d9610558d510cfcf32986e93511a4cef",
        "title": "Asymmetric Distillation Post-Segmentation Method for Image Anomaly Detection",
        "authors": [
            "Peng-Fei Xing",
            "Zechao Li"
        ],
        "date": "2022",
        "abstract": "A novel Asymmetric Distillation Post-Segmentation (ADPS) method to effectively explore the asymmetric structure of the input and the discriminative features of the teacher network is proposed and achieves state-of-the-art anomaly segmentation results. Knowledge distillation-based anomaly detection methods generate same outputs for unknown classes due to the symmetric form of the input and ignore the powerful semantic information of the output of the teacher network since it is only used as a \u201creference standard\u201d. Towards this end, this work proposes a novel Asymmetric Distillation Post-Segmentation (ADPS) method to effectively explore the asymmetric structure of the input and the discriminative features of the teacher network. Speci\ufb01cally, a simple yet effective asymmetric input approach is proposed to make different data \ufb02ows through the teacher and student networks. The student network enables to have different inductive and expressive abilities, which can generate different outputs in anomalous regions. Besides, to further explore the semantic information of the teacher network and obtain effective discriminative boundaries, the Weight Mask Block (WMB) and the post-segmentation module are proposede. WMB leverages a weighted strategy by exploring teacher-student feature maps to highlight anomalous features. The post-segmentation module further learns the anomalous features and obtains valid discriminative boundaries. Experimental results on three benchmark datasets demonstrate that the proposed ADPS achieves state-of-the-art anomaly segmentation results.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "2b32b46f346d9b13268f0e74e5242a10a712a352",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "6517f92d519fc126cc18924231bafd8945a554d1",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f"
        ],
        "related_topics": [
            "Teacher Network",
            "Student Network",
            "Semantic Information",
            "Weighted Mini-bucket",
            "Unknown Classes",
            "Image Anomaly Detection",
            "Anomalous Regions",
            "Knowledge Distillation",
            "Anomaly Detection",
            "Benchmark Dataset"
        ],
        "reference_count": "0",
        "citation_count": "52"
    },
    {
        "Id": "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
        "title": "Explainable Deep Few-shot Anomaly Detection with Deviation Networks",
        "authors": [
            "Guansong Pang",
            "Choubo Ding",
            "Chunhua Shen",
            "Anton van den Hengel"
        ],
        "date": "1 August 2021",
        "abstract": "This work introduces a novel weakly-supervised anomaly detection framework to train detection models without assuming the examples illustrating all possible classes of anomaly, and learns discriminative normality by leveraging the labeled anomalies and a prior probability to enforce expressive representations of normality and unbounded deviated representations of abnormality. Existing anomaly detection paradigms overwhelmingly focus on training detection models using exclusively normal data or unlabeled data (mostly normal samples). One notorious issue with these approaches is that they are weak in discriminating anomalies from normal samples due to the lack of the knowledge about the anomalies. Here, we study the problem of few-shot anomaly detection, in which we aim at using a few labeled anomaly examples to train sample-efficient discriminative detection models. To address this problem, we introduce a novel weakly-supervised anomaly detection framework to train detection models without assuming the examples illustrating all possible classes of anomaly. Specifically, the proposed approach learns discriminative normality (regularity) by leveraging the labeled anomalies and a prior probability to enforce expressive representations of normality and unbounded deviated representations of abnormality. This is achieved by an end-to-end optimization of anomaly scores with a neural deviation learning, in which the anomaly scores of normal samples are imposed to approximate scalar scores drawn from the prior while that of anomaly examples is enforced to have statistically significant deviations from these sampled scores in the upper tail. Furthermore, our model is optimized to learn fine-grained normality and abnormality by top-K multiple-instance-learning-based feature subspace deviation learning, allowing more generalized representations. Comprehensive experiments on nine real-world image anomaly detection benchmarks show that our model is substantially more sample-efficient and robust, and performs significantly better than state-of-the-art competing methods in both closed-set and open-set settings. Our model can also offer explanation capability as a result of its prior-driven anomaly score learning. Code and datasets are available at: https://git.io/DevNet.",
        "references": [
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "8ed98bd58c799718d6fd389e2218bb89b1ecb9d7",
            "30aa23a6a32312666f2609339582744203024993",
            "f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "c48def9076e58095c4aea49a8daa931af1990701",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9",
            "182d11020bf2842f135f1ec1dcac20237e0dc8b7",
            "5f61089d3d548a515f01b473f0119137d1f340d4"
        ],
        "related_topics": [
            "Few Shot Anomaly Detection",
            "Deviation Networks",
            "Fractional Lower Order Statistics",
            "Deviation Loss",
            "DevNet",
            "Anomaly Contamination",
            "Unseen Anomalies",
            "Anomaly Score",
            "Normal Samples",
            "Explainable"
        ],
        "reference_count": "76",
        "citation_count": "43"
    },
    {
        "Id": "4d0506d3e7f5dd23e1620cf8c48650bf1552bd3e",
        "title": "Image-based cell profiling enhancement via data cleaning methods",
        "authors": [
            "Arghavan Rezvani",
            "Mahtab Bigverdi",
            "Mohammad Hossein Rohban"
        ],
        "date": "4 May 2022",
        "abstract": "The experiments indicate that by performing these time-efficient preprocessing steps, image-based profiles can preserve more meaningful information compared to raw profiles, and suggest possible avenues for future research. With the advent of high-throughput assays, a large number of biological experiments can be carried out. Image-based assays are among the most accessible and inexpensive technologies for this purpose. Indeed, these assays have proved to be effective in characterizing unknown functions of genes and small molecules. Image analysis pipelines have a pivotal role in translating raw images that are captured in such assays into useful and compact representation, also known as measurements. CellProfiler is a popular and commonly used tool for this purpose through providing readily available modules for the cell/nuclei segmentation, and making various measurements, or features, for each cell/nuclei. Single cell features are then aggregated for each treatment replica to form treatment \u201cprofiles\u201d. However, there may be several sources of error in the CellProfiler quantification pipeline that affects the downstream analysis that is performed on the profiles. In this work, we examined various preprocessing approaches to improve the profiles. We consider the identification of drug mechanisms of action as the downstream task to evaluate such preprocessing approaches. Our enhancement steps mainly consist of data cleaning, cell level outlier detection, toxic drug detection, and regressing out the cell area from all other features, as many of them are widely affected by the cell area. Our experiments indicate that by performing these time-efficient preprocessing steps, image-based profiles can preserve more meaningful information compared to raw profiles. In the end, we also suggest possible avenues for future research.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "929a490198770abcb8c123d68a59384879b69adb",
            "05ba3c532b7b98c9448da0bee52d7cba0ef29621",
            "c4f82fe429d50064c371eb0ea97caa37b4a879cf",
            "8b73514fd708f6bdd3786d47a8673a97fdce4dfe",
            "c6bf43a2bfffec64989baa2d193a6a845defa59e",
            "f638edf9370a9a07681d80f7c6366e9783848a25",
            "796f794f3aa5366e8f525a0d2c814fa19c917db4",
            "8a2b9eac4e36e5f62a59f118c7bfc1ed9763b59c",
            "559c6ae809700119458006edb129ba966b147bda"
        ],
        "related_topics": [],
        "reference_count": "31",
        "citation_count": "4"
    },
    {
        "Id": "c3f1670b4d61a78c1e142da313ca12b09cca7b34",
        "title": "Reproducible image-based profiling with Pycytominer",
        "authors": [
            "Erik Serrano",
            "Srinivas Niranj Chandrasekaran",
            "Dave Bunten",
            "Kenneth I. Brewer",
            "Jenna Tomkinson",
            "Roshan Kern",
            "Michael Bornholdt",
            "Stephen Fleming",
            "Ruifan Pei",
            "John Arevalo",
            "Hillary Tsang",
            "Vincent Rubinetti",
            "Callum Tromans-Coia",
            "Tim Becker",
            "Erin Weisbart",
            "Charlotte Bunne",
            "Alexandr A. Kalinin",
            "Rebecca Senft",
            "Stephen J. Taylor",
            "Nasim Jamali",
            "Adeniyi Adeboye",
            "Hamdah Shafqat Abbasi",
            "Allen Goodman",
            "Juan C. Caicedo",
            "Anne E Carpenter",
            "Beth A. Cimini",
            "Shantanu Singh",
            "Gregory P. Way"
        ],
        "date": "22 November 2023",
        "abstract": "Pycytominer is presented, an open-source software package with a vibrant community that establishes an image-based profiling standard that ensures consistent data processing pipelines with data provenance, therefore minimizing potential inconsistencies and enabling researchers to confidently derive accurate conclusions and discover novel insights from their data, thus driving progress in the field. Technological advances in high-throughput microscopy have facilitated the acquisition of cell images at a rapid pace, and data pipelines can now extract and process thousands of image-based features from microscopy images. These features represent valuable single-cell phenotypes that contain information about cell state and biological processes. The use of these features for biological discovery is known as image-based or morphological profiling. However, these raw features need processing before use and image-based profiling lacks scalable and reproducible open-source software. Inconsistent processing across studies makes it difficult to compare datasets and processing steps, further delaying the development of optimal pipelines, methods, and analyses. To address these issues, we present Pycytominer, an open-source software package with a vibrant community that establishes an image-based profiling standard. Pycytominer has a simple, user-friendly Application Programming Interface (API) that implements image-based profiling functions for processing high-dimensional morphological features extracted from microscopy images of cells. Establishing Pycytominer as a standard image-based profiling toolkit ensures consistent data processing pipelines with data provenance, therefore minimizing potential inconsistencies and enabling researchers to confidently derive accurate conclusions and discover novel insights from their data, thus driving progress in our field.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "c6bb1291dc4d69d8362d05737dcd97020832c8d4",
            "36748de338909976f72ffbadaf097470ec040da0",
            "29c736eb38861ecf346ce49eedf163c03974566b",
            "0a87ac278987bbf3667fcc1ecefd378234c71628",
            "0e9789615e4f3a55d300b8ca46070efab8a93513",
            "46276273cf92c31e103feab0cdc2ad0adda0dcc9",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "54"
    },
    {
        "Id": "069c15e8891c9260d9545d0f82b8a08145044482",
        "title": "Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays.",
        "authors": [
            "Kevin Smith",
            "Filippo Piccinini",
            "Tam{\\&#x27;a}s Balassa",
            "Krisztian Koos",
            "Tivadar Danka",
            "Hossein Azizpour",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th"
        ],
        "date": "1 June 2018",
        "abstract": "Semantic Scholar extracted view of \"Phenotypic Image Analysis Software Tools for Exploring and Understanding Big Image Data from Cell-Based Assays.\" by Kevin Smith et al.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "be2bd17021d2c3bc28bbaddead0ff40aff12f206",
            "614b6456b0aeb024fee659b094bdd999cb7d021e",
            "b90d44f59fcb74c71d3e31f67a3f09efab187a4e",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "669832b732a20ccbdbb81c22393f4bc8f9371dbc",
            "c1317f812bfaec25958ec7b5b583eb035fe6d5a1",
            "c6bb1291dc4d69d8362d05737dcd97020832c8d4",
            "bfc8a8724b36cd2b1d068d1f997400e74791a68d",
            "357f4b9e31e9cf386d62249651d60b6899640671"
        ],
        "related_topics": [],
        "reference_count": "99",
        "citation_count": "69"
    },
    {
        "Id": "c088b24fb15581d5986e47d7007f961cd0847a5e",
        "title": "Assessing the performance of the Cell Painting assay across different imaging systems",
        "authors": [
            "Nasim Jamali",
            "Callum Tromans-Coia",
            "Hamdah Shafqat Abbasi",
            "Kenneth A. Giuliano",
            "Mai Hagimoto",
            "Kevin Jan",
            "Erika Kaneko",
            "Stefan Letzsch",
            "Alexander Schreiner",
            "Jonathan Z. Sexton",
            "Mahomi Suzuki",
            "O. Joseph Trask",
            "Mitsunari Yamaguchi",
            "Fumiki Yanagawa",
            "Michael Yang",
            "Anne E Carpenter",
            "Beth A. Cimini"
        ],
        "date": "15 February 2023",
        "abstract": "This work examines the performance of the Cell Painting assay across multiple high-throughput microscope systems and determines independently for each microscope system the best performing settings, providing those who wish to adopt this assay an ideal starting point for their own assays. Quantitative microscopy is a powerful method for performing phenotypic screens from which image-based profiling can extract a wealth of information, termed profiles. These profiles can be used to elucidate the changes in cellular phenotypes across cell populations from different patient samples or following genetic or chemical perturbations. One such image-based profiling method is the Cell Painting assay, which provides morphological insight through the imaging of eight cellular compartments. Here, we examine the performance of the Cell Painting assay across multiple high-throughput microscope systems and find that all are compatible with this assay. Furthermore, we determine independently for each microscope system the best performing settings, providing those who wish to adopt this assay an ideal starting point for their own assays. We also explore the impact of microscopy setting changes in the Cell Painting assay and find that few dramatically reduce the quality of a Cell Painting profile, regardless of the microscope used.",
        "references": [
            "36748de338909976f72ffbadaf097470ec040da0",
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "faf891f3f9bd76dcdd348c0ce8c389782cd49c90",
            "6c26c1c68782bb2b4821f9f2343431d3785b3b59",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "46276273cf92c31e103feab0cdc2ad0adda0dcc9",
            "590fc45e167068cac9a8a98eee95725531a5a352",
            "3e173f19037739ae6e84bc3b9f97957c42412660",
            "c6bf43a2bfffec64989baa2d193a6a845defa59e"
        ],
        "related_topics": [],
        "reference_count": "18",
        "citation_count": "4"
    },
    {
        "Id": "0591de4af9853d4fd6225e73568b669d69c5f8a8",
        "title": "Data cleaning for image-based profiling enhancement",
        "authors": [
            "Arghavan Rezvani",
            "Mahtab Bigverdi",
            "Mohammad Hossein Rohban"
        ],
        "date": "10 September 2021",
        "abstract": "Various preprocessing approaches to improve the profiles are examined, including unsupervised and weakly-supervised deep learning based methods to reduce the feature dimensionality, and possible avenues for future research are suggested. With the advent of high-throughput assays, a large number of biological experiments can be carried out. Image-based assays are among the most accessible and inexpensive technologies for this purpose. Indeed, these assays have proved to be effective in characterizing unknown functions of genes and small molecules. Image analysis pipelines have a pivotal role in translating raw images that are captured in such assays into useful and compact representation, also known as measurements. CellProfiler is a popular and commonly used tool for this purpose through providing readily available modules for the cell/nuclei segmentation, and making various measurements, or features, for each cell/nuclei. Single cell features are then aggregated for each treatment replica to form treatment \u201cprofiles.\u201d However, there may be several sources of error in the CellProfiler quantification pipeline that affects the downstream analysis that is performed on the profiles. In this work, we examined various preprocessing approaches to improve the profiles. We consider identification of drug mechanisms of action as the downstream task to evaluate such preprocessing approaches. Our enhancement steps mainly consist of data cleaning, cell level outlier detection, toxic drug detection, and regressing out the cell area from all other features, as many of them are widely affected by the cell area. We also examined unsupervised and weakly-supervised deep learning based methods to reduce the feature dimensionality, and finally suggest possible avenues for future research.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "05ba3c532b7b98c9448da0bee52d7cba0ef29621",
            "929a490198770abcb8c123d68a59384879b69adb",
            "c4f82fe429d50064c371eb0ea97caa37b4a879cf",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "c6bf43a2bfffec64989baa2d193a6a845defa59e",
            "f638edf9370a9a07681d80f7c6366e9783848a25",
            "559c6ae809700119458006edb129ba966b147bda",
            "a8db789522b9375396bd91de631342740ba19a12",
            "c0391c90b06ff7a9be01866ff5e7e5cb08566b4c"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "22"
    },
    {
        "Id": "53ae9442446b22a0b0279a988aa445708edeaee5",
        "title": "Assessing the performance of the Cell Painting assay across different imaging systems",
        "authors": [
            "Callum Tromans-Coia",
            "Nasim Jamali",
            "Hamdah Shafqat Abbasi",
            "Kenneth A. Giuliano",
            "Mai Hagimoto",
            "Kevin Jan",
            "Erika Kaneko",
            "Stefan Letzsch",
            "Alexander Schreiner",
            "Jonathan Z. Sexton",
            "Mahomi Suzuki",
            "O. Joseph Trask",
            "Mitsunari Yamaguchi",
            "Fumiki Yanagawa",
            "Michael Yang",
            "Anne E. Carpenter",
            "Beth A. Cimini"
        ],
        "date": "3 October 2023",
        "abstract": "This work examines the performance of the Cell Painting assay across multiple high\u2010throughput microscope systems and determines independently for each microscope system the best performing settings, providing those who wish to adopt this assay an ideal starting point for their own assays. Quantitative microscopy is a powerful method for performing phenotypic screens from which image\u2010based profiling can extract a wealth of information, termed profiles. These profiles can be used to elucidate the changes in cellular phenotypes across cell populations from different patient samples or following genetic or chemical perturbations. One such image\u2010based profiling method is the Cell Painting assay, which provides morphological insight through the imaging of eight cellular compartments. Here, we examine the performance of the Cell Painting assay across multiple high\u2010throughput microscope systems and find that all are compatible with this assay. Furthermore, we determine independently for each microscope system the best performing settings, providing those who wish to adopt this assay an ideal starting point for their own assays. We also explore the impact of microscopy setting changes in the Cell Painting assay and find that few dramatically reduce the quality of a Cell Painting profile, regardless of the microscope used.",
        "references": [
            "36748de338909976f72ffbadaf097470ec040da0",
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "faf891f3f9bd76dcdd348c0ce8c389782cd49c90",
            "6c26c1c68782bb2b4821f9f2343431d3785b3b59",
            "46276273cf92c31e103feab0cdc2ad0adda0dcc9",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "590fc45e167068cac9a8a98eee95725531a5a352",
            "3e173f19037739ae6e84bc3b9f97957c42412660",
            "c6bf43a2bfffec64989baa2d193a6a845defa59e"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "19"
    },
    {
        "Id": "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
        "title": "Optimizing the Cell Painting assay for image-based profiling",
        "authors": [
            "Beth A. Cimini",
            "Srinivas Niranj Chandrasekaran",
            "Maria Kost-Alimova",
            "Lisa Miller",
            "Amy Goodale",
            "Briana Fritchman",
            "Patrick Byrne",
            "Sakshi Garg",
            "Nasim Jamali",
            "David J. Logan",
            "John B. Concannon",
            "Charles-Hugues Lardeau",
            "Elizabeth Mouchet",
            "Shantanu Singh",
            "Hamdah Shafqat Abbasi",
            "Peter Aspesi",
            "Justin D. Boyd",
            "Tamara Gilbert",
            "David Gnutt",
            "Santosh Hariharan",
            "Desiree Hernandez",
            "Gisela Hormel",
            "Karolina Juhani",
            "Michelle Melanson",
            "Lewis H. Mervin",
            "Tiziana Monteverde",
            "James E Pilling",
            "Adam Skepner",
            "Susanne E. Swalley",
            "Anita Vrcic",
            "Erin Weisbart",
            "Guy B. Williams",
            "Shan Yu",
            "Bolek Zapiec",
            "Anne E Carpenter"
        ],
        "date": "1 December 2022",
        "abstract": "An updated protocol for the most popular assay for image-based profiling, Cell Painting, aiming to improve upon the assay via quantitative optimization, based on the measured ability of the assay to detect morphological phenotypes and group similar perturbations together is described. In image-based profiling, software extracts thousands of morphological features of cells from multi-channel fluorescence microscopy images, yielding single-cell profiles that can be used for basic research and drug discovery. Powerful applications have been proven, including clustering chemical and genetic perturbations based on their similar morphological impact, identifying disease phenotypes by observing differences in profiles between healthy and diseased cells, and predicting assay outcomes using machine learning, among many others. Here we provide an updated protocol for the most popular assay for image-based profiling, Cell Painting. Introduced in 2013, it uses six stains imaged in five channels and labels eight diverse components of the cell: DNA, cytoplasmic RNA, nucleoli, actin, Golgi apparatus, plasma membrane, endoplasmic reticulum, and mitochondria. The original protocol was updated in 2016 based on several years\u2019 experience running it at two sites, after optimizing it by visual stain quality. Here we describe the work of the Joint Undertaking for Morphological Profiling (JUMP) Cell Painting Consortium, aiming to improve upon the assay via quantitative optimization, based on the measured ability of the assay to detect morphological phenotypes and group similar perturbations together. We find that the assay gives very robust outputs despite a variety of changes to the protocol and that two vendors\u2019 dyes work equivalently well. We present Cell Painting version 3, in which some steps are simplified and several stain concentrations can be reduced, saving costs. Cell culture and image acquisition take 1\u20132 weeks for a typically sized batch of 20 or fewer plates; feature extraction and data analysis take an additional 1\u20132 weeks. Key references using this protocol Virtual screening for small-molecule pathway regulators by image-profile matching (https://doi.org/10.1016/j.cels.2022.08.003) - recent work examining the ability to use collected Cell Painting profiles to screen for regulators of a number of diverse biological pathways. JUMP Cell Painting dataset: images and profiles from two billion cells perturbed by 140,000 chemical and genetic perturbations (DOI) - the description of the main JUMP master public data set, using this protocol in the production of >200 TB of image data and >200 TB of measured profiles. Key data used in this protocol Cell Painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes (https://doi.org/10.1038/nprot.2016.105) - this paper provides the first step-by-step Cell Painting protocol ever released.",
        "references": [
            "36748de338909976f72ffbadaf097470ec040da0",
            "c088b24fb15581d5986e47d7007f961cd0847a5e",
            "1f96460299f3c74965b4fe8e64c28957ada06c74",
            "590fc45e167068cac9a8a98eee95725531a5a352",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "8f8a0de72c285f10bd3af311899026193eda632e",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "3e173f19037739ae6e84bc3b9f97957c42412660",
            "c5dd04fdd9e9e55cc172e4e1316b58ff1fb7aa45"
        ],
        "related_topics": [],
        "reference_count": "104",
        "citation_count": "25"
    },
    {
        "Id": "d600ed4d55313022f91294b2ae64d801580a84d0",
        "title": "Evaluating batch correction methods for image-based cell profiling",
        "authors": [
            "John Arevalo",
            "Robert van Dijk",
            "Anne E Carpenter",
            "Shantanu Singh"
        ],
        "date": "17 September 2023",
        "abstract": "This work evaluated seven top-ranked batch correction strategies for mRNA profiles in the context of a newly released Cell Painting dataset, the largest publicly accessible image-based dataset, and found that Harmony, a nonlinear method, consistently outperformed the other tested methods. High-throughput image-based profiling platforms are powerful technologies capable of collecting data from billions of cells exposed to thousands perturbations in a time- and cost-effective manner. Therefore, image-based profiling data has been increasingly used for diverse biological applications, such as predicting drug mechanism of action or gene function. However, batch effects pose severe limitations to community-wide efforts to integrate and interpret image-based profiling data collected across different laboratories and equipment. To address this problem, we evaluated seven top-ranked batch correction strategies for mRNA profiles in the context of a newly released Cell Painting dataset, the largest publicly accessible image-based dataset. We focused on five different use scenarios with varying complexity, and found that Harmony, a nonlinear method, consistently outperformed the other tested methods. Furthermore, we provide a framework, benchmark, and metrics for the future assessment of new batch correction methods. Overall, this work paves the way for improvements that allow the community to make best use of public Cell Painting data for scientific discovery.",
        "references": [
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "6c26c1c68782bb2b4821f9f2343431d3785b3b59",
            "132bac2efedf0cec6fbe4189070a0fe6e7e47c67",
            "463b1c43dfd689841d2fd43d24058e68b7224dcf",
            "d5baf1912d43d0af0e8f683eb5532f5f5445430e",
            "046de0df4562625a14998f96601a28475a05e4bb",
            "1f96460299f3c74965b4fe8e64c28957ada06c74",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "d88158745b69f5732397175389101e2d98799c00"
        ],
        "related_topics": [],
        "reference_count": "51",
        "citation_count": "3"
    },
    {
        "Id": "0b1bde9d681580b807d5da5933cfc6920fa922fc",
        "title": "Image-based profiling: a powerful and challenging new data type",
        "authors": [
            "Gregory P. Way",
            "Hannah Spitzer",
            "Philip Burnham",
            "Arjun Raj",
            "Fabian J Theis",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "28 November 2021",
        "abstract": "The authors are still in the early days of image-based profiling, and it is clear that the many opportunities to interrogate the representations that capture morphological cell state are clear. Software has provided cell biologists the power to quantify specific cellular features in cell images at scale. Before long, these biologists also recognized the potential to extract much more biological information from the same images. From here, the field of image-based profiling, the process of extracting unbiased representations that capture morphological cell state, was born. We are still in the early days of image-based profiling, and it is clear that the many opportunities to interrogate Pacific Symposium on Biocomputing 27:407-411(2022)",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "8b73514fd708f6bdd3786d47a8673a97fdce4dfe",
            "b464bd244357544eed1a07106ff1d69ca9c6f17d",
            "06147c4e743d4cbc7d95541c4aaf08d09139cc79",
            "b9d289cd7062fc6ce190832fbef505b47326f08e",
            "a512d0486bd9f7b2fcfde65c6f73dc8bff7eb7b5",
            "48b915610eb258a8c57c86fa6fe52ad810335f1e",
            "2d01766b69eaeb44a25e76e0c090cdb8d264c839",
            "56caaad719f4e6cec0eab3abbd4893f7fe0f27e9",
            "20a1763fb86166fe6f2fa8cbaadaf61a13a112ef"
        ],
        "related_topics": [],
        "reference_count": "11",
        "citation_count": "2"
    },
    {
        "Id": "57edd06af1aefdda5fe8f555b6122431a9dca13c",
        "title": "High throughput microscopy and single cell phenotypic image-based analysis in toxicology and drug discovery.",
        "authors": [
            "Fabio Stossi",
            "Pankaj K Singh",
            "Kazem Safari",
            "Michela Marini",
            "Demetrio Labate",
            "Michael A. Mancini"
        ],
        "date": "1 September 2023",
        "abstract": "Semantic Scholar extracted view of \"High throughput microscopy and single cell phenotypic image-based analysis in toxicology and drug discovery.\" by F. Stossi et al.",
        "references": [
            "2fa7430bcdcb45ec471bacf3cc3a514cd53edb11",
            "46fb072801cb8bffc29687cd2483171645d9db16",
            "8b73514fd708f6bdd3786d47a8673a97fdce4dfe",
            "99ba4306154620dc64e72010632b4ab498569464",
            "1063da85563433e2edaff09232803c588f320146",
            "6b565993c9acd85194b30197de93e29f40122040",
            "929a490198770abcb8c123d68a59384879b69adb",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "854b428a0f50eb46d20382497e960ed0095d5d8a",
            "c088b24fb15581d5986e47d7007f961cd0847a5e"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "199"
    },
    {
        "Id": "f7ec3c2396844cb7f60649be7c059f206d8c3b46",
        "title": "Variance-stabilization-based compressive inversion under Poisson or Poisson\u2013Gaussian noise with analytical bounds",
        "authors": [
            "Pakshal Bohra",
            "Deepak Garg",
            "Karthik S. Gurumoorthy",
            "Ajit V. Rajwade"
        ],
        "date": "18 June 2019",
        "abstract": "The authors' is the first piece of work to derive bounds on compressive inversion for the Poisson\u2013Gaussian noise model, and uses the properties of the variance stabilizer to develop a principle for selection of the regularization parameter in penalized estimators for Poisson and Poisson-Gaussian inverse problems. Most existing bounds for signal reconstruction from compressive measurements make the assumption of additive signal-independent noise. However in many compressive imaging systems, the noise statistics are more accurately represented by Poisson or Poisson\u2013Gaussian noise models. In this paper, we derive upper bounds for signal reconstruction error from compressive measurements which are corrupted by Poisson or Poisson\u2013Gaussian noise. The features of our bounds are as follows: (1) the bounds are derived for a computationally tractable convex estimator with statistically motivated parameter selection. The estimator penalizes signal sparsity subject to a constraint that imposes a novel statistically motivated upper bound on a term based on variance stabilization transforms to approximate the Poisson or Poisson\u2013Gaussian distributions by distributions with (nearly) constant variance. (2) The bounds are applicable to signals that are sparse as well as compressible in any orthonormal basis, and are derived for compressive systems obeying realistic constraints such as non-negativity and flux-preservation. Our bounds are motivated by several properties of the variance stabilization transforms that we develop and analyze. We present extensive numerical results for signal reconstruction under varying number of measurements and varying signal intensity levels. Ours is the first piece of work to derive bounds on compressive inversion for the Poisson\u2013Gaussian noise model. We also use the properties of the variance stabilizer to develop a principle for selection of the regularization parameter in penalized estimators for Poisson and Poisson\u2013Gaussian inverse problems.",
        "references": [
            "90d5988e60124db3e2221b90e2560d99790e7daf",
            "c0ff008fe0ea97f117d200b702be481a073fbae8",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "34a03e0290d580f9a8a61cc8c7005f8d7e0bdfc7",
            "dca27a6f185258eca876c209ceaab0bf810f16c8",
            "1ed22f981940b25d3ceac8846153b3521e8cae7b",
            "b88f2df713c30c358076441e188d4339db993f65",
            "18b04752a09119494d21c9d57c6097e9595be8c8",
            "5136c28493c6cc15a61c4a6aa45b834494935b39"
        ],
        "related_topics": [],
        "reference_count": "60",
        "citation_count": "10"
    },
    {
        "Id": "2f9fc08a7349e65dc2d315796a67a541d6d03f4e",
        "title": "Reconstruction Error Bounds for Compressed Sensing under Poisson or Poisson-Gaussian Noise Using Variance Stabilization Transforms",
        "authors": [
            "Deepak Garg",
            "Ajit V. Rajwade"
        ],
        "date": "3 July 2017",
        "abstract": "Upper bounds for signal reconstruction error from compressive measurements which are corrupted by Poisson or Poisson-Gaussian noise are derived and are applicable to signals that are sparse as well as compressible in any orthonormal basis. Most existing bounds for signal reconstruction from compressive measurements make the assumption of additive signal-independent noise. However in many compressive imaging systems, the noise statistics are more accurately represented by Poisson or Poisson-Gaussian noise models. In this paper, we derive upper bounds for signal reconstruction error from compressive measurements which are corrupted by Poisson or Poisson-Gaussian noise. The features of our bounds are as follows: (1) The bounds are derived for a probabilistically motivated, computationally tractable convex estimator with principled parameter selection. The estimator penalizes signal sparsity subject to a constraint that imposes an upper bound on a term based on variance stabilization transforms to approximate the Poisson or Poisson-Gaussian negative log-likelihoods. (2) They are applicable to signals that are sparse as well as compressible in any orthonormal basis, and are derived for compressive systems obeying realistic constraints such as non-negativity and flux-preservation. We present extensive numerical results for signal reconstruction under varying number of measurements and varying signal intensity levels.",
        "references": [
            "34a03e0290d580f9a8a61cc8c7005f8d7e0bdfc7",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "c0ff008fe0ea97f117d200b702be481a073fbae8",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "1ed22f981940b25d3ceac8846153b3521e8cae7b",
            "b88f2df713c30c358076441e188d4339db993f65",
            "dca27a6f185258eca876c209ceaab0bf810f16c8",
            "18b04752a09119494d21c9d57c6097e9595be8c8",
            "5136c28493c6cc15a61c4a6aa45b834494935b39",
            "0c6398c4231aa0f03121bee8632d5cba4da67de3"
        ],
        "related_topics": [],
        "reference_count": "50",
        "citation_count": "2"
    },
    {
        "Id": "9e35901cbd1c258e99a2d119c666d8f1f0c62c88",
        "title": "Two penalized estimators based on variance stabilization transforms for sparse compressive recovery with Poisson measurement noise",
        "authors": [
            "Ajit V. Rajwade",
            "Karthik S. Gurumoorthy"
        ],
        "date": "1 November 2021",
        "abstract": "Semantic Scholar extracted view of \"Two penalized estimators based on variance stabilization transforms for sparse compressive recovery with Poisson measurement noise\" by Ajit V. Rajwade et al.",
        "references": [
            "f7ec3c2396844cb7f60649be7c059f206d8c3b46",
            "c0ff008fe0ea97f117d200b702be481a073fbae8",
            "e8e1b2d7917c22b01d0528d6d6159564cc284cfd",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "90d5988e60124db3e2221b90e2560d99790e7daf",
            "ab2452f31db5505c2a03553acc573167a4f8840c",
            "20723ba867599bdd492b1b0fd6f05072626ea84a",
            "96c3209106cc0fd1310be658eac19124ec01bf85",
            "18b04752a09119494d21c9d57c6097e9595be8c8"
        ],
        "related_topics": [
            "Poisson Concentration Inequalities",
            "Compressed Sensing",
            "Sparse Signals",
            "Variance Stabilization"
        ],
        "reference_count": "44",
        "citation_count": "2"
    },
    {
        "Id": "90d5988e60124db3e2221b90e2560d99790e7daf",
        "title": "Using an Information Theoretic Metric for Compressive Recovery under Poisson Noise",
        "authors": [
            "Sukanya Patil",
            "Karthik S. Gurumoorthy",
            "Ajit V. Rajwade"
        ],
        "date": "1 September 2019",
        "abstract": "Semantic Scholar extracted view of \"Using an Information Theoretic Metric for Compressive Recovery under Poisson Noise\" by Sukanya Patil et al.",
        "references": [
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "c0ff008fe0ea97f117d200b702be481a073fbae8",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "b88f2df713c30c358076441e188d4339db993f65",
            "1ed22f981940b25d3ceac8846153b3521e8cae7b",
            "e4d56ec1be5603ddb2cd5b9264e365a19dbd5217",
            "16a23a15741e085dfae592fca4d4808c38c8c719",
            "ec603e605e84a2fe02dc9ac008fee71748ef04c2",
            "dde8bee5a0ccaaa629d635d56d1643c878ca26d8",
            "18b04752a09119494d21c9d57c6097e9595be8c8"
        ],
        "related_topics": [
            "Compressed Sensing",
            "Flux Preservation",
            "Jensen-shannon Divergence",
            "Restricted Isometry Property",
            "Regularization",
            "Negative Log Likelihood",
            "Gaussian"
        ],
        "reference_count": "59",
        "citation_count": "3"
    },
    {
        "Id": "c0ff008fe0ea97f117d200b702be481a073fbae8",
        "title": "Performance bounds for Poisson compressed sensing using Variance Stabilization Transforms",
        "authors": [
            "Deepak Garg",
            "Ajit V. Rajwade"
        ],
        "date": "1 March 2017",
        "abstract": "The analysis of reconstruction errors for compressed sensing under Poisson noise is challenging due to the signal dependent nature of the noise, and also because the Poisson negative log-likelihood is not a metric. In this paper, we present error bounds for reconstruction of signals which are sparse or compressible under any given orthonormal basis, given compressed measurements corrupted by Poisson noise and acquired in a realistic physical system. The concerned optimization problem is framed based on the well-known Variance Stabilization Transforms which transform the noise to (approximately) Gaussian with a fixed variance. This problem also turns out to be convex. We demonstrate promising numerical results on signals with different sparsity, intensity levels and given different numbers of compressed measurements.",
        "references": [
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "61b913d48b9b35dcec22d602f4209750ef963ab3",
            "8de53c63014fda2f4554fc67d3e819ed7ff387dd",
            "b88f2df713c30c358076441e188d4339db993f65",
            "beb96e51a221577d5b53f8f0d102ce1427f270fd",
            "cc79e154ee8cf75e8d132f497b64f7c10c380bcd",
            "96c3209106cc0fd1310be658eac19124ec01bf85",
            "94e731d30647e88d7e3bef13887986940dbd59bc",
            "e7321ab0f3be0b29aaf5f073fd7de7da5fed2f92"
        ],
        "related_topics": [
            "Poisson Compressed Sensing",
            "Compressed Measurement",
            "Compressed Sensing",
            "Signal-dependent",
            "Compressible",
            "Gaussian",
            "Convex",
            "Negative Log Likelihood"
        ],
        "reference_count": "19",
        "citation_count": "5"
    },
    {
        "Id": "a2c7b7db0ef5994cc43fb550880406c595d14d5f",
        "title": "Sparse parametric estimation of Poisson processes",
        "authors": [
            "Michael G. Moore",
            "Mark A. Davenport"
        ],
        "date": "2017",
        "abstract": "This work extends parameter estimation for Poisson counting processes to the more-general setting of Poisson arrival processes (i.e., with infinite arrival resolution), and struggles to combine a concrete guarantee with a tractable program. Previous results have achieved reliable Poisson estimation mostly by adapting existing estimators intended for independent noise (e.g., [1]) and studied this problem from the perspective of minimax risk bounds that hold for any estimator [2]. Only recently have recovery guarantees been established for the maximum-likelihood estimator, which often outperforms other estimators in practice [3]. However, these results apply only to Poisson counting processes, in which event arrivals are histogrammed into bins (or, equivalently, the rate function Rx\u0304(t) is piecewise-constant). The guarantee we provide here extends parameter estimation for Poisson counting processes to the more-general setting of Poisson arrival processes (i.e., with infinite arrival resolution). We highlight that improved results are possible when the solution is known to be sparse, as is already known in the Poisson counting case [3]. We note that our result requires fewer assumptions than previous results for Poisson estimation. In particular, we do not require the common constraint that the rate be a nonnegative combination of nonnegative functions and our recovery program requires less knowledge regarding the true parameters. However, our result struggles to combine a concrete guarantee with a tractable program.",
        "references": [
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "bc0aef148b8ec2f3dbdced4686b1ad094a2f0789",
            "18b04752a09119494d21c9d57c6097e9595be8c8",
            "b88f2df713c30c358076441e188d4339db993f65",
            "697d27649e857b207671672052765f49826ed2e1"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "5"
    },
    {
        "Id": "bc0aef148b8ec2f3dbdced4686b1ad094a2f0789",
        "title": "A Data-Dependent Weighted LASSO Under Poisson Noise",
        "authors": [
            "Xin Jiang Hunt",
            "Patricia Reynaud-Bouret",
            "Vincent Rivoirard",
            "Laure Sansonnet",
            "Rebecca M. Willett"
        ],
        "date": "29 September 2015",
        "abstract": "A novel alternative analysis approach for sparse Poisson inverse problems that sidesteps the technical challenges present in previous work, admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and hints at a general framework for broad classes of noise in sparse linear inverse problems. Sparse linear inverse problems appear in a variety of settings, but often the noise contaminating observations cannot accurately be described as bounded by or arising from a Gaussian distribution. Poisson observations in particular are a characteristic feature of several real-world applications. Previous work on sparse Poisson inverse problems encountered several limiting technical hurdles. This paper describes a novel alternative analysis approach for sparse Poisson inverse problems that 1) sidesteps the technical challenges present in previous work, 2) admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and 3) hints at a general framework for broad classes of noise in sparse linear inverse problems. At the heart of this new approach lies a weighted LASSO estimator for which data-dependent weights are based on Poisson concentration inequalities. Unlike previous analyses of the weighted LASSO, the proposed analysis depends on conditions which can be checked or shown to hold in general settings with high probability. 2000 Math Subject Classification: 60E15, 62G05, 62G08, and 94A12.",
        "references": [
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "00b7f44857676600805172e99be6f9f2987e98eb",
            "94338966e48c843ff882fa91faa1592985f6f228",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "1ed22f981940b25d3ceac8846153b3521e8cae7b",
            "ece5045d0f495834b25d2188735bcee982edd8c2",
            "5f0d051ffcbde45f92daee6395883cd476777ad9",
            "5f56320c5979faeab78dbd9ddb7db755ba4550f3",
            "df1b5b183c0120757124b7b014a82e64829f9778",
            "42c4fd5c7e45b31651b8cee4adc3ad8b6bcf4d87"
        ],
        "related_topics": [
            "Poisson Concentration Inequalities",
            "Poisson Compressed Sensing",
            "Sparse Linear Inverse Problems"
        ],
        "reference_count": "63",
        "citation_count": "24"
    },
    {
        "Id": "e8e1b2d7917c22b01d0528d6d6159564cc284cfd",
        "title": "Analyzing cross-validation in compressed sensing with Poisson noise",
        "authors": [
            "Sudarsanan Rajasekaran",
            "Ajit V. Rajwade"
        ],
        "date": "1 May 2021",
        "abstract": "Semantic Scholar extracted view of \"Analyzing cross-validation in compressed sensing with Poisson noise\" by S. Rajasekaran et al.",
        "references": [
            "468f7ee911659639c72beacebf760d2c4b8e1000",
            "f15eb1306e73c6ab8b5eff4a7e32d95459a94f51",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "20723ba867599bdd492b1b0fd6f05072626ea84a",
            "db09000f10d2d8c54057f53f34b2bd6dae57a727",
            "90d5988e60124db3e2221b90e2560d99790e7daf",
            "ab2452f31db5505c2a03553acc573167a4f8840c",
            "f7ec3c2396844cb7f60649be7c059f206d8c3b46",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "beb96e51a221577d5b53f8f0d102ce1427f270fd"
        ],
        "related_topics": [
            "Cross-validation",
            "Compressed Sensing",
            "Reconstruction Set",
            "Compressive Sensing",
            "LASSO Estimator",
            "Parameters"
        ],
        "reference_count": "34",
        "citation_count": "5"
    },
    {
        "Id": "9e46ff3d4b13b099fb6281a7d74f2777c238f038",
        "title": "Estimation of Poisson Arrival Processes Under Linear Models",
        "authors": [
            "Michael G. Moore",
            "Mark A. Davenport"
        ],
        "date": "2 March 2018",
        "abstract": "This paper considers the problem of estimating the parameters of a Poisson arrival process, where the intensity function is assumed to lie in the span of a known basis, and establishes novel guarantees concerning the accuracy achieved by the maximum likelihood estimate. In this paper, we consider the problem of estimating the parameters of a Poisson arrival process, where the intensity function is assumed to lie in the span of a known basis. Our goal is to estimate the basis expansions coefficients given a realization of this process. We establish novel guarantees concerning the accuracy achieved by the maximum likelihood estimate. Our initial result is near-optimal, with the exception of an undesirable dependence on the dynamic range of the intensity function. We then show how to remove this dependence through a process of \u201cnoise regularization,\u201d which results in an improved bound under our analysis. We conjecture that a similar guarantee should be possible when using a more direct (deterministic) regularization scheme. We conclude with a discussion of practical applications and an empirical examination of the proposed regularization schemes.",
        "references": [
            "1c5a139dd45637f588c4c0506abed0db30e14310",
            "91e74d02d20a35c06dc1affacdb9ba93cbd1d72d",
            "0e16a10560b4f7d5f891b932c79d1b2005407acf",
            "18b04752a09119494d21c9d57c6097e9595be8c8",
            "bc0aef148b8ec2f3dbdced4686b1ad094a2f0789",
            "f1b8a1a01313245af4d41d463e9a2300babd91f6",
            "94338966e48c843ff882fa91faa1592985f6f228",
            "51ff6cfb98eec39da52035d87f329cf9e89462e6",
            "a250b2605337985501c35ac43b5f3bb2696da5cd",
            "cfd614f4e2589ebe9e477a60898ef77f50724f6a"
        ],
        "related_topics": [
            "Basis Expansion Coefficients"
        ],
        "reference_count": "28",
        "citation_count": "2"
    },
    {
        "Id": "2850271fefff4e277c8cd5d27867c06d770b60d4",
        "title": "Robust Estimation of Self-Exciting Generalized Linear Models With Application to Neuronal Modeling",
        "authors": [
            "Abbas Kazemipour",
            "Min Wu",
            "Behtash Babadi"
        ],
        "date": "14 July 2015",
        "abstract": "This work analyzes the performance of two classes of estimators, namely, the $\\ell _1$-regularized maximum likelihood and greedy estimator, for a canonical self-exciting process and characterize the sampling trade-offs required for stable recovery in the non-asymptotic regime. We consider the problem of estimating self-exciting generalized linear models from limited binary observations, where the history of the process serves as the covariate. We analyze the performance of two classes of estimators, namely, the $\\ell _1$-regularized maximum likelihood and greedy estimators, for a canonical self-exciting process and characterize the sampling trade-offs required for stable recovery in the non-asymptotic regime. Our results extend those of compressed sensing for linear and generalized linear models with independent identically distributed covariates to those with highly interdependent covariates. We further provide simulation studies as well as application to real spiking data from the mouse's lateral geniculate nucleus and the ferret's retinal ganglion cells, which agree with our theoretical predictions.",
        "references": [
            "fbb393f1d61cacba3c1d543003c7c346e3157e49",
            "0d92ce89894d50af18c902e6b89f85eb0d4fcde2",
            "c76d84d779e28e3b236054a0f34c3e48910399d8",
            "8a7c8f6abbe092446921f9e6c4ec180360842ee6",
            "5fbdcdeed961c2a20b12c3233eec4ef9365cb775",
            "77e577d7fa26679f722f98cbe110ad5255721f97",
            "006a34dcdfc63c5b7a2cf4d0b76cecedbb4b93d9",
            "f168f04e6e003bf3bccd9a5e868cf29500a95e74",
            "326ceb2b80b4a1cc5cf91f5e4f37fe3497ebf154",
            "c08206b44dd1f0ea54bd073e4effaf2e4483169b"
        ],
        "related_topics": [
            "Generalized Linear Models",
            "Self-Exciting",
            "Stable Recovery",
            "Compressed Sensing"
        ],
        "reference_count": "60",
        "citation_count": "10"
    },
    {
        "Id": "38ca689c2f916c648ea3ecb1043facbc4bea0d4f",
        "title": "Puzzle-AE: Novelty Detection in Images through Solving Puzzles",
        "authors": [
            "Mohammadreza Salehi",
            "Ainaz Eftekhar",
            "Niousha Sadjadi",
            "Mohammad Hossein Rohban",
            "Hamid R. Rabiee"
        ],
        "date": "29 August 2020",
        "abstract": "This work proposes adversarial robust training as an effective automatic shortcut removal and achieves competitive or superior results compared to the State of the Art (SOTA) anomaly detection methods on various toy and real-world datasets. Autoencoder, as an essential part of many anomaly detection methods, is lacking flexibility on normal data in complex datasets. U-Net is proved to be effective for this purpose but overfits on the training data if trained by just using reconstruction error similar to other AE-based frameworks. Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods, has earlier proved its ability in learning semantically meaningful features. We show that training U-Nets based on this task is an effective remedy that prevents overfitting and facilitates learning beyond pixel-level features. Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw puzzles. We propose adversarial robust training as an effective automatic shortcut removal. We achieve competitive or superior results compared to the State of the Art (SOTA) anomaly detection methods on various toy and real-world datasets. Unlike many competitors, the proposed framework is stable, fast, data-efficient, and does not require unprincipled early stopping.",
        "references": [
            "10b219619e88931fabb674037bbb633682775136",
            "c2b733a79db700b971327a58ef42699fe8a416aa",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "6d4a87759917132913319960389f17fa1fe8b630",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "70f9968a356d840040a1c9207906f60376dc6bd4",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "599fd051c9438011ec5b581983c89e8922b4a5e6"
        ],
        "related_topics": [
            "Puzzle-AE",
            "Latent Space Autoregression",
            "U-Net",
            "Anomaly Detection",
            "Semi-Supervised Learning",
            "Data Efficient",
            "Early Stopping",
            "Self-Supervised Learning",
            "Overfitting",
            "Novelty Detection"
        ],
        "reference_count": "66",
        "citation_count": "34"
    },
    {
        "Id": "12ebe64bf81b85b2331875895bd3a2b5978dabd8",
        "title": "Ano-Graph: Learning Normal Scene Contextual Graphs to Detect Video Anomalies",
        "authors": [
            "Masoud PourReza",
            "Mohammadreza Salehi",
            "M. Sabokrou"
        ],
        "date": "18 March 2021",
        "abstract": "This work proposes a novel yet efficient method named Ano-Graph for learning and modeling the interaction of normal objects, which is data-efficient, significantly more robust against common real-world variations such as illumination, and passes SOTA by a large margin on the challenging datasets ADOC and Street Scene. Video anomaly detection has proved to be a challenging task owing to its unsupervised training procedure and high spatio-temporal complexity existing in real-world scenarios. In the absence of anomalous training samples, state-of-the-art methods try to extract features that fully grasp normal behaviors in both space and time domains using different approaches such as autoencoders, or generative adversarial networks. However, these approaches completely ignore or, by using the ability of deep networks in the hierarchical modeling, poorly model the spatio-temporal interactions that exist between objects. To address this issue, we propose a novel yet efficient method named Ano-Graph for learning and modeling the interaction of normal objects. Towards this end, a Spatio-Temporal Graph (STG) is made by considering each node as an object's feature extracted from a real-time off-the-shelf object detector, and edges are made based on their interactions. After that, a self-supervised learning method is employed on the STG in such a way that encapsulates interactions in a semantic space. Our method is data-efficient, significantly more robust against common real-world variations such as illumination, and passes SOTA by a large margin on the challenging datasets ADOC and Street Scene while stays competitive on Avenue, ShanghaiTech, and UCSD.",
        "references": [
            "fef6f1e04fa64f2f26ac9f01cd143dd19e549790",
            "1df6db34c477aa43218f2efc4ec5458eaf734849",
            "a03bda078490e8ee991a1f86b53f27df7cf93a14",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
            "a4c94b221062d0737ee967affa80ce2110cc50c0",
            "96d7a07237e146c28173767dfc6290a337696c04",
            "e4667dab1f686537b66806b9061275f8ee1eba85",
            "7c75203739f5f89e109b11144d170d4d3f2a6abc",
            "355b4e74774798c177c82943eef925d66a2bb2ce"
        ],
        "related_topics": [
            "Data Efficient",
            "Deep Network",
            "Generative Adversarial Networks",
            "Semantic Space",
            "Normal Behavior",
            "Video Anomaly Detection",
            "Street Scenes",
            "Illumination",
            "Autoencoders",
            "Avenue"
        ],
        "reference_count": "92",
        "citation_count": "12"
    },
    {
        "Id": "027c44214e34f06bfd82625c393ddc73cf4ae760",
        "title": "An Experimental Study of Semantic Continuity for Deep Learning Models",
        "authors": [
            "Shangxi Wu",
            "Jitao Sang",
            "Xian Zhao",
            "Lizhang Chen"
        ],
        "date": "19 November 2020",
        "abstract": "Qualitative and quantitative experiments prove that semantically continuous models successfully reduce the use of non-semantic information, which further contributes to the improvement in adversarial robustness, interpretability, model transfer, and machine bias. Deep learning models suffer from the problem of semantic discontinuity: small perturbations in the input space tend to cause semantic-level interference to the model output. We argue that the semantic discontinuity results from these inappropriate training targets and contributes to notorious issues such as adversarial robustness, interpretability, etc. We first conduct data analysis to provide evidence of semantic discontinuity in existing deep learning models, and then design a simple semantic continuity constraint which theoretically enables models to obtain smooth gradients and learn semantic-oriented features. Qualitative and quantitative experiments prove that semantically continuous models successfully reduce the use of non-semantic information, which further contributes to the improvement in adversarial robustness, interpretability, model transfer, and machine bias.",
        "references": [
            "38a547b5933758a2018f0a40f2c3d538077c4d01",
            "5d28bdfa02f0766febff32b7a6b287611d6f2995",
            "e088f262c96a957b0760b753d1f4d34d3b9923e1",
            "393619a12436701c796d8b0bf9e72efb68c4913f",
            "17293cd36ee5e7ec37dcec1d5ab85f9b77ad65d5",
            "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "08e82ca7906efae2e6bf4b9523c38972e509f96a",
            "10b219619e88931fabb674037bbb633682775136",
            "a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb",
            "5582bebed97947a41e3ddd9bd1f284b73f1648c2"
        ],
        "related_topics": [
            "Adversarial Robustness",
            "Interpretability",
            "Model Output",
            "Input Space",
            "Training Targets"
        ],
        "reference_count": "0",
        "citation_count": "35"
    },
    {
        "Id": "bd6262ebdd1a865e8e6859ab7dd8dc576d2a90e6",
        "title": "One-Class Classification: A Survey",
        "authors": [
            "Pramuditha Perera",
            "Poojan Oza",
            "Vishal M. Patel"
        ],
        "date": "8 January 2021",
        "abstract": "A survey of classical statistical and recent deep learning-based OCC methods for visual recognition discusses the merits and drawbacks of existing OCC approaches and identifies promising avenues for research in this field. One-Class Classification (OCC) is a special case of multi-class classification, where data observed during training is from a single positive class. The goal of OCC is to learn a representation and/or a classifier that enables recognition of positively labeled queries during inference. This topic has received considerable amount of interest in the computer vision, machine learning and biometrics communities in recent years. In this article, we provide a survey of classical statistical and recent deep learning-based OCC methods for visual recognition. We discuss the merits and drawbacks of existing OCC approaches and identify promising avenues for research in this field. In addition, we present a discussion of commonly used datasets and evaluation metrics for OCC.",
        "references": [
            "c0c07935977e70e71d296535729fc718636d76c4",
            "732c21998e251d64cd58b6a86886ee5907efeaa5",
            "9eb1b16fbd4786eaac91f308d75609b9321868ce",
            "3a9b175324ba11bc0e16c0633912d897b2fac4e2",
            "00695a31a80221c7125e49885a4767896ec2c4f7",
            "9d868ed92f1c78d1f696a26c71176a6a33a72423",
            "f685d8a883604d62c039b2334a854fa34eee2289",
            "c72bb6c83db08e17e2736bc1d2cf439fe1f71905",
            "967d532a66dab7edcb818b0f9dc59fe8da7dc171",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "One-class Classification",
            "Multi-class Classification",
            "Positive Class",
            "Classifier",
            "Visual Recognition",
            "Computer Vision",
            "Biometrics",
            "Inference",
            "Machine Learning"
        ],
        "reference_count": "103",
        "citation_count": "73"
    },
    {
        "Id": "69244cd6455392f7eb0dec90119d5c689ee6f1ba",
        "title": "Robust Semi-Supervised Anomaly Detection via Adversarially Learned Continuous Noise Corruption",
        "authors": [
            "Jack W. Barker",
            "Neelanjan Bhowmik",
            "Yona Falinie A. Gaus",
            "T. Breckon"
        ],
        "date": "2 March 2023",
        "abstract": "A training scheme applied to a Denoising Autoencoder (DAE) is implemented which introduces an efficient method of producing Adversarially Learned Continuous Noise (ALCN) to maximally globally corrupt the input priorto denoising. Anomaly detection is the task of recognising novel samples which deviate significantly from pre-establishednormality. Abnormal classes are not present during training meaning that models must learn effective rep-resentations solely across normal class data samples. Deep Autoencoders (AE) have been widely used foranomaly detection tasks, but suffer from overfitting to a null identity function. To address this problem, weimplement a training scheme applied to a Denoising Autoencoder (DAE) which introduces an efficient methodof producing Adversarially Learned Continuous Noise (ALCN) to maximally globally corrupt the input priorto denoising. Prior methods have applied similar approaches of adversarial training to increase the robustnessof DAE, however they exhibit limitations such as slow inference speed reducing their real-world applicabilityor producing generalised obfuscation which is more trivial to denoise. We show through rigorous evaluationthat our ALCN method of regularisation during training improves AUC performance during inference whileremaining efficient over both classical, leave-one-out novelty detection tasks with the variations-: 9 (normal)vs. 1 (abnormal)&1 (normal) vs. 9 (abnormal); MNIST - AUCavg: 0.890&0.989, CIFAR-10 - AUCavg: 0.670&0.742, in addition to challenging real-world anomaly detection tasks: industrial inspection (MVTEC-AD -AUCavg: 0.780) and plant disease detection (Plant Village - AUC: 0.770) when compared to prior approaches.",
        "references": [
            "10b219619e88931fabb674037bbb633682775136",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "49130e2084f99c59150a86d4f1be6623a694386b",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "1856809b8bcd0ba7b2c294201718018ead419cb7",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "6a8acb6db2a835f83782b2968dd42a4a14709461",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "2b75ba7f75170b73d913c515cc0deefef6c88f5f"
        ],
        "related_topics": [
            "Anomaly Detection",
            "Overfitting",
            "Plant Disease Detection",
            "Denoising Autoencoders",
            "Industrial Inspection",
            "Inference Speeds",
            "Adversarial Training",
            "Differential-algebraic Equations",
            "Deep Autoencoder",
            "Semi-supervised Anomaly Detection"
        ],
        "reference_count": "33",
        "citation_count": "2"
    },
    {
        "Id": "929b4bcb34e726bcc593946c9b084996e51e0ba1",
        "title": "Learning Representations for Novelty and Anomaly Detection",
        "authors": [
            "Ranya Almohsen"
        ],
        "date": "2023",
        "abstract": "Learning Representations for Novelty and Anomaly Detection",
        "references": [
            "ee70341e75c2dffebbabd24b239cc158ad691ed1",
            "109eef0f2319c6173b7cc22120254f01746ef77a",
            "11709bfadfd6bbb371f4077bccb7c26d93c39cdd",
            "08f99af9f5d6d351201ee4563e407bf37bc164fd",
            "ec5d26aaf2e6902ca28effa0f94c9556571160b8",
            "094ac7510d1723cb9c2da01db47291322aa29025",
            "22c0bf46726ad1c90b5fac7f16638c813e86c829",
            "4b9f4f7ea5466e6e7545efe7bd095c27e7454494",
            "130a5595c38becad29e4fc9108f4427b42a5ac54",
            "37595f7a51982d776e57c7280b9445474d90f0be"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "109"
    },
    {
        "Id": "5e5ae3d3439151cd7a026aadb8da9eab16a7b7a7",
        "title": "Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection",
        "authors": [
            "Gaoang Wang",
            "Yibing Zhan",
            "Xinchao Wang",
            "Min-Gyoo Song",
            "Klara Nahrstedt"
        ],
        "date": "24 July 2022",
        "abstract": "A novel hierarchical semi-supervised contrastive learning (HSCL) framework, for contamination-resistant anomaly detection, that hierarchically regulates three complementary relations, enlarging the discrimination between normal and abnormal samples with a comprehensive exploration of the contaminated data and is an end-to-end learning approach that can efficiently learn discriminative representations without fine-tuning. Anomaly detection aims at identifying deviant samples from the normal data distribution. Contrastive learning has provided a successful way to sample representation that enables effective discrimination on anomalies. However, when contaminated with unlabeled abnormal samples in training set under semi-supervised settings, current contrastive-based methods generally 1) ignore the comprehensive relation between training data, leading to suboptimal performance, and 2) require fine-tuning, resulting in low efficiency. To address the above two issues, in this paper, we propose a novel hierarchical semi-supervised contrastive learning (HSCL) framework, for contamination-resistant anomaly detection. Specifically, HSCL hierarchically regulates three complementary relations: sample-to-sample, sample-to-prototype, and normal-to-abnormal relations, enlarging the discrimination between normal and abnormal samples with a comprehensive exploration of the contaminated data. Besides, HSCL is an end-to-end learning approach that can efficiently learn discriminative representations without fine-tuning. HSCL achieves state-of-the-art performance in multiple scenarios, such as one-class classification and cross-dataset detection. Extensive ablation studies further verify the effectiveness of each considered relation. The code is available at https://github.com/GaoangW/HSCL.",
        "references": [
            "7d90243c5a46430a36c5ba88627b5d254450a1e1",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "5484999008a0ab9a5adae33dae0a91df25aa0504",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "568a93409f91e959b075ffee9435204b4f15569c",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9",
            "4b83e9a048e677ff9fab226e8f7fbaa33fd32024",
            "232b26f231122f6332d66244e5ad61d8225312a2",
            "5f61089d3d548a515f01b473f0119137d1f340d4"
        ],
        "related_topics": [
            "Fine-tuning",
            "Discrimination",
            "Semi-supervised Setting",
            "Unlabeled",
            "Training Set",
            "Samples",
            "Anomaly Detection",
            "One-class Classification",
            "Contrastive Learning"
        ],
        "reference_count": "83",
        "citation_count": "6"
    },
    {
        "Id": "32c82fa3e438521d010e7c7b95a7c01d61f2ceff",
        "title": "Future frame prediction based on generative assistant discriminative network for anomaly detection",
        "authors": [
            "Chaobo Li",
            "Hongjun Li",
            "Guoan Zhang"
        ],
        "date": "20 April 2022",
        "abstract": "A generative assistant discriminative network for anomaly detection by predicting future frames in combination of adversary and cooperation, which mainly consists of the generator, discriminator and assistor. Anomaly detection plays an important role in intelligent surveillance and has attracted increasing attention from researchers in recent years. It is generally regarded as discrimination that cannot be properly represented in most approaches. Despite its importance in probing the scarcity and indefinability of abnormal data during training, designing an effective network is exceptionally complex due to the diversity of the motion information, difficulty of parsing prediction errors, robustness and so on. To improve the ability to extract subtle features between normal and abnormal frames, and to improve the robustness to noise, a generative assistant discriminative network for anomaly detection is proposed. This method detects anomalies by predicting future frames in combination of adversary and cooperation, which mainly consists of the generator, discriminator and assistor. The generator predicts the future frames, while the discriminator distinguishes the predicted future frames from actual future frames. Moreover, by means of noise, the assistor is able to learn from pseudo abnormal future frames and predicted future frames. This helps the generator strengthen the ability to extract the discriminative features between normal and abnormal events. The motion information is used for integration into the predicted future frames. Extensive experiments are conducted on the UCSD Ped1, Ped2, CUHK Avenue and ShanghaiTech datasets. A comparison with the state-of-the-art methods shows the effectiveness and advantages of our method for anomaly detection.",
        "references": [
            "7207721ba8e12750758269722668ccb03707ddd0",
            "fe09f7a379944444201552e952b910188c0aeaca",
            "58a9a798525969cdeeb610a52e6f58cb67b24186",
            "953312b683a1e69f1c4f6fefef6dfc769fb648f5",
            "16e1bdb834340b0fd7a5737dfb87abda373f72ca",
            "8a6acba7fb2aad1299fcf35701417e063d410ed4",
            "799426e24403a50bb9012f59c890fccd46375e20",
            "91336c65cddd8546745f67a1b860c2595904ac15",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "945b53ede48dae40af9870030fc4985a119cd1b8"
        ],
        "related_topics": [
            "Future Frames",
            "Anomaly Detection",
            "Motion Information",
            "Discriminator",
            "Adversary",
            "UCSD Ped1",
            "Discrimination",
            "ShanghaiTech",
            "Ped2",
            "Future Frame Prediction"
        ],
        "reference_count": "51",
        "citation_count": "11"
    },
    {
        "Id": "8b153cc2c7f5ea9f307f12ea945a5e9196ee5c52",
        "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges",
        "authors": [
            "Mohammadreza Salehi",
            "Hossein Mirzaei",
            "Dan Hendrycks",
            "Yixuan Li",
            "Mohammad Hossein Rohban",
            "M. Sabokrou"
        ],
        "date": "26 October 2021",
        "abstract": "This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities and discusses and shed light on future lines of research, intending to bring these fields closer together. Machine learning models often encounter samples that are diverged from the training distribution. Failure to recognize an out-of-distribution (OOD) sample, and consequently assign that sample to an in-class label significantly compromises the reliability of a model. The problem has gained significant attention due to its importance for safety deploying models in open-world settings. Detecting OOD samples is challenging due to the intractability of modeling all possible unknown distributions. To date, several research domains tackle the problem of detecting unfamiliar samples, including anomaly detection, novelty detection, one-class learning, open set recognition, and out-of-distribution detection. Despite having similar and shared concepts, out-of-distribution, open-set, and anomaly detection have been investigated independently. Accordingly, these research avenues have not cross-pollinated, creating research barriers. While some surveys intend to provide an overview of these approaches, they seem to only focus on a specific domain without examining the relationship between different domains. This survey aims to provide a cross-domain and comprehensive review of numerous eminent works in respective areas while identifying their commonalities. Researchers can benefit from the overview of research advances in different fields and develop future methodology synergistically. Furthermore, to the best of our knowledge, while there are surveys in anomaly detection or one-class learning, there is no comprehensive or up-to-date survey on out-of-distribution detection, which our survey covers extensively. Finally, having a unified cross-domain perspective, we discuss and shed light on future lines of research, intending to bring these fields closer together.",
        "references": [
            "19745a4ae79264948b168400463bfe50343a736a",
            "7437fb168cea92e1df8332ac618f7f07b071aca8",
            "04513c7c0b3a63fde81a996dae064a28d453c17a",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "acd87843a451d18b4dc6474ddce1ae946429eaf1",
            "4b83e9a048e677ff9fab226e8f7fbaa33fd32024",
            "34733eaf66007516347a40ad5d9bbe1cc9dacb6b",
            "4b04b42dc57b99060a3b9a870012659dca44054f",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "8381157eae4fbf8908d0312a9642f8e69e944449"
        ],
        "related_topics": [
            "Cross-domain Perspective",
            "Anomaly Detection",
            "Out-of-Distribution Detection",
            "Outof Distribution",
            "Samples",
            "Out-of-Distribution",
            "Open Set Recognition",
            "Training Distributions",
            "Novelty Detection",
            "Open-world Setting"
        ],
        "reference_count": "194",
        "citation_count": "115"
    },
    {
        "Id": "c7c1b005a042e52be8523ddf0d26941a242d5402",
        "title": "ASW-Net: A Deep Learning-based Tool for Cell Nucleus Segmentation of Fluorescence Microscopy",
        "authors": [
            "Weihao Pan",
            "Zhe Liu",
            "Guan Ning Lin"
        ],
        "date": "29 October 2021",
        "abstract": "This paper proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used, and showed that this lightweight model could reach remarkable segmentation performance in the testing set. Nucleus segmentation of fluorescence microscopy is a critical step in quantifying measurements in cell biology. Automatic and accurate nucleus segmentation has powerful applications in analyzing intrinsic characterization in nucleus morphology. However, existing methods have limited capacity to perform accurate segmentation in challenging samples, such as noisy images and clumped nuclei. In this paper, inspired by the idea of cascaded U-Net (or W-Net) and its remarkable performance improvement in medical image segmentation, we proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used. Results showed that this lightweight model could reach remarkable segmentation performance in the testing set (aggregated Jaccard index, 0.7981). In addition, our proposed framework performed better than the state-of-the-art methods in terms of segmentation performance. Moreover, we further explored the effectiveness of our designed network by visualizing the deep features from the network. Notably, our proposed framework is open-source.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "0ea3e5215ac2676a15bef2354b2938704a0789a3",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "29fbe4a6c55f8eae8ff40841440e4cb198cd9aec",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668"
        ],
        "related_topics": [
            "Nucleus Segmentation",
            "Segmentation Performance",
            "Deep Features",
            "Cascaded U-Net",
            "Medical Image Segmentation",
            "Noisy Image",
            "W-Net",
            "Deep Learning",
            "Aggregated Jaccard Index"
        ],
        "reference_count": "0",
        "citation_count": "35"
    },
    {
        "Id": "3a8150de52d52c3d3ac36a7db695b00949409942",
        "title": "Multi-Modality Microscopy Image Style Transfer for Nuclei Segmentation",
        "authors": [
            "Ye Liu",
            "Sophia J. Wagner",
            "Tingying Peng"
        ],
        "date": "23 November 2021",
        "abstract": "With the authors' style augmentation, the segmentation accuracy of the two top-ranked Mask R-CNN-based nuclei segmentation algorithms in the competition increases significantly and helps counteract class imbalance without resampling of minority classes. Annotating microscopy images for nuclei segmentation is laborious and time-consuming. To leverage the few existing annotations, also across multiple modalities, we propose a novel microscopy-style augmentation technique based on a generative adversarial network (GAN). Unlike other style transfer methods, it can not only deal with different cell assay types and lighting conditions, but also with different imaging modalities, such as bright-field and fluorescence microscopy. Using disentangled representations for content and style, we can preserve the structure of the original image while altering its style during augmentation. We evaluate our data augmentation on the 2018 Data Science Bowl dataset consisting of various cell assays, lighting conditions, and imaging modalities. With our style augmentation, the segmentation accuracy of the two top-ranked Mask R-CNN-based nuclei segmentation algorithms in the competition increases significantly. Thus, our augmentation technique renders the downstream task more robust to the test data heterogeneity and helps counteract class imbalance without resampling of minority classes.",
        "references": [
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "7b83fd62774adc795464ca3ad9b3c238c9ce5daf",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "1a0912bb76777469295bb2c059faee907e7f3258",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852",
            "3d5d9d8e74b215609eabba80ef79a35ebf460e49",
            "bd898f483476e3dcacf83cd85efc64e6319da0e1"
        ],
        "related_topics": [
            "Nuclei Segmentation",
            "Generative Adversarial Networks",
            "Style Augmentation",
            "Style Transfer Method",
            "Minority Classes",
            "Image Style Transfer",
            "Disentangled Representations",
            "Class Imbalance"
        ],
        "reference_count": "0",
        "citation_count": "9"
    },
    {
        "Id": "81f0e648e4776dcbe933bda553f6ac4e5b31876e",
        "title": "Learning with minimal effort: leveraging in silico labeling for cell and nucleus segmentation",
        "authors": [
            "Thomas Bonte",
            "Maxence Philbert",
            "Emeline Coleno",
            "Edouard Bertrand",
            "Arthur Imbert",
            "Thomas Walter"
        ],
        "date": "10 January 2023",
        "abstract": "This paper proposes to use In Silico Labeling (ISL) as a pretraining scheme for segmentation tasks, and shows that such a scheme can dramatically reduce the number of required annotations. Deep learning provides us with powerful methods to perform nucleus or cell segmentation with unprecedented quality. However, these methods usually require large training sets of manually annotated images, which are tedious and expensive to generate. In this paper we propose to use In Silico Labeling (ISL) as a pretraining scheme for segmentation tasks. The strategy is to acquire label-free microscopy images (such as bright-field or phase contrast) along fluorescently labeled images (such as DAPI or CellMask). We then train a model to predict the fluorescently labeled images from the label-free microscopy images. By comparing segmentation performance across several training set sizes, we show that such a scheme can dramatically reduce the number of required annotations.",
        "references": [
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "78db21fabea962592ac0aef54624251550929109",
            "a8e4afbba1f79d2291b3a02f61572b49ecbba982",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "a7c2b84df8a8f2f573efb0220e24853c3c16ca1a",
            "070a7bb5607fe6c81974190644ea62f119b146f4",
            "15caf7f2f7940ffc9cf90494bf9de496f6cf3a26"
        ],
        "related_topics": [
            "Nucleus Segmentation",
            "Segmentation Performance",
            "Pretraining Schemes",
            "Training Set",
            "Deep Learning",
            "Cell Segmentation",
            "Distributed Averaging Proportional Integral",
            "Inter-satellite Links"
        ],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "9fc2e80923c10f1619e51be64cf44516f55cd131",
        "title": "A Deep Learning Model for Automated Segmentation of Fluorescence Cell images",
        "authors": [
            "Musa Ayd\u0131n",
            "Berna Kiraz",
            "Furkan Eren",
            "Yi\u011fit Uysalh",
            "Berna Morova",
            "Selahattin Can Ozcan",
            "Ceyda Acilan",
            "Alper Kiraz"
        ],
        "date": "1 February 2022",
        "abstract": "This study develops a deep learning model for automated segmentation of fuorescence cell images, and applies it to fuorescence images recorded with a home-built epi-fuorescence microscope. Deep learning techniques bring together key advantages in biomedical image segmentation. They speed up the process, increase the reproducibility, and reduce the workload in segmentation and classifcation. Deep learning techniques can be used for analysing cell concentration, cell viability, as well as the size and form of each cell. In this study, we develop a deep learning model for automated segmentation of fuorescence cell images, and apply it to fuorescence images recorded with a home-built epi-fuorescence microscope. A deep neural network model based on U-Net architecture was built using a publicly available dataset of cell nuclei images [1]. A model accuracy of 97.3% was reached at the end of model training. Fluorescence cell images acquired with our home-built microscope were then segmented using the developed model. 141 of 151 cells in 5 images were successfully segmented, revealing a segmentation success rate of 93.4%. This deep learning model can be extended to the analysis of diferent cell types and cell viability.",
        "references": [
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "441ff323c92331e655ce9ff896773fc00b55089a",
            "adcda84902e4def22f89321768c535eff153d17d",
            "fde0b093a0f11cb5a49297252eb5e894505ea794",
            "907d6d24ddf4a74491f611be6fb57fd102c6d9b0",
            "aead79813d2f6c27ea931ab4070bb011b8f606a7",
            "ae81ea70a79661ed56cef139c6062b31fb4eba95",
            "aca1f456fc89c0aa38f931d4d0ba4f7a927373a4"
        ],
        "related_topics": [],
        "reference_count": "11",
        "citation_count": "2"
    },
    {
        "Id": "191d3915ce8f8edb9d38a5486798f84c9c7488a8",
        "title": "Multi-Modality Microscopy Image Style Augmentation for Nuclei Segmentation",
        "authors": [
            "Ye Liu",
            "Sophia J. Wagner",
            "Tingying Peng"
        ],
        "date": "1 March 2022",
        "abstract": "This work proposes a novel microscopy-style augmentation technique based on a generative adversarial network that can deal with different cell assay types and lighting conditions, but also with different imaging modalities, such as bright-field and fluorescence microscopy. Annotating microscopy images for nuclei segmentation by medical experts is laborious and time-consuming. To leverage the few existing annotations, also across multiple modalities, we propose a novel microscopy-style augmentation technique based on a generative adversarial network (GAN). Unlike other style transfer methods, it can not only deal with different cell assay types and lighting conditions, but also with different imaging modalities, such as bright-field and fluorescence microscopy. Using disentangled representations for content and style, we can preserve the structure of the original image while altering its style during augmentation. We evaluate our data augmentation on the 2018 Data Science Bowl dataset consisting of various cell assays, lighting conditions, and imaging modalities. With our style augmentation, the segmentation accuracy of the two top-ranked Mask R-CNN-based nuclei segmentation algorithms in the competition increases significantly. Thus, our augmentation technique renders the downstream task more robust to the test data heterogeneity and helps counteract class imbalance without resampling of minority classes.",
        "references": [
            "7b83fd62774adc795464ca3ad9b3c238c9ce5daf",
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "1d97f37cbec5e6796dd8eacf72ba6868e20a51e1",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "7dfa3f6eab0f1c953a19712534b2bfa9d28c47b8",
            "3fd2fd728c4bf3f891a1b6bfe9f44bda70029a4b",
            "8acbe90d5b852dadea7810345451a99608ee54c7",
            "1a0912bb76777469295bb2c059faee907e7f3258"
        ],
        "related_topics": [],
        "reference_count": "20",
        "citation_count": "4"
    },
    {
        "Id": "86e92a4b836db293a7a63d81e2a400846088648a",
        "title": "Segmentor: a tool for manual refinement of 3D microscopy annotations",
        "authors": [
            "David Borland",
            "Carolyn M. McCormick",
            "Niyanta K. Patel",
            "Oleh Krupa",
            "Jessica T. Mory",
            "Alvaro A. Beltran",
            "Tala M. Farah",
            "Carla F. Escobar-Tomlienovich",
            "Sydney S. Olson",
            "Minjeong Kim",
            "Guorong Wu",
            "Jason L. Stein"
        ],
        "date": "27 January 2021",
        "abstract": "It is shown that editing simultaneously in 2D and 3D using Segmentor significantly decreases time spent on manual annotations without affecting accuracy as compared to editing the same set of images with only 2D capabilities. Recent advances in tissue clearing techniques, combined with high-speed image acquisition through light sheet microscopy, enable rapid three-dimensional (3D) imaging of biological specimens, such as whole mouse brains, in a matter of hours. Quantitative analysis of such 3D images can help us understand how changes in brain structure lead to differences in behavior or cognition, but distinguishing densely packed features of interest, such as nuclei, from background can be challenging. Recent deep learning-based nuclear segmentation algorithms show great promise for automated segmentation, but require large numbers of accurate manually labeled nuclei as training data. We present Segmentor, an open-source tool for reliable, efficient, and user-friendly manual annotation and refinement of objects (e.g., nuclei) within 3D light sheet microscopy images. Segmentor employs a hybrid 2D-3D approach for visualizing and segmenting objects and contains features for automatic region splitting, designed specifically for streamlining the process of 3D segmentation of nuclei. We show that editing simultaneously in 2D and 3D using Segmentor significantly decreases time spent on manual annotations without affecting accuracy as compared to editing the same set of images with only 2D capabilities. Segmentor is a tool for increased efficiency of manual annotation and refinement of 3D objects that can be used to train deep learning segmentation algorithms, and is available at https://www.nucleininja.org/ and https://github.com/RENCI/Segmentor.",
        "references": [
            "3a840fbedbafbd436b6b2b85cd4fceb3ec006e01",
            "7e4f3d338e25add155abd339f9ed51ca5c6b1b3d",
            "3ad4b29906c8987bac46ea0ea9586b58a38fa8f4",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "fbf309595262bdadfebab64a5b868811716814b2",
            "b3816c29d0a696b4c562284b99bda3d906060d55",
            "a9c066cbba03b5fda843db3ab20a9ae212cd89d2",
            "fa1f107a5bfc66bc8b7430f612bd3a0c27b00304"
        ],
        "related_topics": [
            "Segmentors",
            "3D Segmentation",
            "Editing",
            "Whole Mouse Brain",
            "Deep Learning"
        ],
        "reference_count": "30",
        "citation_count": "11"
    },
    {
        "Id": "2b40eabd11fe435dfa56bb0f8a6434b0dcccb5bf",
        "title": "Automatic improvement of deep learning-based cell segmentation in time-lapse microscopy by neural architecture search",
        "authors": [
            "Yanming Zhu",
            "Erik H. W. Meijering"
        ],
        "date": "30 July 2021",
        "abstract": "This work proposes a novel NAS-based solution for deep learning-based cell segmentation in time-lapse microscopy images by jointly searching non-repeatable micro architectures to construct the macro network for exploring greater NAS potential and better performance. Abstract Motivation Live cell segmentation is a crucial step in biological image analysis and is also a challenging task because time-lapse microscopy cell sequences usually exhibit complex spatial structures and complicated temporal behaviors. In recent years, numerous deep learning-based methods have been proposed to tackle this task and obtained promising results. However, designing a network with excellent performance requires professional knowledge and expertise and is very time-consuming and labor-intensive. Recently emerged neural architecture search (NAS) methods hold great promise in eliminating these disadvantages, because they can automatically search an optimal network for the task. Results We propose a novel NAS-based solution for deep learning-based cell segmentation in time-lapse microscopy images. Different from current NAS methods, we propose (i) jointly searching non-repeatable micro architectures to construct the macro network for exploring greater NAS potential and better performance and (ii) defining a specific search space suitable for the live cell segmentation task, including the incorporation of a convolutional long short-term memory network for exploring the temporal information in time-lapse sequences. Comprehensive evaluations on the 2D datasets from the cell tracking challenge demonstrate the competitiveness of the proposed method compared to the state of the art. The experimental results show that the method is capable of achieving more consistent top performance across all ten datasets than the other challenge methods. Availabilityand implementation The executable files of the proposed method as well as configurations for each dataset used in the presented experiments will be available for non-commercial purposes from https://github.com/291498346/nas_cellseg. Supplementary information Supplementary data are available at Bioinformatics online.",
        "references": [
            "ec5a566be5cb54a8729343e2bb54ea262a3ec650",
            "89bcdd2fa6c058b32ad76da1b8ca247213dd3bbb",
            "f12ca74c2e0be7f13bdcdae6e4a4944de858ebdb",
            "75bdaacf355d0a6d52d9a4a8790e1fdc936eb867",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "866eef39af3ccfa7d7f69c57bf6ec834461b6702",
            "2630db1a63fb32751f2f8c9f256d5f1bb07e8961",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "a76e9cc1405f92af1f3a99c3cd713c6dfb197db5"
        ],
        "related_topics": [
            "Neural Architecture Search",
            "Cell Tracking Challenge",
            "Deep Learning",
            "Cell Segmentation"
        ],
        "reference_count": "41",
        "citation_count": "11"
    },
    {
        "Id": "3ad4b29906c8987bac46ea0ea9586b58a38fa8f4",
        "title": "AnnotatorJ: an ImageJ plugin to ease hand annotation of cellular compartments",
        "authors": [
            "R{\\&#x27;e}ka Hollandi",
            "Akos Diosdi",
            "G{\\&#x27;a}bor Hollandi",
            "Nikita Moshkov",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th"
        ],
        "date": "27 February 2020",
        "abstract": "AnnotatorJ, an ImageJ plugin for the semi-automatic annotation of cells (or generally, objects of interest) on microscopy images in 2D that helps find the true contour of individual objects by applying U-Net-based pre-segmentation, enables users to create datasets that could potentially increase the accuracy of state-of-the-art solutions, deep learning or otherwise, when used as training data. AnnotatorJ combines single-cell identification with deep learning and manual annotation. Cellular analysis quality depends on accurate and reliable detection and segmentation of cells so that the subsequent steps of analyses e.g. expression measurements may be carried out precisely and without bias. Deep learning has recently become a popular way of segmenting cells, performing unimaginably better than conventional methods. However, such deep learning applications may be trained on a large amount of annotated data to be able to match the highest expectations. High-quality annotations are unfortunately expensive as they require field experts to create them, and often cannot be shared outside the lab due to medical regulations. We propose AnnotatorJ, an ImageJ plugin for the semi-automatic annotation of cells (or generally, objects of interest) on (not only) microscopy images in 2D that helps find the true contour of individual objects by applying U-Net-based pre-segmentation. The manual labour of hand-annotating cells can be significantly accelerated by using our tool. Thus, it enables users to create such datasets that could potentially increase the accuracy of state-of-the-art solutions, deep learning or otherwise, when used as training data.",
        "references": [
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "2b0e0080bb545b8e8f69895bf980b28aea065554",
            "8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8",
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "e07e61de05d30a348cb7b4cb38aede541f80941c",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7"
        ],
        "related_topics": [
            "AnnotatorJ",
            "Deep Learning",
            "ImageJ"
        ],
        "reference_count": "50",
        "citation_count": "27"
    },
    {
        "Id": "e33f7928a45e395f1bc2a1aa1dd9349bc725ba24",
        "title": "An Integrative Segmentation Framework for Cell Nucleus of Fluorescence Microscopy",
        "authors": [
            "Weihao Pan",
            "Zhe Liu",
            "Weichen Song",
            "Xu Zhen",
            "Kai Yuan",
            "Fei Xu",
            "Guan Ning Lin"
        ],
        "date": "26 February 2022",
        "abstract": "This paper proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used, and showed that this lightweight model could reach remarkable segmentation performance in the BBBC039 testing set. Nucleus segmentation of fluorescence microscopy is a critical step in quantifying measurements in cell biology. Automatic and accurate nucleus segmentation has powerful applications in analyzing intrinsic characterization in nucleus morphology. However, existing methods have limited capacity to perform accurate segmentation in challenging samples, such as noisy images and clumped nuclei. In this paper, inspired by the idea of cascaded U-Net (or W-Net) and its remarkable performance improvement in medical image segmentation, we proposed a novel framework called Attention-enhanced Simplified W-Net (ASW-Net), in which a cascade-like structure with between-net connections was used. Results showed that this lightweight model could reach remarkable segmentation performance in the BBBC039 testing set (aggregated Jaccard index, 0.90). In addition, our proposed framework performed better than the state-of-the-art methods in terms of segmentation performance. Moreover, we further explored the effectiveness of our designed network by visualizing the deep features from the network. Notably, our proposed framework is open source.",
        "references": [
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "0ea3e5215ac2676a15bef2354b2938704a0789a3",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "4a91c15880a788711c0a7e00ba3968580e3052a5"
        ],
        "related_topics": [],
        "reference_count": "35",
        "citation_count": "3"
    },
    {
        "Id": "b751ecf715dce14db2ad9410d60038c1b92e38f0",
        "title": "Dice-XMBD: Deep Learning-Based Cell Segmentation for Imaging Mass Cytometry",
        "authors": [
            "Xu Xiao",
            "Ying Qiao",
            "Yudi Jiao",
            "Na Fu",
            "Wenxian Yang",
            "Liansheng Wang",
            "Rongshan Yu",
            "Jiahuai Han"
        ],
        "date": "7 June 2021",
        "abstract": "Dice-XMBD, a Deep learnIng-based Cell sEgmentation algorithm for tissue multiplexed imaging data generates more accurate single cell masks efficiently on IMC images produced with different nuclear, membrane and cytoplasm markers. Highly multiplexed imaging technology is a powerful tool to facilitate understanding cells composition and interaction in tumor microenvironment at subcellular resolution, which is crucial for both basic research and clinical applications. Imaging mass cytometry (IMC), a multiplex imaging method recently introduced, can measure up to 40 markers simultaneously in one tissue section by using a high-resolution laser with a mass cytometer. However, due to its high resolution and large number of channels, how to process and interpret the image data from IMC remains a key challenge for its further applications. Accurate and reliable single cell segmentation is the first and a critical step to process IMC image data. Unfortunately, existing segmentation pipelines either produce inaccurate cell segmentation results, or require manual annotation which is very time-consuming. Here, we developed Dice-XMBD, a Deep learnIng-based Cell sEgmentation algorithm for tissue multiplexed imaging data. In comparison with other state-of-the-art cell segmentation methods currently used in IMC, Dice-XMBD generates more accurate single cell masks efficiently on IMC images produced with different nuclear, membrane and cytoplasm markers. All codes and datasets are available at https://github.com/xmuyulab/Dice-XMBD.",
        "references": [
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "40af8e077b162551888ea348d16f458a12c147e5",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "01dd8c73df2b37b2e02a9e25612ed86a06f25cc5",
            "4c805a6daef0cab96d734172cbfa02f057ae8135",
            "77cf8e02fe7664e4df70fbe2c0704bfcce9e43fc",
            "30d06a109c55dacfd84028bf466ca3884c384392",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "75284cab476321603c5aa0e890e59ba2d743617c",
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf"
        ],
        "related_topics": [],
        "reference_count": "47",
        "citation_count": "12"
    },
    {
        "Id": "c89bfd998b0a6c656010b629814ab0cad3cff72e",
        "title": "Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images",
        "authors": [
            "Juan C. Caicedo",
            "Jonathan F Roth",
            "Allen Goodman",
            "Tim Becker",
            "Kyle W. Karhohs",
            "Claire McQuin",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "31 May 2018",
        "abstract": "This work presents an evaluation framework to measure accuracy, types of errors, and computational efficiency; and uses it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. Identifying nuclei is often a critical first step in analyzing microscopy images of cells, and classical image processing algorithms are most commonly used for this task. Recent developments in deep learning can yield superior accuracy, but typical evaluation metrics for nucleus segmentation do not satisfactorily capture error modes that are relevant in cellular images. Besides, large image data sets with ground truth for evaluation have been limiting. We present an evaluation framework to measure accuracy, types of errors, and computational efficiency; and use it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. We publicly release a set of 23,165 manually annotated nuclei and source code to reproduce experiments. Our results show that U-Net outperforms both pixel-wise classification networks and classical algorithms. Also, our evaluation framework shows that deep learning improves accuracy and reduces the number of biologically relevant errors by half.",
        "references": [
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "cdecac6e2f578cfc56140e00aaa74a78f864fea2",
            "318f82a3e593e391cfd0da7964b16d83299aa943",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "1268de7bda769e651dc6c089d006c7edbe37f563",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "36748de338909976f72ffbadaf097470ec040da0"
        ],
        "related_topics": [
            "DeepCell",
            "Broad Bioimage Benchmark Collection",
            "Deep Learning",
            "Nucleus Segmentation",
            "International Society For Advancement Of Cytometry",
            "Cytometry"
        ],
        "reference_count": "53",
        "citation_count": "226"
    },
    {
        "Id": "0ea3e5215ac2676a15bef2354b2938704a0789a3",
        "title": "Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images",
        "authors": [
            "Lipeng Xie",
            "Jin Qi",
            "Lili Pan",
            "Samad Wali"
        ],
        "date": "1 February 2020",
        "abstract": "Semantic Scholar extracted view of \"Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images\" by Lipeng Xie et al.",
        "references": [
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "78db21fabea962592ac0aef54624251550929109",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "3ba787154b2411fe89d88b6ab50ea84aa62dea04",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "0aa9c29e32cb13b2d011e5730bd4955c7962083b"
        ],
        "related_topics": [
            "Marker-Controlled Watershed",
            "Nuclei Segmentation",
            "Histopathology Images",
            "Instance Segmentation"
        ],
        "reference_count": "44",
        "citation_count": "43"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "4a91c15880a788711c0a7e00ba3968580e3052a5",
        "title": "RIC-Unet: An Improved Neural Network Based on Unet for Nuclei Segmentation in Histology Images",
        "authors": [
            "Zitao Zeng",
            "Weihao Xie",
            "Yunzhe Zhang",
            "Yao Lu"
        ],
        "date": "1 February 2019",
        "abstract": "The techniques of residual blocks, multi-scale and channel attention mechanism are applied on RIC-Unet to segment nuclei more accurately and the method won the third place in the computational precision medicine nuclei segmentation challenge together with MICCAI 2018. As a prerequisite for cell detection, cell classification, and cancer grading, nuclei segmentation in histology images has attracted wide attention in recent years. It is quite a challenging task due to the diversity in staining procedure, cell morphology, and cell arrangement between different histopathology images, especially with different color contrasts. In this paper, an Unet-based neural network, RIC-Unet (residual-inception-channel attention-Unet), for nuclei segmentation is proposed. The techniques of residual blocks, multi-scale and channel attention mechanism are applied on RIC-Unet to segment nuclei more accurately. RIC-Unet is compared with two traditional segmentation methods: CP and Fiji, two original CNN methods: CNN2, CNN3, and original U-net on The Cancer Genomic Atlas (TCGA) dataset. Besides, in this paper, we use Dice, F1-score, and aggregated Jaccard index to evaluate these methods. The average of RIC-Unet and U-net on these three indicators are 0.8008 versus 0.7844, 0.8278 versus 0.8155, and 0.5635 versus 0.5462. In addition, our method won the third place in the computational precision medicine nuclei segmentation challenge together with MICCAI 2018.",
        "references": [
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "ba124bbde8d114a9103eb5751036e2e4710e2fff",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "acf6f783fe65fb0a034348ee2a9a1e5297946963",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "3d7cd4a1f626dae9ae41bb5e6c764020df21f12e",
            "e729fc95d04f32d9353f3c63a2ab3a04eef17a0b",
            "de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
            "cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "88513e738a95840de05a62f0e43d30a67b3c542e"
        ],
        "related_topics": [
            "RIC-Unet",
            "Nuclei Segmentation",
            "U-Net",
            "Histology Images",
            "Staining Procedures",
            "Color Contrast",
            "Residual Blocks",
            "Histopathology Images",
            "Aggregated Jaccard Index",
            "The Cancer Genomic Atlas"
        ],
        "reference_count": "27",
        "citation_count": "197"
    },
    {
        "Id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "date": "18 May 2015",
        "abstract": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "references": [
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "eb42cf88027de515750f230b23b1a057dc782108",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "4f03888450fde9e8234f616badaae499740e57a4",
            "3b2ccc97f1433cf8750a2ad5a05555ccd10e9cdf",
            "3bffa23a16c273ac2228a13e65dade6766ce7777"
        ],
        "related_topics": [
            "U-Net",
            "Contracting Path",
            "ISBI Cell Tracking Challenge 2015",
            "Sliding-window Convolutional Network",
            "Neuronal Structures",
            "Expansive Path",
            "Biomedical Segmentation",
            "ISBI 2012",
            "U-net Architecture",
            "Drosophila First Instar Larva Ventral Nerve Cord"
        ],
        "reference_count": "18",
        "citation_count": "56,807"
    },
    {
        "Id": "5986547c7380f5a8fb6028093f827b3662f838a2",
        "title": "Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images",
        "authors": [
            "Simon Graham",
            "Quoc Dang Vu",
            "Shan E. Ahmed Raza",
            "Ayesha S Azam",
            "Yee-Wah Tsang",
            "Jin Tae Kwak",
            "Nasir M. Rajpoot"
        ],
        "date": "16 December 2018",
        "abstract": "Semantic Scholar extracted view of \"Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images\" by S. Graham et al.",
        "references": [
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "e312652daf82ed144d1696aae7ab412030d4f7eb",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "78db21fabea962592ac0aef54624251550929109",
            "9eb4f09a7de530a4b6ee1bd8bafed229275e4527",
            "114a42b24290829c16550781ec1a2d6c09fc969e",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102"
        ],
        "related_topics": [
            "HoVer-Net",
            "Nuclear Pixels",
            "CoNSeP",
            "Nuclear Instance Segmentation",
            "CPM-17",
            "Nuclear Segmentation",
            "Computational Pathology",
            "Nuclear Instances",
            "Nuclear Type",
            "Nuclear Classification"
        ],
        "reference_count": "49",
        "citation_count": "623"
    },
    {
        "Id": "29fbe4a6c55f8eae8ff40841440e4cb198cd9aec",
        "title": "W-net: Bridged U-net for 2D Medical Image Segmentation",
        "authors": [
            "Wanli Chen",
            "Yue Zhang",
            "Junjun He",
            "Yu Qiao",
            "Yifan Chen",
            "Hongjian Shi",
            "Xiaoying Tang"
        ],
        "date": "12 July 2018",
        "abstract": "A deeper network that can fit medical image datasets that are usually small in the sample size is proposed and validated and a new loss function to accelerate the learning process and a combination of different activation functions to improve the network performance are proposed. In this paper, we focus on three problems in deep learning based medical image segmentation. Firstly, U-net, as a popular model for medical image segmentation, is difficult to train when convolutional layers increase even though a deeper network usually has a better generalization ability because of more learnable parameters. Secondly, the exponential ReLU (ELU), as an alternative of ReLU, is not much different from ReLU when the network of interest gets deep. Thirdly, the Dice loss, as one of the pervasive loss functions for medical image segmentation, is not effective when the prediction is close to ground truth and will cause oscillation during training. To address the aforementioned three problems, we propose and validate a deeper network that can fit medical image datasets that are usually small in the sample size. Meanwhile, we propose a new loss function to accelerate the learning process and a combination of different activation functions to improve the network performance. Our experimental results suggest that our network is comparable or superior to state-of-the-art methods.",
        "references": [
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "7fc464470b441c691d10e7331b14a525bc79b8bb",
            "00fc2f71a3fc3e67d8cc96b9a345f6b223c64aa6",
            "cc83eabf4f833b8c92dcf6012dcce348591d060f",
            "047efd37f34a916d50dbacf4c52afd968389e5e1",
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "5694e46284460a648fe29117cbc55f6c9be3fa3c"
        ],
        "related_topics": [
            "U-Net",
            "Medical Image Segmentation",
            "W-Net",
            "PROMISE12 Challenge",
            "Convergence",
            "Prostate",
            "Conv",
            "Stacked U-Nets",
            "Exponential Linear Units",
            "Rectified Linear Units"
        ],
        "reference_count": "18",
        "citation_count": "25"
    },
    {
        "Id": "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
        "title": "A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology",
        "authors": [
            "Neeraj Kumar",
            "Ruchika Verma",
            "Sanuj Sharma",
            "S. K. Bhargava",
            "Abhishek Vahadane",
            "Amit Sethi"
        ],
        "date": "6 March 2017",
        "abstract": "A large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries is introduced, whose quality was validated by a medical doctor. Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.",
        "references": [
            "e312652daf82ed144d1696aae7ab412030d4f7eb",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "6c1108c63693a08da9d8fb418b14289bae73cc66",
            "13de33ee941f1ebf3ed185c20fb4453a07302c30",
            "930b28f5255a6633ec89bdafb9f72af8e9148fac",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "856a3c76453a798556096cd23848402d1351c1f9"
        ],
        "related_topics": [
            "Nuclear Segmentation",
            "Digital Microscopic Tissue Images",
            "Nuclear Morphometrics",
            "Nuclear Boundaries",
            "Computational Pathology",
            "Overlapping Nuclei",
            "Machine Learning",
            "Eosin",
            "Deep Learning",
            "Object Recognition"
        ],
        "reference_count": "46",
        "citation_count": "663"
    },
    {
        "Id": "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
        "title": "Methods for Segmentation and Classification of Digital Microscopy Tissue Images",
        "authors": [
            "Quoc Dang Vu",
            "Simon Graham",
            "Minh Nguyen Nhat To",
            "Muhammad Shaban",
            "Talha Qaiser",
            "Navid Alemi Koohbanani",
            "Syed Ali Khurram",
            "Tahsin M. Kur\u00e7",
            "Keyvan Farahani",
            "Tianhao Zhao",
            "Rajarsi R. Gupta",
            "Jin Tae Kwak",
            "Nasir M. Rajpoot",
            "J. Saltz"
        ],
        "date": "31 October 2018",
        "abstract": "Two computer algorithms are presented; one designed for segmentation of nuclei and the other for classification of whole slide tissue images, both of which were evaluated in the MICCAI 2017 Digital Pathology challenge. High-resolution microscopy images of tissue specimens provide detailed information about the morphology of normal and diseased tissue. Image analysis of tissue morphology can help cancer researchers develop a better understanding of cancer biology. Segmentation of nuclei and classification of tissue images are two common tasks in tissue image analysis. Development of accurate and efficient algorithms for these tasks is a challenging problem because of the complexity of tissue morphology and tumor heterogeneity. In this paper we present two computer algorithms; one designed for segmentation of nuclei and the other for classification of whole slide tissue images. The segmentation algorithm implements a multiscale deep residual aggregation network to accurately segment nuclear material and then separate clumped nuclei into individual nuclei. The classification algorithm initially carries out patch-level classification via a deep learning method, then patch-level statistical and morphological features are used as input to a random forest regression model for whole slide image classification. The segmentation and classification algorithms were evaluated in the MICCAI 2017 Digital Pathology challenge. The segmentation algorithm achieved an accuracy score of 0.78. The classification algorithm achieved an accuracy score of 0.81. These scores were the highest in the challenge.",
        "references": [
            "ee6711484a60756877d8ef06c3f3179c73dcbb20",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "39a5e190eaa6325fa3ed93d2e6fe55bdddc2dded",
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "91dc37e4da622afe495f95aba708611087fbf4aa",
            "004ef85f9c53dc0db6dc3adc1ca8ecb71f7a06b4",
            "33252a1699675af76a3c420bd5693ff1a75055b2",
            "bd898f483476e3dcacf83cd85efc64e6319da0e1"
        ],
        "related_topics": [
            "Whole Slide Tissue Images",
            "Classification"
        ],
        "reference_count": "53",
        "citation_count": "190"
    },
    {
        "Id": "a195a2c234ef7a72cc448cb9b58404ff83077f65",
        "title": "Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy",
        "authors": [
            "Tim Scherr",
            "Katharina L{\\&quot;o}ffler",
            "Moritz B{\\&quot;o}hland",
            "Ralf Mikut"
        ],
        "date": "3 April 2020",
        "abstract": "This paper presents a method for the segmentation of touching cells in microscopy images using a novel representation of cell borders, inspired by distance maps, which is capable to utilize not only touching cells but also close cells in the training process. The accurate segmentation and tracking of cells in microscopy image sequences is an important task in biomedical research, e.g., for studying the development of tissues, organs or entire organisms. However, the segmentation of touching cells in images with a low signal-to-noise-ratio is still a challenging problem. In this paper, we present a method for the segmentation of touching cells in microscopy images. By using a novel representation of cell borders, inspired by distance maps, our method is capable to utilize not only touching cells but also close cells in the training process. Furthermore, this representation is notably robust to annotation errors and shows promising results for the segmentation of microscopy images containing in the training data underrepresented or not included cell types. For the prediction of the proposed neighbor distances, an adapted U-Net convolutional neural network (CNN) with two decoder paths is used. In addition, we adapt a graph-based cell tracking algorithm to evaluate our proposed method on the task of cell tracking. The adapted tracking algorithm includes a movement estimation in the cost function to re-link tracks with missing segmentation masks over a short sequence of frames. Our combined tracking by detection method has proven its potential in the IEEE ISBI 2020 Cell Tracking Challenge (http://celltrackingchallenge.net/) where we achieved as team KIT-Sch-GE multiple top three rankings including two top performances using a single segmentation model for the diverse data sets.",
        "references": [
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "f7eac8600e31bbd1371601a694b188858b657af0",
            "01dd8c73df2b37b2e02a9e25612ed86a06f25cc5",
            "3bffa23a16c273ac2228a13e65dade6766ce7777",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "2f62e40170265ffb1d0c79b6539d71457456880b",
            "f3bdd04e9b71c76ee4ae8f476740685e65ee0f0e",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "23e78e98e17d21d9b1c57cbe3e24ce47b1e4048c",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7"
        ],
        "related_topics": [
            "Convolutional Neural Network",
            "Sequence",
            "Cell Segmentation",
            "Cost Functions"
        ],
        "reference_count": "42",
        "citation_count": "49"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "4cc5ac4a27a4c33722173c8b42aab1009b7bb793",
        "title": "Semi-Supervised Medical Image Segmentation via Cross Teaching between CNN and Transformer",
        "authors": [
            "Xiangde Luo",
            "Minhao Hu",
            "Tao Song",
            "Guotai Wang",
            "Shaoting Zhang"
        ],
        "date": "9 December 2021",
        "abstract": "This work presents a very simple yet efficient framework for semi-supervised medical image segmentation by introducing the cross teaching between CNN and Transformer by simplifying the classical deep co-training from consistency regularization to cross teaching. Recently, deep learning with Convolutional Neural Networks (CNNs) and Transformers has shown encouraging results in fully supervised medical image segmentation. However, it is still challenging for them to achieve good performance with limited annotations for training. In this work, we present a very simple yet efficient framework for semi-supervised medical image segmentation by introducing the cross teaching between CNN and Transformer. Specifically, we simplify the classical deep co-training from consistency regularization to cross teaching, where the prediction of a network is used as the pseudo label to supervise the other network directly end-to-end. Considering the difference in learning paradigm between CNN and Transformer, we introduce the Cross Teaching between CNN and Transformer rather than just using CNNs. Experiments on a public benchmark show that our method outperforms eight existing semi-supervised learning methods just with a simpler framework. Notably, this work may be the first attempt to combine CNN and transformer for semi-supervised medical image segmentation and achieve promising results on a public benchmark. The code will be released at: https://github.com/HiLab-git/SSL4MIS.",
        "references": [
            "8356d155d730e374f4db6dfd03d19a7b66c348a8",
            "f356edffeb43bc2e1050071e59cd718f5b0f60a5",
            "0fde3f0ef001b5d6eb9d71970ba6d0f0b9a653cb",
            "ea7cfe7f2340584cbe653da6077ee7c213e49b92",
            "88a85c56aac5e926f020851c21ba3e82d6b677c8",
            "1c8fd5102798a26270d98501703e38464ba4b805",
            "24b8a0b02bcb7934967757fc59d273a71ba67e30",
            "076a8e778f2e9efb3c2fd45fed534ae9e6035f1b",
            "ad1bbb2cc0fc7ea460cb8af6bf0c6e10cbffb66b",
            "0a56ba8eaf9fc83363786cb6d1da0b13efffd285"
        ],
        "related_topics": [
            "Cross Teaching",
            "Semi-supervised Medical Image Segmentation",
            "Cross Pseudo Supervision",
            "Convolutional Neural Network",
            "Transformer",
            "Medical Image Segmentation",
            "Deep Co-Training",
            "Deep Learning",
            "Consistency Regularization",
            "Pseudo Labels"
        ],
        "reference_count": "46",
        "citation_count": "92"
    },
    {
        "Id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "date": "18 May 2015",
        "abstract": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "references": [
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "eb42cf88027de515750f230b23b1a057dc782108",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "4f03888450fde9e8234f616badaae499740e57a4",
            "3b2ccc97f1433cf8750a2ad5a05555ccd10e9cdf",
            "3bffa23a16c273ac2228a13e65dade6766ce7777"
        ],
        "related_topics": [
            "U-Net",
            "Contracting Path",
            "ISBI Cell Tracking Challenge 2015",
            "Sliding-window Convolutional Network",
            "Neuronal Structures",
            "Expansive Path",
            "Biomedical Segmentation",
            "ISBI 2012",
            "U-net Architecture",
            "Drosophila First Instar Larva Ventral Nerve Cord"
        ],
        "reference_count": "18",
        "citation_count": "56,807"
    },
    {
        "Id": "40af8e077b162551888ea348d16f458a12c147e5",
        "title": "Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning",
        "authors": [
            "Noah F. Greenwald",
            "Geneva Miller",
            "Erick Moen",
            "Alex Kong",
            "Adam Kagel",
            "Thomas Dougherty",
            "Christine Camacho Fullaway",
            "Brianna J. McIntosh",
            "Kee Sing. Leow",
            "Morgan Schwartz",
            "Cole Pavelchek",
            "Sunny Cui",
            "I. Camplisson",
            "Omer Bar-Tal",
            "Jaiveer Singh",
            "Mara Fong",
            "Gautam Chaudhry",
            "Zion Abraham",
            "John Moseley",
            "Shiri Warshawsky",
            "Erin Soon",
            "Shirley Greenbaum",
            "Tyler T. Risom",
            "Travis J. Hollmann",
            "Sean C. Bendall",
            "Leeat Keren",
            "William Graf",
            "Michael Angelo",
            "David Van Valen"
        ],
        "date": "2 March 2021",
        "abstract": "This work constructed TissueNet, a dataset for training segmentation models that contains more than 1\u2009million manually labeled cells, an order of magnitude more than all previously published segmentation training datasets, and used it to train Mesmer, a deep-learning-enabled segmentation algorithm. A principal challenge in the analysis of tissue imaging data is cell segmentation\u2014the task of identifying the precise boundary of every cell in an image. To address this problem we constructed TissueNet, a dataset for training segmentation models that contains more than 1\u2009million manually labeled cells, an order of magnitude more than all previously published segmentation training datasets. We used TissueNet to train Mesmer, a deep-learning-enabled segmentation algorithm. We demonstrated that Mesmer is more accurate than previous methods, generalizes to the full diversity of tissue types and imaging platforms in TissueNet, and achieves human-level performance. Mesmer enabled the automated extraction of key cellular features, such as subcellular localization of protein signal, which was challenging with previous approaches. We then adapted Mesmer to harness cell lineage information in highly multiplexed datasets and used this enhanced version to quantify cell morphology changes during human gestation. All code, data and models are released as a community resource. Deep learning algorithms perform as well as humans in identifying cells in tissue images.",
        "references": [
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "20c97e24db99e4667813051dc1fa3e8f5b4d4983",
            "0ea25f1599fd520c1df29d2bde83f09cb53fb2da",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "9c32a3dfad76a4dae4ed347cde0f19ac07de4c04",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "87c0dd990287d92796c7dc83edba6f52a2f52e21"
        ],
        "related_topics": [],
        "reference_count": "95",
        "citation_count": "301"
    },
    {
        "Id": "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
        "title": "Cellpose: a generalist algorithm for cellular segmentation",
        "authors": [
            "Carsen Stringer",
            "Tim Wang",
            "Michalis Michaelos",
            "Marius Pachitariu"
        ],
        "date": "3 February 2020",
        "abstract": "This work introduces a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly. Cellpose is a generalist, deep learning-based approach for segmenting structures in a wide range of image types. Cellpose does not require parameter adjustment or model retraining and outperforms established methods on 2D and 3D datasets.",
        "references": [
            "f12ca74c2e0be7f13bdcdae6e4a4944de858ebdb",
            "7b79a48b5985ea85ac7f38bb579b4824a7ba1fcc",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "5e1335f1cf45fb4c29a0697f65c1e5c8e26d7dd4",
            "0a9966e8c21fb96de384daf445d311bdbdd74993",
            "268241fd604918a86e1b27e1a880d47e0ecc2d5b",
            "da895fa70d45ba06efae6cb128e885ab49c5e977",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a"
        ],
        "related_topics": [
            "Cellpose",
            "Cellpose Model",
            "StarDist",
            "nucleAIzer",
            "Ilastik",
            "Deep Learning"
        ],
        "reference_count": "73",
        "citation_count": "1,157"
    },
    {
        "Id": "a2aa5a08ad6d5e6432f0468e077e523430cf46a1",
        "title": "Meta multi-task nuclei segmentation with fewer training samples",
        "authors": [
            "Chu Han",
            "Huasheng Yao",
            "Bingchao Zhao",
            "Zhenhui Li",
            "Zhenwei Shi",
            "Lei Wu",
            "Xin Chen",
            "Jin Qu",
            "Ke Zhao",
            "Rushi Lan",
            "Changhong Liang",
            "Xipeng Pan",
            "Zaiyi Liu"
        ],
        "date": "1 May 2022",
        "abstract": "Semantic Scholar extracted view of \"Meta multi-task nuclei segmentation with fewer training samples\" by Chu Han et al.",
        "references": [
            "2e71e26bb816c6729e955e3c9e2b1c14906e73b4",
            "b893086a41be6fbe191cc04540a7c6c17bc80b71",
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "ba124bbde8d114a9103eb5751036e2e4710e2fff",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb",
            "a89cf9c6aa4cad62a78421916726b8b16c0cb9f2",
            "f77ae2f69b7432d9505d737c6209661f7be1eeb4",
            "a5782ff434f20c13e6ce689b698dfe3446db3a30",
            "99ebeb354c9538539e48c605446739007e9f4926"
        ],
        "related_topics": [
            "Model Agnostic Meta Learning",
            "Generalization",
            "State-of-the-art Models",
            "Fast Adaptation",
            "Pathologist",
            "Nuclei Segmentation",
            "Meta Multi-Task Learning",
            "Interaction Blocks",
            "Unseen Domains"
        ],
        "reference_count": "47",
        "citation_count": "15"
    },
    {
        "Id": "c616b0fe93dc0b52b04ce6bf68ae3a67701200e8",
        "title": "Weakly Supervised Deep Nuclei Segmentation Using Partial Points Annotation in Histopathology Images",
        "authors": [
            "Hui Qu",
            "Pengxiang Wu",
            "Qiaoying Huang",
            "Jingru Yi",
            "Zhennan Yan",
            "Kang Li",
            "Gregory M. Riedlinger",
            "Subhajyoti De",
            "Shaoting Zhang",
            "Dimitris N. Metaxas"
        ],
        "date": "15 June 2020",
        "abstract": "A novel weakly supervised segmentation framework based on partial points annotation that can achieve competitive performance compared to the fully supervised counterpart and the state-of-the-art methods while requiring significantly less annotation effort. Nuclei segmentation is a fundamental task in histopathology image analysis. Typically, such segmentation tasks require significant effort to manually generate accurate pixel-wise annotations for fully supervised training. To alleviate such tedious and manual effort, in this paper we propose a novel weakly supervised segmentation framework based on partial points annotation, i.e., only a small portion of nuclei locations in each image are labeled. The framework consists of two learning stages. In the first stage, we design a semi-supervised strategy to learn a detection model from partially labeled nuclei locations. Specifically, an extended Gaussian mask is designed to train an initial model with partially labeled data. Then, self-training with background propagation is proposed to make use of the unlabeled regions to boost nuclei detection and suppress false positives. In the second stage, a segmentation model is trained from the detected nuclei locations in a weakly-supervised fashion. Two types of coarse labels with complementary information are derived from the detected points and are then utilized to train a deep neural network. The fully-connected conditional random field loss is utilized in training to further refine the model without introducing extra computational complexity during inference. The proposed method is extensively evaluated on two nuclei segmentation datasets. The experimental results demonstrate that our method can achieve competitive performance compared to the fully supervised counterpart and the state-of-the-art methods while requiring significantly less annotation effort.",
        "references": [
            "2b60f01e916725a8f0b5dddc44b4cb9bd3e5e8c3",
            "b057cf2fc1ee8360d83283724329e5046bff80de",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "da2f85e313160992b1a0e3db70eb02b58ec740c0",
            "7ae5b1b4b488473314d40711e4d2b0e8d7e210ed",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "a4387fb99253e13ef998446e33b0737f0903b269",
            "09e0363a81c010157ebb2454497a3a06c249c458",
            "abee18114d9ff757dc82e44bead6bad39cfb758c"
        ],
        "related_topics": [
            "Partial Points Annotation",
            "Weakly Supervised Nuclei Segmentation",
            "Aggregated Jaccard Index",
            "Nuclei Segmentation",
            "Coarse Labels",
            "Deep Neural Networks",
            "Weakly-supervised",
            "Histopathology Images",
            "Annotation Effort"
        ],
        "reference_count": "48",
        "citation_count": "110"
    },
    {
        "Id": "e16cc2ce5e961447cb7a1c227764231ab406cef8",
        "title": "Weakly Supervised Cell Segmentation by Point Annotation",
        "authors": [
            "Tianyi Zhao",
            "Zhaozheng Yin"
        ],
        "date": "21 December 2020",
        "abstract": "This work proposes weakly supervised training schemes to train end-to-end cell segmentation networks that only require a single point annotation per cell as the training label and generates a high-quality segmentation mask close to those fully supervised methods using mask annotation on cells. We propose weakly supervised training schemes to train end-to-end cell segmentation networks that only require a single point annotation per cell as the training label and generate a high-quality segmentation mask close to those fully supervised methods using mask annotation on cells. Three training schemes are investigated to train cell segmentation networks, using the point annotation. First, self-training is performed to learn additional information near the annotated points. Next, co-training is applied to learn more cell regions using multiple networks that supervise each other. Finally, a hybrid-training scheme is proposed to leverage the advantages of both self-training and co-training. During the training process, we propose a divergence loss to avoid the overfitting and a consistency loss to enforce the consensus among multiple co-trained networks. Furthermore, we propose weakly supervised learning with human in the loop, aiming at achieving high segmentation accuracy and annotation efficiency simultaneously. Evaluated on two benchmark datasets, our proposal achieves high-quality cell segmentation results comparable to the fully supervised methods, but with much less amount of human annotation effort.",
        "references": [],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "30"
    },
    {
        "Id": "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
        "title": "U-Net: deep learning for cell counting, detection, and morphometry",
        "authors": [
            "Thorsten Falk",
            "Dominic Mai",
            "Robert Bensch",
            "{\\&quot;O}zg{\\&quot;u}n \u00c7i\u00e7ek",
            "Ahmed Abdulkadir",
            "Yassine Marrakchi",
            "Anton B{\\&quot;o}hm",
            "Jan Deubner",
            "Zoe J{\\&quot;a}ckel",
            "Katharina Seiwald",
            "Alexander Dovzhenko",
            "Olaf Tietz",
            "Cristina Dal Bosco",
            "Sean Walsh",
            "Deniz Saltukoglu",
            "Tuan Leng Tay",
            "Marco Prinz",
            "Klaus Palme",
            "Matias Simons",
            "Ilka Diester",
            "Thomas Brox",
            "Olaf Ronneberger"
        ],
        "date": "17 December 2018",
        "abstract": "An ImageJ plugin is presented that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service and comes with pretrained models for single-cell segmentation. U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples. A user-friendly ImageJ plugin enables the application and training of U-Nets for deep-learning-based image segmentation, detection and classification tasks with minimal labeling requirements.",
        "references": [
            "1eab84d51b484d0f79e611979916fa97086e869d",
            "5e39ec7bb7fbf77fc15224ee0dc71bb4f2c44174",
            "6ff93aa6ca902002d16fb0c2d3fb48aead92c61e",
            "7b5be0cdec2a1b36cd8b61d161cff716b3594846",
            "66507340decdc4612b06329d4649b1e8f95a206a",
            "8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8",
            "7fc464470b441c691d10e7331b14a525bc79b8bb",
            "ff263a45eeebe41f6b79dcc3ec10f3bab6806029",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0"
        ],
        "related_topics": [],
        "reference_count": "15",
        "citation_count": "1,228"
    },
    {
        "Id": "aabd9c8f5ef73211d079be8627ee9532922267c4",
        "title": "Nuclei and glands instance segmentation in histology images: a narrative review",
        "authors": [
            "Esha Sadia Nasir",
            "Arshi Parvaiz",
            "Muhammad Moazam Fraz"
        ],
        "date": "26 August 2022",
        "abstract": "In this survey, 126 papers illustrating the AI-based methods for nuclei and glands instance segmentation published in the last five years are deeply analyzed, and the limitations of current approaches and the open challenges are discussed. Examination of tissue biopsy and quantification of the various characteristics of cellular processes are clinical benchmarks in cancer diagnosis. Nuclei and glands instance segmentation greatly assists the high-throughput quantification of cellular process and accurate appraisal of tissue biopsy. It subsequently makes a significant improvement to the computational pathology process for cancer diagnosis, treatment planning, and survival analysis. Recent advancements in the field of computer vision have automated the manual, laborious, and time-consuming histopathological analysis process. Automated image analysis of histopathological images for cells and tissues to trace the entirety of the ultrastructures, has been an active area of research in medical informatics for decades. The developments in computers, microscopy hardware, and the availability of large-scale public datasets have further fastened the development in this field. And the realization that scientific and diagnostic pathology calls for fresh ways to undertake, automated image analysis of histopathological images has captivated contemporary attention. In this survey, 126 papers illustrating the AI-based methods for nuclei and glands instance segmentation published in the last five years (2017\u20132022) are deeply analyzed, and the limitations of current approaches and the open challenges are discussed. Moreover, the potential future research direction is presented, and the contribution of state-of-the-art methods is summarized. Further, a generalized summary of publicly available datasets and detailed insights on the grand challenges illustrating the top-performing methods specific to each challenge is also provided. Besides, we intended to give the reader the current state of existing research and pointers to the future directions in developing methods that can be used in clinical practice enabling improved diagnosis, grading, prognosis, and treatment planning of cancer. To the best of our knowledge, no previous work has reviewed the instance segmentation in histology images focusing on nuclei and glands instance segmentation.",
        "references": [
            "e0b984f5fce8535ba1306cd4d735aa0ed9ef7e62",
            "c9a372fc3b30bb2da051e941cab44c3e5ba31065",
            "348eea0020bdf24329331d9df21da7033eb59bf3",
            "78db21fabea962592ac0aef54624251550929109",
            "ac158a0fa7ae122cc5e3785382aa364762c26839",
            "d23d38e8d400ed14bd6fbfe8867f7b06b8b9fbd6",
            "8ecc6ecd5b77665b261cce1a96d431d9f43bdaea",
            "60ebb13073843322f2927edcf69ae214710d6448",
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "737142a5c402034df9825904f7f5d5379bd1384a"
        ],
        "related_topics": [
            "Instance Segmentation",
            "Histology Images",
            "Glands",
            "Treatment Planning",
            "Computational Pathology",
            "Survival Analysis"
        ],
        "reference_count": "168",
        "citation_count": "2"
    },
    {
        "Id": "05bcc2f7dd8bc31a30cd6674a4667768b1c31db5",
        "title": "Nucleus segmentation: towards automated solutions.",
        "authors": [
            "R{\\&#x27;e}ka Hollandi",
            "Nikita Moshkov",
            "Lassi Paavolainen",
            "Ervin A. Tasn{\\&#x27;a}di",
            "Filippo Piccinini",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th"
        ],
        "date": "1 January 2022",
        "abstract": "Semantic Scholar extracted view of \"Nucleus segmentation: towards automated solutions.\" by R\u00e9ka Hollandi et al.",
        "references": [
            "3d157f03484d913cbdb2463b8bc33fc4266c42d6",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "f259a4088240b1f2a71d68b1a465113a8b18e63e",
            "c8606e6b3de21307205488fd41ca586df9b708e6",
            "86e92a4b836db293a7a63d81e2a400846088648a",
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "48d0975ff581640b970d29b84af95cdd04e63f11",
            "67711d66ff40194c4578b9135ff05d7e60033f35",
            "65389e7ab951a21fd0610236ae84f0cd48941860",
            "3ad4b29906c8987bac46ea0ea9586b58a38fa8f4"
        ],
        "related_topics": [],
        "reference_count": "103",
        "citation_count": "28"
    },
    {
        "Id": "cca8bbf14d4a37afc20e25f73b4016194567f154",
        "title": "Biopsy-free in vivo virtual histology of skin using deep learning",
        "authors": [
            "Jingxi Li",
            "Jason Garfinkel",
            "Xiaoran Zhang",
            "Di Wu",
            "Yijie Zhang",
            "Kevin de Haan",
            "Hongda Wang",
            "Tairan Liu",
            "Bijie Bai",
            "Yair Rivenson",
            "Gennady Rubinstein",
            "Philip O. Scumpia",
            "Aydogan Ozcan"
        ],
        "date": "18 November 2021",
        "abstract": "It is shown that this trained neural network can be used to rapidly perform virtual histology of in vivo, label-free RCM images of normal skin structure, basal cell carcinoma, and melanocytic nevi with pigmented melanocytes, demonstrating similar histological features to traditional histology from the same excised tissue. An invasive biopsy followed by histological staining is the benchmark for pathological diagnosis of skin tumors. The process is cumbersome and time-consuming, often leading to unnecessary biopsies and scars. Emerging noninvasive optical technologies such as reflectance confocal microscopy (RCM) can provide label-free, cellular-level resolution, in vivo images of skin without performing a biopsy. Although RCM is a useful diagnostic tool, it requires specialized training because the acquired images are grayscale, lack nuclear features, and are difficult to correlate with tissue pathology. Here, we present a deep learning-based framework that uses a convolutional neural network to rapidly transform in vivo RCM images of unstained skin into virtually-stained hematoxylin and eosin-like images with microscopic resolution, enabling visualization of the epidermis, dermal-epidermal junction, and superficial dermis layers. The network was trained under an adversarial learning scheme, which takes ex vivo RCM images of excised unstained/label-free tissue as inputs and uses the microscopic images of the same tissue labeled with acetic acid nuclear contrast staining as the ground truth. We show that this trained neural network can be used to rapidly perform virtual histology of in vivo, label-free RCM images of normal skin structure, basal cell carcinoma, and melanocytic nevi with pigmented melanocytes, demonstrating similar histological features to traditional histology from the same excised tissue. This application of deep learning-based virtual staining to noninvasive imaging technologies may permit more rapid diagnoses of malignant skin neoplasms and reduce invasive skin biopsies.",
        "references": [
            "15b087d4d92aa5452046002328add94850ea86a3",
            "ad41b667c2bdb7715b99ca18d1bb2d784de4312b",
            "e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "c1dc043f623a210e6a7c2f6b6055ba3805eb2ec0",
            "f382760f8a9a81ab0ffd9ce49e1f6caf8935bf80",
            "dbd60e38888d3bdab0cc09a35bc86efe4b66501d",
            "b0d7fd9e22987af33ed515902541d4dc3dc7cf72",
            "0eaba35456c22650b692334188f5865c0fe58d25",
            "fb6dfdaeaea6ca7fef5caf7cb19cf8ba6f2b28e2",
            "7f03f10813db5cc476ab53b733133c3a7be9a2ab"
        ],
        "related_topics": [],
        "reference_count": "64",
        "citation_count": "37"
    },
    {
        "Id": "23803091c216b08f1c8a6386b9a731584a2159e1",
        "title": "Digital Image Analysis Tools Developed by the Indiana O\u2019Brien Center",
        "authors": [
            "Kenneth W. Dunn"
        ],
        "date": "16 December 2021",
        "abstract": "This paper describes biomedical image analysis software developed by the Indiana O\u2019Brien Center over the past 25 years, and describes the development and commercialization of multiphoton microscopy. The scale and complexity of images collected in biological microscopy have grown enormously over the past 30 years. The development and commercialization of multiphoton microscopy has promoted a renaissance of intravital microscopy, providing a window into cell biology in vivo. New methods of optical sectioning and tissue clearing now enable biologists to characterize entire organs at subcellular resolution. New methods of multiplexed imaging support simultaneous localization of forty or more probes at a time. Exploiting these exciting new techniques has increasingly required biomedical researchers to master procedures of image analysis that were once the specialized province of imaging experts. A primary goal of the Indiana O\u2019Brien Center has been to develop robust and accessible image analysis tools for biomedical researchers. Here we describe biomedical image analysis software developed by the Indiana O\u2019Brien Center over the past 25 years.",
        "references": [
            "7e14f3a27c1f407fb0207bbfdc180e6ccf0cd467",
            "a57095b5ea46625de1d45e3166eeb2fa207efe4c",
            "8054b857fc8d6456128c8c862fd3a2bc21d76900",
            "dd232022deb5039d2010fc4630ac23137558796d",
            "5f45036a0c22ebaa53e0bc60d6cb9e9a8a089547",
            "a0211eeec5843f6adccc0f291d3b7700e4b237ca",
            "355cb644403fddc267feda48a7853221c199b411",
            "aae0713c0a0f2e9d31ea349586aac0a46578d43e",
            "ef56d81dd13b70f5f82c72973a3f37f2376aa2ff",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "7998bd9b3f5b5c9169e333133f7e884f5eeb9a42",
        "title": "Automated segmentation of endometriosis using transfer learning technique",
        "authors": [
            "S. Visalaxi",
            "T. Sudalaimuthu"
        ],
        "date": "28 March 2022",
        "abstract": "The proposed SSAE approach identifies the affected region using U-Net architecture and systematic sampling procedure and proves the similarity between pathologically identified images and the corresponding annotated images using a statistical evaluation. Background: This paper focuses on segmenting the exact location of endometriosis using the state-of-art technique known as U-Net. Endometriosis is a progressive disorder that has a significant impact on women. The lesion-like appearance that grows inside the uterus and sheds for every periodical cycle is known as endometriosis. If the lesion exists and is transferred to other locations in the women\u2019s reproductive system, it may lead to a serious problem. Besides radiologists deep learning techniques exist for recognizing the presence and aggravation of endometriosis. Methods: The proposed method known as structural similarity analysis of endometriosis (SSAE) identifies the similarity between pathologically identified and annotated images obtained from standardized dataset known as GLENDA v1.5 by implementing two systematic approaches. The first approach is based on semantic segmentation and the second approach uses statistical analysis. Semantic segmentation is a cutting-edge technology for identifying exact locations by performing pixel-level classification. In semantic segmentation, U-Net is a transfer-learning architecture that works effectively for biomedical image classification. The SSAE implements the U-Net architecture for segmenting endometriosis based on the region of occurrence. The second approach proves the similarity between pathologically identified images and the corresponding annotated images using a statistical evaluation. Statistical analysis was performed using calculation of both the mean and standard deviation of all four regions by implementing systematic sampling procedure. Results: The SSAE obtains the intersection over union value of 0.72 and the F1 score of 0.74 for the trained dataset. The means of both the laparoscopic and annotated images for all regions were similar. Consequently, the SSAE facilitated the presence of abnormalities in a specific region. Conclusions: The proposed SSAE approach identifies the affected region using U-Net architecture and systematic sampling procedure.",
        "references": [
            "cd745f279a2c9a6a5a375b9de939c160b28afbb9",
            "164fb2794ede4425d07eb9721f8bb68a7129f911",
            "bf10f7df55e80523a46618b60b281e9961c2b3c0",
            "009b13f845344456f22eccd9e7ab058e2e5a6802",
            "8e9b8e0408c660bfa659500c740d9baab5587e99",
            "387dac0b0d8f9d0721786265e8e1e053c0af61dc",
            "3f58e18dc3d8ac9a7b7d9501b0f36961352ec6a2",
            "3a9306d98f86aa3bc6cdcfedb5bfd6b4654ce29b",
            "c786bd5034590217b2d3c477bf04e02d568f211a",
            "dbfcaf5a99755f5fe8c0ff267ccbc726680f767c"
        ],
        "related_topics": [
            "U-Net",
            "Semantic Segmentation",
            "Annotated Images",
            "Radiologist"
        ],
        "reference_count": "47",
        "citation_count": "2"
    },
    {
        "Id": "6f5fe81c50cd278de70839d6508eca502336a93f",
        "title": "Deep Learning Based Segmentation of Nuclei from Fluorescence Microscopy Images",
        "authors": [
            "Prabhakar R. Gudla",
            "George Zaki",
            "Sigal Shachar",
            "Tom Misteli",
            "Gianluca Pegoraro"
        ],
        "date": "1 August 2019",
        "abstract": "A novel image analysis workflow based on D-CNNs and transfer learning for the segmentation of fluorescently labelled mammalian cell nuclei is described. Segmentation of cellular objects is a necessary step in many image processing pipelines of fluorescent microscopy images. As an example, High-Throughput Imaging (HTI) assays often require accurate detection of nuclei and other subcellular compartments to quantify the biological effects of RNAi or chemical reagents Deep Convolution Neural Networks models (D-CNNs) have surpassed traditional image processing approaches for biological object detection are mostly non-parametric, and require few and simple image pre-processing steps. However, successful training of these models generally requires large numbers (hundreds to thousands) of representative images along with their human expert-generated ground-truth annotations. This hinders the rapid implementation of D-CNN models for a wide range of HTI assays. Here we describe a novel image analysis workflow based on D-CNNs and transfer learning for the segmentation of fluorescently labelled mammalian cell nuclei. The workflow uses few heavily augmented images for training of D-CNNs, of preliminary ground-truth annotations by traditional image processing methods by pre-trained D-CNNs, a rapid preliminary ground-truth correction step by expert annotators, and state-of-the-art D-CNNs: U-Net and R-CNN and",
        "references": [
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "f638edf9370a9a07681d80f7c6366e9783848a25",
            "dbefe593b3b9f235095c07b6db67782960b0d598",
            "aca7cb2bb531e60a590e40a256674b67f0a88cde"
        ],
        "related_topics": [],
        "reference_count": "4",
        "citation_count": "2"
    },
    {
        "Id": "c1f68f43973e9312bd35420d3c25e79375f3b520",
        "title": "Deep Learning architectures for generalized immunofluorescence based nuclear image segmentation",
        "authors": [
            "Florian Kromp",
            "Lukas Fischer",
            "Eva Bozsaky",
            "Inge M. Ambros",
            "Wolfgang Doerr",
            "Sabine Taschner-Mandl",
            "Peter F. Ambros",
            "Allan Hanbury"
        ],
        "date": "30 July 2019",
        "abstract": "Mask R-CNN, an architecture designed to address instance aware segmentation tasks, outperforms other architectures and is compared to architectures that operate on pixel to pixel translation and an architecture that operates on object detection and subsequent locally applied segmentation. Separating and labeling each instance of a nucleus (instance-aware segmentation) is the key challenge in segmenting single cell nuclei on fluorescence microscopy images. Deep Neural Networks can learn the implicit transformation of a nuclear image into a probability map indicating the class membership of each pixel (nucleus or background), but the use of post-processing steps to turn the probability map into a labeled object mask is error-prone. This especially accounts for nuclear images of tissue sections and nuclear images across varying tissue preparations. In this work, we aim to evaluate the performance of state-of-the-art deep learning architectures to segment nuclei in fluorescence images of various tissue origins and sample preparation types without post-processing. We compare architectures that operate on pixel to pixel translation and an architecture that operates on object detection and subsequent locally applied segmentation. In addition, we propose a novel strategy to create artificial images to extend the training set. We evaluate the influence of ground truth annotation quality, image scale and segmentation complexity on segmentation performance. Results show that three out of four deep learning architectures (U-Net, U-Net with ResNet34 backbone, Mask R-CNN) can segment fluorescent nuclear images on most of the sample preparation types and tissue origins with satisfactory segmentation performance. Mask R-CNN, an architecture designed to address instance aware segmentation tasks, outperforms other architectures. Equal nuclear mean size, consistent nuclear annotations and the use of artificially generated images result in overall acceptable precision and recall across different tissues and sample preparation types.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "7ae5b1b4b488473314d40711e4d2b0e8d7e210ed",
            "d6785c954cc5562838a57e185e99d0496b5fd5a2",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "b4a0cb0837c47d325ccb0a8bded7baaf23753932",
            "3d5f72d8f649d3014e53a2ca1f4145a15cd5953e",
            "1ec20454b36b1181062af2587831b49d85253245"
        ],
        "related_topics": [
            "U-Net",
            "Deep Neural Networks",
            "Image Scale",
            "Training Set",
            "Deep Learning",
            "Object Detection"
        ],
        "reference_count": "31",
        "citation_count": "15"
    },
    {
        "Id": "c89bfd998b0a6c656010b629814ab0cad3cff72e",
        "title": "Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images",
        "authors": [
            "Juan C. Caicedo",
            "Jonathan F Roth",
            "Allen Goodman",
            "Tim Becker",
            "Kyle W. Karhohs",
            "Claire McQuin",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "31 May 2018",
        "abstract": "This work presents an evaluation framework to measure accuracy, types of errors, and computational efficiency; and uses it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. Identifying nuclei is often a critical first step in analyzing microscopy images of cells, and classical image processing algorithms are most commonly used for this task. Recent developments in deep learning can yield superior accuracy, but typical evaluation metrics for nucleus segmentation do not satisfactorily capture error modes that are relevant in cellular images. Besides, large image data sets with ground truth for evaluation have been limiting. We present an evaluation framework to measure accuracy, types of errors, and computational efficiency; and use it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. We publicly release a set of 23,165 manually annotated nuclei and source code to reproduce experiments. Our results show that U-Net outperforms both pixel-wise classification networks and classical algorithms. Also, our evaluation framework shows that deep learning improves accuracy and reduces the number of biologically relevant errors by half.",
        "references": [
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "cdecac6e2f578cfc56140e00aaa74a78f864fea2",
            "318f82a3e593e391cfd0da7964b16d83299aa943",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "1268de7bda769e651dc6c089d006c7edbe37f563",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "36748de338909976f72ffbadaf097470ec040da0"
        ],
        "related_topics": [
            "DeepCell",
            "Broad Bioimage Benchmark Collection",
            "Deep Learning",
            "Nucleus Segmentation",
            "International Society For Advancement Of Cytometry",
            "Cytometry"
        ],
        "reference_count": "53",
        "citation_count": "226"
    },
    {
        "Id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "date": "18 May 2015",
        "abstract": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "references": [
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "eb42cf88027de515750f230b23b1a057dc782108",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "4f03888450fde9e8234f616badaae499740e57a4",
            "3b2ccc97f1433cf8750a2ad5a05555ccd10e9cdf",
            "3bffa23a16c273ac2228a13e65dade6766ce7777"
        ],
        "related_topics": [
            "U-Net",
            "Contracting Path",
            "ISBI Cell Tracking Challenge 2015",
            "Sliding-window Convolutional Network",
            "Neuronal Structures",
            "Expansive Path",
            "Biomedical Segmentation",
            "ISBI 2012",
            "U-net Architecture",
            "Drosophila First Instar Larva Ventral Nerve Cord"
        ],
        "reference_count": "18",
        "citation_count": "56,807"
    },
    {
        "Id": "20c97e24db99e4667813051dc1fa3e8f5b4d4983",
        "title": "Learn to segment single cells with deep distance estimator and deep cell detector",
        "authors": [
            "Weikang Wang",
            "David A Taft",
            "Yi-jiun Chen",
            "Jingyu Zhang",
            "Callen T. Wallace",
            "Min Xu",
            "Simon C Watkins",
            "Jianhua Xing"
        ],
        "date": "28 March 2018",
        "abstract": "Semantic Scholar extracted view of \"Learn to segment single cells with deep distance estimator and deep cell detector\" by Weikang Wang et al.",
        "references": [
            "d1f788e9ac58de5d2dc6a3928939b411cf9ec5a6",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f",
            "a2e06c347c192c94bcae153c36199d1272f7408f",
            "f82c12dd609c4f4b34603e81d4dfb60fd3272013",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "a782971fa27ae701a2610d7d18e99833583e7ca1",
            "39ad6c911f3351a3b390130a6e4265355b4d593b",
            "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "b0c065cd43aa7280e766b5dcbcc7e26abce59330"
        ],
        "related_topics": [],
        "reference_count": "42",
        "citation_count": "64"
    },
    {
        "Id": "85ebc2a7e916680b9d0d20601e2230c21e43f729",
        "title": "AI-based automated active learning for discovery of hidden dynamic processes: A use case in light microscopy",
        "authors": [
            "Nils Friederich",
            "Angelo Jovin Yamachui Sitcheu",
            "Oliver Neumann",
            "S{\\&quot;u}heyla Eroglu-Kayik\u00e7i",
            "Roshan Prizak",
            "Lennart Hilbert",
            "Ralf Mikut"
        ],
        "date": "5 October 2023",
        "abstract": "Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based and represents dynamic processes so as to allow prediction of pseudo-time values from single still images, and a Machine Learning Operations (MLOps)-based pipeline that uses the extracted knowledge from EDP to efficiently schedule acquisition in biomedical experiments for dynamic processes in practice is presented. In the biomedical environment, experiments assessing dynamic processes are primarily performed by a human acquisition supervisor. Contemporary implementations of such experiments frequently aim to acquire a maximum number of relevant events from sometimes several hundred parallel, non-synchronous processes. Since in some high-throughput experiments, only one or a few instances of a given process can be observed simultaneously, a strategy for planning and executing an efficient acquisition paradigm is essential. To address this problem, we present two new methods in this paper. The first method, Encoded Dynamic Process (EDP), is Artificial Intelligence (AI)-based and represents dynamic processes so as to allow prediction of pseudo-time values from single still images. Second, with Experiment Automation Pipeline for Dynamic Processes (EAPDP), we present a Machine Learning Operations (MLOps)-based pipeline that uses the extracted knowledge from EDP to efficiently schedule acquisition in biomedical experiments for dynamic processes in practice. In a first experiment, we show that the pre-trained State-Of-The- Art (SOTA) object segmentation method Contour Proposal Networks (CPN) works reliably as a module of EAPDP to extract the relevant object for EDP from the acquired three-dimensional image stack.",
        "references": [
            "7fbacba72ca30c55de36830e7a3065984c87935c",
            "3dfd41a3a004987229c890f99fd8f624887bb919",
            "e44800634a510b97ec65ca10c34c3517c58bae89",
            "04c6365ecabbdab97522c9a54f937f791edb8edf",
            "d2c9f9473d11ac628408c12958cfef2bf6d39336",
            "1ef7b04975ce44df7ee019978aecbabd4d11870e",
            "34044849fcbc97a21217a4d213889a9b76c26652",
            "94eb8e46767ae77e265b0a20dcc0d9f69d2d6e2b",
            "b3b7433817380b98951d4a6502a889b8ce8c7422",
            "2a00dad16dcaf818ed4bb1c114c2f3769ea4d466"
        ],
        "related_topics": [],
        "reference_count": "50",
        "citation_count": "One"
    },
    {
        "Id": "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
        "title": "Cellpose: a generalist algorithm for cellular segmentation",
        "authors": [
            "Carsen Stringer",
            "Tim Wang",
            "Michalis Michaelos",
            "Marius Pachitariu"
        ],
        "date": "3 February 2020",
        "abstract": "This work introduces a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Many biological applications require the segmentation of cell bodies, membranes and nuclei from microscopy images. Deep learning has enabled great progress on this problem, but current methods are specialized for images that have large training datasets. Here we introduce a generalist, deep learning-based segmentation method called Cellpose, which can precisely segment cells from a wide range of image types and does not require model retraining or parameter adjustments. Cellpose was trained on a new dataset of highly varied images of cells, containing over 70,000 segmented objects. We also demonstrate a three-dimensional (3D) extension of Cellpose that reuses the two-dimensional (2D) model and does not require 3D-labeled data. To support community contributions to the training data, we developed software for manual labeling and for curation of the automated results. Periodically retraining the model on the community-contributed data will ensure that Cellpose improves constantly. Cellpose is a generalist, deep learning-based approach for segmenting structures in a wide range of image types. Cellpose does not require parameter adjustment or model retraining and outperforms established methods on 2D and 3D datasets.",
        "references": [
            "f12ca74c2e0be7f13bdcdae6e4a4944de858ebdb",
            "7b79a48b5985ea85ac7f38bb579b4824a7ba1fcc",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "5e1335f1cf45fb4c29a0697f65c1e5c8e26d7dd4",
            "0a9966e8c21fb96de384daf445d311bdbdd74993",
            "268241fd604918a86e1b27e1a880d47e0ecc2d5b",
            "da895fa70d45ba06efae6cb128e885ab49c5e977",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a"
        ],
        "related_topics": [
            "Cellpose",
            "Cellpose Model",
            "StarDist",
            "nucleAIzer",
            "Ilastik",
            "Deep Learning"
        ],
        "reference_count": "73",
        "citation_count": "1,157"
    },
    {
        "Id": "bd316183cad100b12d717d25f2e55676eaa8f701",
        "title": "CellSeg: a robust, pre-trained nucleus segmentation and pixel quantification software for highly multiplexed fluorescence images",
        "authors": [
            "Michael Y. Lee",
            "Jacob S. Bedia",
            "Salil S. Bhate",
            "Graham L. Barlow",
            "Darci J. Phillips",
            "Wendy J. Fantl",
            "Garry P. Nolan",
            "Christian M. Sch{\\&quot;u}rch"
        ],
        "date": "18 January 2022",
        "abstract": "CellSeg is an open-source, pre-trained nucleus segmentation and signal quantification software based on the Mask region-convolutional neural network (R-CNN) architecture that performs at the level of top segmentation algorithms in the 2018 Kaggle Data Challenge both qualitatively and quantitatively. Algorithmic cellular segmentation is an essential step for the quantitative analysis of highly multiplexed tissue images. Current segmentation pipelines often require manual dataset annotation and additional training, significant parameter tuning, or a sophisticated understanding of programming to adapt the software to the researcher\u2019s need. Here, we present CellSeg, an open-source, pre-trained nucleus segmentation and signal quantification software based on the Mask region-convolutional neural network (R-CNN) architecture. CellSeg is accessible to users with a wide range of programming skills. CellSeg performs at the level of top segmentation algorithms in the 2018 Kaggle Data Challenge both qualitatively and quantitatively and generalizes well to a diverse set of multiplexed imaged cancer tissues compared to established state-of-the-art segmentation algorithms. Automated segmentation post-processing steps in the CellSeg pipeline improve the resolution of immune cell populations for downstream single-cell analysis. Finally, an application of CellSeg to a highly multiplexed colorectal cancer dataset acquired on the CO-Detection by indEXing (CODEX) platform demonstrates that CellSeg can be integrated into a multiplexed tissue imaging pipeline and lead to accurate identification of validated cell populations. CellSeg is a robust cell segmentation software for analyzing highly multiplexed tissue images, accessible to biology researchers of any programming skill level.",
        "references": [
            "40af8e077b162551888ea348d16f458a12c147e5",
            "e3380ca81e25540023261f9f1c17eae197f1fad4",
            "f12ca74c2e0be7f13bdcdae6e4a4944de858ebdb",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "f28e387d4229c5f690ce4570a391c0f47e7155c7"
        ],
        "related_topics": [
            "Nucleus Segmentation",
            "Pre-trained",
            "R-CNN"
        ],
        "reference_count": "46",
        "citation_count": "47"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
        "title": "Cell Detection with Star-convex Polygons",
        "authors": [
            "Uwe Schmidt",
            "Martin Weigert",
            "Coleman Broaddus",
            "Eugene Wimberly Myers"
        ],
        "date": "9 June 2018",
        "abstract": "This work proposes to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. Automatic detection and segmentation of cells and nuclei in microscopy images is important for many biological applications. Recent successful learning-based approaches include per-pixel cell segmentation with subsequent pixel grouping, or localization of bounding boxes with subsequent shape refinement. In situations of crowded cells, these can be prone to segmentation errors, such as falsely merging bordering cells or suppressing valid cell instances due to the poor approximation with bounding boxes. To overcome these issues, we propose to localize cell nuclei via star-convex polygons, which are a much better shape representation as compared to bounding boxes and thus do not need shape refinement. To that end, we train a convolutional neural network that predicts for every pixel a polygon for the cell instance at that position. We demonstrate the merits of our approach on two synthetic datasets and one challenging dataset of diverse fluorescence microscopy images.",
        "references": [
            "da895fa70d45ba06efae6cb128e885ab49c5e977",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "5896bd31724bd2f57630473e1e466ea08c3b0c34",
            "c6c2be7ba4afccc4ac2449b60acac69cd44c4ce3",
            "75b982621864188df5917f6d38141209d30c7590",
            "7b5be0cdec2a1b36cd8b61d161cff716b3594846",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "1a0912bb76777469295bb2c059faee907e7f3258",
            "ba124bbde8d114a9103eb5751036e2e4710e2fff",
            "1c9352d77cc72de03e9cf47eabc4b587af6d17be"
        ],
        "related_topics": [
            "Star-convex Polygons",
            "StarDist",
            "Data Science Bowl",
            "Shape Representation",
            "Cell Detection",
            "Convolutional Neural Network"
        ],
        "reference_count": "20",
        "citation_count": "677"
    },
    {
        "Id": "c7edc5bf1d811e42c0c7de542ca6f395b82b3264",
        "title": "Omnipose: a high-precision morphology-independent solution for bacterial cell segmentation",
        "authors": [
            "Kevin J. Cutler",
            "Carsen Stringer",
            "Paul A. Wiggins",
            "Joseph D. Mougous"
        ],
        "date": "4 November 2021",
        "abstract": "Omnipose is a deep neural network algorithm for image segmentation that improves upon existing approaches by solving the challenging problem of accurately segmenting morphologically diverse cells from images acquired with any modality. Advances in microscopy hold great promise for allowing quantitative and precise measurement of morphological and molecular phenomena at the single-cell level in bacteria; however, the potential of this approach is ultimately limited by the availability of methods to faithfully segment cells independent of their morphological or optical characteristics. Here, we present Omnipose, a deep neural network image-segmentation algorithm. Unique network outputs such as the gradient of the distance field allow Omnipose to accurately segment cells on which current algorithms, including its predecessor, Cellpose, produce errors. We show that Omnipose achieves unprecedented segmentation performance on mixed bacterial cultures, antibiotic-treated cells and cells of elongated or branched morphology. Furthermore, the benefits of Omnipose extend to non-bacterial subjects, varied imaging modalities and three-dimensional objects. Finally, we demonstrate the utility of Omnipose in the characterization of extreme morphological phenotypes that arise during interbacterial antagonism. Our results distinguish Omnipose as a powerful tool for characterizing diverse and arbitrarily shaped cell types from imaging data. Omnipose is a deep neural network algorithm for image segmentation that improves upon existing approaches by solving the challenging problem of accurately segmenting morphologically diverse cells from images acquired with any modality.",
        "references": [
            "c12abac631df81855b20a3820bbb365c82ca43df",
            "0542f288d6a4cddc2cdcbdd398be98c9c9ae3dcf",
            "0ff7ede6c0a9bdab90c22fc88bc38f4c36fe2be2",
            "5ce5240f8203e6e1a9cf3ce6c1e0847b40a76475",
            "30ed4511a88545fc9e6fd665779baa115b81d4df",
            "322a2e5e990ecdc525a610547a62718f53b7e348",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "47c559cba6651400e21b69120ccf50f19f4a8dd7",
            "e48cc0a42748d7da5b5c710fa7a44e273a1fabf0",
            "9c3d224d328ec5a44b24f87589232d8aed499c11"
        ],
        "related_topics": [],
        "reference_count": "71",
        "citation_count": "65"
    },
    {
        "Id": "268241fd604918a86e1b27e1a880d47e0ecc2d5b",
        "title": "Segmentation of touching cell nuclei using gradient flow tracking",
        "authors": [
            "Gang Li",
            "Gang Li",
            "Tian Qi Liu",
            "Tian Qi Liu",
            "Jingxin Nie",
            "Lei Guo",
            "J. Chen",
            "Jinmin Zhu",
            "Jinmin Zhu",
            "Weiming Xia",
            "Andrew Mara",
            "Scott A. Holley",
            "Stephen T. C. Wong",
            "Stephen T. C. Wong"
        ],
        "date": "1 July 2008",
        "abstract": "This method is able to segment closely juxtaposed or clustered cell nuclei, with high sensitivity and specificity in different situations, with both over\u2010segmentation and under\u2010se segmentation rates below 3%. Reliable cell nuclei segmentation is an important yet unresolved problem in biological imaging studies. This paper presents a novel computerized method for robust cell nuclei segmentation based on gradient flow tracking. This method is composed of three key steps: (1) generate a diffused gradient vector flow field; (2) perform a gradient flow tracking procedure to attract points to the basin of a sink; and (3) separate the image into small regions, each containing one nucleus and nearby peripheral background, and perform local adaptive thresholding in each small region to extract the cell nucleus from the background. To show the generality of the proposed method, we report the validation and experimental results using microscopic image data sets from three research labs, with both over\u2010segmentation and under\u2010segmentation rates below 3%. In particular, this method is able to segment closely juxtaposed or clustered cell nuclei, with high sensitivity and specificity in different situations.",
        "references": [
            "c0da38acd8e44c77e732b21e505f7042a8736a9c",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "0640f9ac79a8a4fa033c379f1cbde7033a41eeea",
            "e86c88e690715bc3beeebc8738ea7912f0c8ccc1",
            "e3aa7e680a95d7343fcd3cd27cad85b443d1933d",
            "a9c006a11c8576350838c8f6ca8d418957f1287a",
            "f3a3254d7e233ae36baef93ebe5008c21151f467",
            "f859d132aa7e868609bace85291c0fc1c6636fe2",
            "41ae827281ac2949ed19a60890b047d02f1212da",
            "0a8a1b1f491ad1a99edadf9fe32f3a8702457269"
        ],
        "related_topics": [
            "Gradient Flow Tracking",
            "Cell Nuclei Segmentation",
            "Over-segmentation"
        ],
        "reference_count": "37",
        "citation_count": "81"
    },
    {
        "Id": "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
        "title": "U-Net: deep learning for cell counting, detection, and morphometry",
        "authors": [
            "Thorsten Falk",
            "Dominic Mai",
            "Robert Bensch",
            "{\\&quot;O}zg{\\&quot;u}n \u00c7i\u00e7ek",
            "Ahmed Abdulkadir",
            "Yassine Marrakchi",
            "Anton B{\\&quot;o}hm",
            "Jan Deubner",
            "Zoe J{\\&quot;a}ckel",
            "Katharina Seiwald",
            "Alexander Dovzhenko",
            "Olaf Tietz",
            "Cristina Dal Bosco",
            "Sean Walsh",
            "Deniz Saltukoglu",
            "Tuan Leng Tay",
            "Marco Prinz",
            "Klaus Palme",
            "Matias Simons",
            "Ilka Diester",
            "Thomas Brox",
            "Olaf Ronneberger"
        ],
        "date": "17 December 2018",
        "abstract": "An ImageJ plugin is presented that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service and comes with pretrained models for single-cell segmentation. U-Net is a generic deep-learning solution for frequently occurring quantification tasks such as cell detection and shape measurements in biomedical image data. We present an ImageJ plugin that enables non-machine-learning experts to analyze their data with U-Net on either a local computer or a remote server/cloud service. The plugin comes with pretrained models for single-cell segmentation and allows for U-Net to be adapted to new tasks on the basis of a few annotated samples. A user-friendly ImageJ plugin enables the application and training of U-Nets for deep-learning-based image segmentation, detection and classification tasks with minimal labeling requirements.",
        "references": [
            "1eab84d51b484d0f79e611979916fa97086e869d",
            "5e39ec7bb7fbf77fc15224ee0dc71bb4f2c44174",
            "6ff93aa6ca902002d16fb0c2d3fb48aead92c61e",
            "7b5be0cdec2a1b36cd8b61d161cff716b3594846",
            "66507340decdc4612b06329d4649b1e8f95a206a",
            "8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8",
            "7fc464470b441c691d10e7331b14a525bc79b8bb",
            "ff263a45eeebe41f6b79dcc3ec10f3bab6806029",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0"
        ],
        "related_topics": [],
        "reference_count": "15",
        "citation_count": "1,228"
    },
    {
        "Id": "c68fc9bb0d611547d0f7cafc0717e1f02a291766",
        "title": "Scale-Aware Neural Network for Semantic Segmentation of Multi-Resolution Remote Sensing Images",
        "authors": [
            "Libo Wang",
            "Ce Zhang",
            "Rui Li",
            "Chenxi Duan",
            "Xiaoliang Meng",
            "Peter M. Atkinson"
        ],
        "date": "14 March 2021",
        "abstract": "This paper proposes a novel scale-aware neural network (SaNet) for the semantic segmentation of MSR remotely sensed imagery and demonstrates the effectiveness of the proposed SaNet in cross-resolution segmentation. Assigning geospatial objects with specific categories at the pixel level is a fundamental task in remote sensing image analysis. Along with the rapid development of sensor technologies, remotely sensed images can be captured at multiple spatial resolutions (MSR) with information content manifested at different scales. Extracting information from these MSR images represents huge opportunities for enhanced feature representation and characterisation. However, MSR images suffer from two critical issues: (1) increased scale variation of geo-objects and (2) loss of detailed information at coarse spatial resolutions. To bridge these gaps, in this paper, we propose a novel scale-aware neural network (SaNet) for the semantic segmentation of MSR remotely sensed imagery. SaNet deploys a densely connected feature network (DCFFM) module to capture high-quality multi-scale context, such that the scale variation is handled properly and the quality of segmentation is increased for both large and small objects. A spatial feature recalibration (SFRM) module was further incorporated into the network to learn intact semantic content with enhanced spatial relationships, where the negative effects of information loss are removed. The combination of DCFFM and SFRM allows SaNet to learn scale-aware feature representation, which outperforms the existing multi-scale feature representation. Extensive experiments on three semantic segmentation datasets demonstrated the effectiveness of the proposed SaNet in cross-resolution segmentation.",
        "references": [
            "561f5bd4dd8db04e8b74c12b225757e0665b707a",
            "39b45b0d60807296db97d9eb37b1fb99d244e2ed",
            "73b551f5fbaef0403935101b920e0a07880b4927",
            "9c6eca31f311eae935e84efa6966c0165bc4e14a",
            "f2e0cce0fa68a65b2e3cf254505439d325ce092c",
            "b09a92ff2642d1a3c09e96e38f389ddc28951b68",
            "49595205981e115d85f1445429694909a1a25040",
            "f3d081eb9d1ebd9d907993a3834e5d967bc56027",
            "eaed51bdcb4f4000734cc596f8a16fb1dce9862c",
            "83ef7de2669bb2827208fd3a64ac910e276fbdb4"
        ],
        "related_topics": [
            "SANet",
            "Mining Software Repositories",
            "Semantic Segmentation",
            "Scale Variations",
            "Pixel Level",
            "Small Objects",
            "Multi-scale Feature Representation",
            "Geospatial Objects",
            "Multi-scale Context"
        ],
        "reference_count": "63",
        "citation_count": "9"
    },
    {
        "Id": "8c04f169203f9e55056a6f7f956695babe622a38",
        "title": "Distinctive Image Features from Scale-Invariant Keypoints",
        "authors": [
            "David G. Lowe"
        ],
        "date": "1 November 2004",
        "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene and can robustly identify objects among clutter and occlusion while achieving near real-time performance. This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
        "references": [
            "f9f836d28f52ad260213d32224a6d227f8e8849a",
            "5e80772f40e8ef924727c6c24168cadc3be0b856",
            "9188b661f2ae65a080676617cc83b7d09773c59f",
            "45cb5262b1fb9f149e8f4171e0b1e52d748e062d",
            "67f693427d956c0dbc822e7f3452aee8ca36204b",
            "2a7c89fcfc49c56ec3557dc507f0477676a2dd6a",
            "6d1e696747a5452364579bea583196ed8bc2b254",
            "62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "9d5ea177c7fcaf88ec6f56cbeb3e9b74c08e98a3",
            "f691349c928b1d65546f243075d5e8995b82f958"
        ],
        "related_topics": [
            "Affine Distortion",
            "Image Matching",
            "Correct Matches",
            "Keypoint Descriptor",
            "Orientation Assignment",
            "Orientation Histogram",
            "SIFT Features",
            "Keypoint",
            "Feature Matches",
            "Scale-space Extrema"
        ],
        "reference_count": "50",
        "citation_count": "43,718"
    },
    {
        "Id": "6730dd574b1405b589382ebf5ec346f4e7ca1667",
        "title": "Three-Dimensional Nuclei Synthesis for Fluorescence Microscopy Image Analysis",
        "authors": [
            "Liming Wu",
            "Alain Chen",
            "Paul Salama",
            "Kenneth W. Dunn",
            "Edward J. Delp"
        ],
        "date": "18 April 2023",
        "abstract": "A 3D nuclei synthesis method for generating 3D ground truth volumes along with corresponding synthetic microscopy volumes is proposed and it is shown that using the synthetic volumes generated by 3DSpCycleGAN as training data improves segmentation accuracy for deep learning segmentation techniques. Three-dimensional tissue cytometry is an important technique for quantitative analysis of cell structures in large fluorescence microscopy volumes. Accurate nuclei detection and segmentation is an important step for 3D tissue cytometry. Deep learning methods have shown promising results for nuclei detection and segmentation. However, manually annotating ground truth for training deep learning methods is labor-intensive and not practical for large 3D volumes. In this paper, we propose a 3D nuclei synthesis method, known as 3DSpCycleGAN, for generating 3D ground truth volumes along with corresponding synthetic microscopy volumes. Experimental results using fluorescence microscopy volumes demonstrate that our method generates more realistic 3D volumes when evaluated both visually and quantitatively than previously reported. We also show that using the synthetic volumes generated by 3DSpCycleGAN as training data improves segmentation accuracy for deep learning segmentation techniques.",
        "references": [
            "21d78c645b1029f7694f7742658e69a38e9e85e1",
            "1ec20454b36b1181062af2587831b49d85253245",
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "c9716a73db358852c2250814a8dc8c3646890028",
            "784f0a5eaf03f493fd065344de5bb39363d943ed",
            "33e632c844d242a2cc3106f69069d3411df0cde8",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "c97042386e3571fa39e6e97aeb7dd966636eef05",
            "c63d67cd97a3737eebacd9b67cbe2aa728173c29"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "24"
    },
    {
        "Id": "341f7523018f2d7663bad6d031f77b8cdc4f3856",
        "title": "3D Nuclei Segmentation by Combining GAN Based Image Synthesis and Existing 3D Manual Annotations",
        "authors": [
            "Xareni Galindo",
            "Thierno Barry",
            "Pauline Guyot",
            "Charlotte Rivi{\\`e}re",
            "R{\\&#x27;e}mi Galland",
            "Florian Levet"
        ],
        "date": "8 December 2023",
        "abstract": "This work combines the use of a robust 2D segmentation method, Stardist 2D, which have been trained on thousands of already available ground truth datasets, with the generation of pair of 3D masks and synthetic fluorescence volumes through a conditional GAN to segment nuclei in 3D when no specific ground truth exists. Nuclei segmentation is an important task in cell biology analysis that requires accurate and reliable methods, especially within complex low signal to noise ratio images with crowded cells populations. In this context, deep learning-based methods such as Stardist have emerged as the best performing solutions for segmenting nucleus. Unfortunately, the performances of such methods rely on the availability of vast libraries of ground truth hand-annotated data-sets, which become especially tedious to create for 3D cell cultures in which nuclei tend to overlap. In this work, we present a workflow to segment nuclei in 3D in such conditions when no specific ground truth exists. It combines the use of a robust 2D segmentation method, Stardist 2D, which have been trained on thousands of already available ground truth datasets, with the generation of pair of 3D masks and synthetic fluorescence volumes through a conditional GAN. It allows to train a Stardist 3D model with 3D ground truth masks and synthetic volumes that mimic our fluorescence ones. This strategy allows to segment 3D data that have no available ground truth, alleviating the need to perform manual annotations, and improving the results obtained by training Stardist with the original ground truth data.",
        "references": [
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "21d78c645b1029f7694f7742658e69a38e9e85e1",
            "a88ce45d29531e591802b4fb276b9e8d98b8788d",
            "630fe2f0389f8415fa39251b114b165d12dfd6f1",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "c97042386e3571fa39e6e97aeb7dd966636eef05",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "c4ecdad010b120c35724910e5afadcfee5d1d574"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "28"
    },
    {
        "Id": "1c6fdfc53c1bf2263a993121be5508a6d12b1142",
        "title": "Ensemble Processing and Synthetic Image Generation for Abnormally Shaped Nuclei Segmentation",
        "authors": [
            "Yue Han",
            "Yang Lei",
            "Viktor Shkolnikov",
            "Daisy Xin",
            "Alicia Auduong",
            "Steven Barcelo",
            "Jan P. Allebach",
            "Edward J. Delp"
        ],
        "date": "26 January 2023",
        "abstract": "This paper proposes a system to segment abnormally shaped nuclei with a limited amount of training data, and generates specific shapes of synthetic nuclei groundtruth, and designs an ensemble strategy to combine or fuse segmentation results from the Mask R-CNNs. Abnormalities in biological cell nuclei morphology are correlated with cell cycle stages, disease states, and various external stimuli. There have been many deep learning approaches that have described nuclei segmentation and analysis of nuclear morphology. One problem with many deep learning methods is acquiring large amounts of annotated nuclei data, which is generally expensive to obtain. In this paper, we propose a system to segment abnormally shaped nuclei with a limited amount of training data. We first generate specific shapes of synthetic nuclei groundtruth. We randomly sample these synthetic groundtruth images into training sets to train several Mask R-CNNs. We design an ensemble strategy to combine or fuse segmentation results from the Mask R-CNNs. We also design an oval nuclei removal by StarDist to reduce the false positives and improve the overall segmentation performance. Our experiments indicate that our method outperforms other methods in segmenting abnormally shaped nuclei.",
        "references": [
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "c9716a73db358852c2250814a8dc8c3646890028",
            "05bcc2f7dd8bc31a30cd6674a4667768b1c31db5",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "a91791f16efe2b2ba565c94b3e78003423ca1fd9",
            "21d78c645b1029f7694f7742658e69a38e9e85e1",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "ffcf67f3eab62c4a30bb77def214468bdf0b3ea5"
        ],
        "related_topics": [],
        "reference_count": "23",
        "citation_count": "One"
    },
    {
        "Id": "1d851de632015df478e8a055fd8f11543acdfa0f",
        "title": "AnyStar: Domain randomized universal star-convex 3D instance segmentation",
        "authors": [
            "Neel Dey",
            "S. M. Abulnaga",
            "Benjamin Billot",
            "Esra Abaci Turk",
            "Patricia Ellen Grant",
            "Adrian V. Dalca",
            "Polina Golland"
        ],
        "date": "13 July 2023",
        "abstract": "AnyStar is presented, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM, and placental cotyledons in human fetal MRI, all without any retraining, finetuning, transfer learning, or domain adaptation. Code is available at https://github.com/neel-dey/AnyStar.",
        "references": [
            "1648b4b355e15a814453c3a3c297fad815172f67",
            "40071396490823a1f110911ee314c8bdb6aea813",
            "0a5223eb33e90df3c177e25e157303f97c99f6dc",
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "142d050df6d346d602f8dde624552de919074259",
            "c7a73483923573ba0a652dd5bd78ada5cc486f66",
            "974ffaf2775a8c32e405c551436a111714a5dc3c",
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "7ae5b1b4b488473314d40711e4d2b0e8d7e210ed",
            "c97042386e3571fa39e6e97aeb7dd966636eef05"
        ],
        "related_topics": [
            "Finetuning",
            "Retraining",
            "3D Instance Segmentation",
            "3D Segments",
            "Nodule",
            "Domain Adaptation",
            "Radiology",
            "Star-convex Shapes",
            "Transfer Learning",
            "Reengineering"
        ],
        "reference_count": "61",
        "citation_count": "2"
    },
    {
        "Id": "f725c9a78c74c47db78c757503aca0d62679cfef",
        "title": "An Ensemble Method with Edge Awareness for Abnormally Shaped Nuclei Segmentation",
        "authors": [
            "Yue Han",
            "Yang Lei",
            "Viktor Shkolnikov",
            "Daisy Xin",
            "Alicia Auduong",
            "Steven Barcelo",
            "Jan P. Allebach",
            "Edward J. Delp"
        ],
        "date": "1 June 2023",
        "abstract": "This paper proposes a Transformer and CNN hybrid ensemble processing method with edge awareness for accurately segmenting abnormally shaped nuclei, and describes an ensemble processing strategy to combine or fuse individual segmentations from the CNN and the Transformer. Abnormalities in biological cell nuclei shapes are correlated with cell cycle stages, disease states, and various external stimuli. There have been many deep learning approaches that are being used for nuclei segmentation and analysis. In recent years, transformers have performed better than CNN methods on many computer vision tasks. One problem with many deep learning nuclei segmentation methods is acquiring large amounts of annotated nuclei data, which is generally expensive to obtain. In this paper, we propose a Transformer and CNN hybrid ensemble processing method with edge awareness for accurately segmenting abnormally shaped nuclei. We call this method Hybrid Edge Mask R-CNN (HER-CNN), which uses Mask R-CNNs with the ResNet and the Swin-Transformer to segment abnormally shaped nuclei. We add an edge awareness loss to the mask prediction step of the Mask R-CNN to better distinguish the edge difference between the abnormally shaped nuclei and typical oval nuclei. We describe an ensemble processing strategy to combine or fuse individual segmentations from the CNN and the Transformer. We introduce the use of synthetic ground truth image generation to supplement the annotated training images due to the limited amount of data. Our proposed method is compared with other segmentation methods for segmenting abnormally shaped nuclei. We also include ablation studies to show the effectiveness of the edge awareness loss and the use of synthetic ground truth images.",
        "references": [
            "1c6fdfc53c1bf2263a993121be5508a6d12b1142",
            "c9716a73db358852c2250814a8dc8c3646890028",
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "e0b984f5fce8535ba1306cd4d735aa0ed9ef7e62",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "3ab1348d060f3ab7e70a2e368a30af14bc57c35c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "a91791f16efe2b2ba565c94b3e78003423ca1fd9"
        ],
        "related_topics": [
            "Deep Learning",
            "Transformer",
            "Nuclei Segmentation",
            "Swin Transformer",
            "ResNet",
            "Convolutional Neural Network",
            "Ensemble Methods"
        ],
        "reference_count": "53",
        "citation_count": "5"
    },
    {
        "Id": "33e632c844d242a2cc3106f69069d3411df0cde8",
        "title": "3D Ground Truth Annotations of Nuclei in 3D Microscopy Volumes",
        "authors": [
            "Alain Chen",
            "Liming Wu",
            "Seth Winfree",
            "Kenneth W. Dunn",
            "Paul Salama",
            "Edward J. Delp"
        ],
        "date": "26 September 2022",
        "abstract": "A set of 3D microscopy volumes that have been partially manually annotated and synthetically generated volumes that can be used for training segmentation methods are described. In this paper we describe a set of 3D microscopy volumes we have partially manually annotated. We describe the volumes annotated and the tools and processes we use to annotate the volumes. In addition, we provide examples of annotated subvolumes. We also provide synthetically generated 3D microscopy volumes that can be used for training segmentation methods. The full set of annotations, synthetically generated volumes, and original volumes can be accessed as described in the paper.",
        "references": [
            "784f0a5eaf03f493fd065344de5bb39363d943ed",
            "21d78c645b1029f7694f7742658e69a38e9e85e1",
            "a91791f16efe2b2ba565c94b3e78003423ca1fd9",
            "ae6a81542262d3f51f4b1b791b82a71d9807eb64",
            "c9716a73db358852c2250814a8dc8c3646890028",
            "99f3d0831e13040e99f67620bdac56bdcf50f49f",
            "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "ff8a3d41bf4a3a6415d77a84ba58cd9b0b2c4869",
            "40af8e077b162551888ea348d16f458a12c147e5"
        ],
        "related_topics": [],
        "reference_count": "23",
        "citation_count": "2"
    },
    {
        "Id": "fde083063740b6c266aa78cf5d43ed5f4303ed4f",
        "title": "DeepSynth: Three-dimensional nuclear segmentation of biological images using neural networks trained with synthetic data",
        "authors": [
            "Kenneth W. Dunn",
            "Chichen Fu",
            "David Joon Ho",
            "Soonam Lee",
            "Shuo Han",
            "Paul Salama",
            "Edward J. Delp"
        ],
        "date": "1 December 2019",
        "abstract": "Comparisons with results obtained using commonly-used image processing packages demonstrate that DeepSynth provides the superior results associated with deep-learning techniques without the need for manual annotation. The scale of biological microscopy has increased dramatically over the past ten years, with the development of new modalities supporting collection of high-resolution fluorescence image volumes spanning hundreds of microns if not millimeters. The size and complexity of these volumes is such that quantitative analysis requires automated methods of image processing to identify and characterize individual cells. For many workflows, this process starts with segmentation of nuclei that, due to their ubiquity, ease-of-labeling and relatively simple structure, make them appealing targets for automated detection of individual cells. However, in the context of large, three-dimensional image volumes, nuclei present many challenges to automated segmentation, such that conventional approaches are seldom effective and/or robust. Techniques based upon deep-learning have shown great promise, but enthusiasm for applying these techniques is tempered by the need to generate training data, an arduous task, particularly in three dimensions. Here we present results of a new technique of nuclear segmentation using neural networks trained on synthetic data. Comparisons with results obtained using commonly-used image processing packages demonstrate that DeepSynth provides the superior results associated with deep-learning techniques without the need for manual annotation.",
        "references": [
            "5e1256abd795f7cda3711071512c5cdb01a8f808",
            "1ec20454b36b1181062af2587831b49d85253245",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "13a33c0fe3bb09f9c0dcaa60553130048d8e7048",
            "d1f788e9ac58de5d2dc6a3928939b411cf9ec5a6",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "16807204b95bb3db64d9b64e2e0cddeeed9c147a",
            "21d78c645b1029f7694f7742658e69a38e9e85e1",
            "fa927c77843592f486e7cc1a7d9e63f63a4f0f76",
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf"
        ],
        "related_topics": [
            "DeepSynth",
            "Squassh",
            "SpCycleGAN",
            "Microscopy Volumes",
            "Nuclear Segmentation",
            "Deep Learning",
            "Neural Network"
        ],
        "reference_count": "50",
        "citation_count": "67"
    },
    {
        "Id": "86e92a4b836db293a7a63d81e2a400846088648a",
        "title": "Segmentor: a tool for manual refinement of 3D microscopy annotations",
        "authors": [
            "David Borland",
            "Carolyn M. McCormick",
            "Niyanta K. Patel",
            "Oleh Krupa",
            "Jessica T. Mory",
            "Alvaro A. Beltran",
            "Tala M. Farah",
            "Carla F. Escobar-Tomlienovich",
            "Sydney S. Olson",
            "Minjeong Kim",
            "Guorong Wu",
            "Jason L. Stein"
        ],
        "date": "27 January 2021",
        "abstract": "It is shown that editing simultaneously in 2D and 3D using Segmentor significantly decreases time spent on manual annotations without affecting accuracy as compared to editing the same set of images with only 2D capabilities. Recent advances in tissue clearing techniques, combined with high-speed image acquisition through light sheet microscopy, enable rapid three-dimensional (3D) imaging of biological specimens, such as whole mouse brains, in a matter of hours. Quantitative analysis of such 3D images can help us understand how changes in brain structure lead to differences in behavior or cognition, but distinguishing densely packed features of interest, such as nuclei, from background can be challenging. Recent deep learning-based nuclear segmentation algorithms show great promise for automated segmentation, but require large numbers of accurate manually labeled nuclei as training data. We present Segmentor, an open-source tool for reliable, efficient, and user-friendly manual annotation and refinement of objects (e.g., nuclei) within 3D light sheet microscopy images. Segmentor employs a hybrid 2D-3D approach for visualizing and segmenting objects and contains features for automatic region splitting, designed specifically for streamlining the process of 3D segmentation of nuclei. We show that editing simultaneously in 2D and 3D using Segmentor significantly decreases time spent on manual annotations without affecting accuracy as compared to editing the same set of images with only 2D capabilities. Segmentor is a tool for increased efficiency of manual annotation and refinement of 3D objects that can be used to train deep learning segmentation algorithms, and is available at https://www.nucleininja.org/ and https://github.com/RENCI/Segmentor.",
        "references": [
            "3a840fbedbafbd436b6b2b85cd4fceb3ec006e01",
            "7e4f3d338e25add155abd339f9ed51ca5c6b1b3d",
            "3ad4b29906c8987bac46ea0ea9586b58a38fa8f4",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "fbf309595262bdadfebab64a5b868811716814b2",
            "b3816c29d0a696b4c562284b99bda3d906060d55",
            "a9c066cbba03b5fda843db3ab20a9ae212cd89d2",
            "fa1f107a5bfc66bc8b7430f612bd3a0c27b00304"
        ],
        "related_topics": [
            "Segmentors",
            "3D Segmentation",
            "Editing",
            "Whole Mouse Brain",
            "Deep Learning"
        ],
        "reference_count": "30",
        "citation_count": "11"
    },
    {
        "Id": "5e1256abd795f7cda3711071512c5cdb01a8f808",
        "title": "Nuclei segmentation of fluorescence microscopy images using convolutional neural networks",
        "authors": [
            "Chichen Fu",
            "David Joon Ho",
            "Shuo Han",
            "Paul Salama",
            "Kenneth W. Dunn",
            "Edward J. Delp"
        ],
        "date": "1 April 2017",
        "abstract": "A nuclei segmentation method using a deep convolutional neural network, data augmentation to generate training images of different shapes and contrasts, a refinement process combining segmentation results of horizontal, frontal, and sagittal planes in a volume, and a watershed technique to count the number of nuclei is described. Fluorescence microscopy has emerged as a powerful tool for studying cell biology because it enables the acquisition of 3D image volumes deeper into tissue and the imaging of complex subcellular structures. Quantitative analysis of these structures, which is needed to characterize the structure and constitution of tissue volumes, is facilitated by nuclei segmentation. However, manual segmentation is a laborious and intractable process due to the size and complexity of the data. In this paper, we describe a nuclei segmentation method using a deep convolutional neural network, data augmentation to generate training images of different shapes and contrasts, a refinement process combining segmentation results of horizontal, frontal, and sagittal planes in a volume, and a watershed technique to count the number of nuclei. Our results indicate that compared to 3D ground truth data, our method is able to successfully segment and count 3D nuclei.",
        "references": [
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "9b89eb27ab13df87ee217438d125431ce4d7d18f",
            "40521ea2dc363b5164e9bcdec9de908cab18b617",
            "c68f873005ca9f1352abf29ad4fb96d9e8eebe6c",
            "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "6179689e093d269c48574213a41acb7a4d3a8c6d",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "f667d8fbf63e49773eca2e83c8b644887b011b35",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "1b979ed50b3a7c68c79726d7c22e078c579501f3"
        ],
        "related_topics": [
            "Nuclei Segmentation",
            "Training Images"
        ],
        "reference_count": "35",
        "citation_count": "27"
    },
    {
        "Id": "5986547c7380f5a8fb6028093f827b3662f838a2",
        "title": "Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images",
        "authors": [
            "Simon Graham",
            "Quoc Dang Vu",
            "Shan E. Ahmed Raza",
            "Ayesha S Azam",
            "Yee-Wah Tsang",
            "Jin Tae Kwak",
            "Nasir M. Rajpoot"
        ],
        "date": "16 December 2018",
        "abstract": "Semantic Scholar extracted view of \"Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images\" by S. Graham et al.",
        "references": [
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "e312652daf82ed144d1696aae7ab412030d4f7eb",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "78db21fabea962592ac0aef54624251550929109",
            "9eb4f09a7de530a4b6ee1bd8bafed229275e4527",
            "114a42b24290829c16550781ec1a2d6c09fc969e",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102"
        ],
        "related_topics": [
            "HoVer-Net",
            "Nuclear Pixels",
            "CoNSeP",
            "Nuclear Instance Segmentation",
            "CPM-17",
            "Nuclear Segmentation",
            "Computational Pathology",
            "Nuclear Instances",
            "Nuclear Type",
            "Nuclear Classification"
        ],
        "reference_count": "49",
        "citation_count": "623"
    },
    {
        "Id": "a34a4934642bdc26326cca976cdeee308c932db6",
        "title": "A Segmentation Method for fluorescence images without a machine learning approach",
        "authors": [
            "G. Giacopelli",
            "Michele Migliore",
            "Domenico Tegolo"
        ],
        "date": "28 December 2022",
        "abstract": "The excellent performance of the deterministic method (NeuronalAlg) to segment cells and nuclei from fluorescence images was measured with quantitative indicators and compared with those achieved by three published ML approaches. Background: Image analysis applications in digital pathology include various methods for segmenting regions of interest. Their identification is one of the most complex steps, and therefore of great interest for the study of robust methods that do not necessarily rely on a machine learning (ML) approach. Method: A fully automatic and optimized segmentation process for different datasets is a prerequisite for classifying and diagnosing Indirect ImmunoFluorescence (IIF) raw data. This study describes a deterministic computational neuroscience approach for identifying cells and nuclei. It is far from the conventional neural network approach, but it is equivalent to their quantitative and qualitative performance, and it is also solid to adversative noise. The method is robust, based on formally correct functions, and does not suffer from tuning on specific data sets. Results: This work demonstrates the robustness of the method against the variability of parameters, such as image size, mode, and signal-to-noise ratio. We validated the method on two datasets (Neuroblastoma and NucleusSegData) using images annotated by independent medical doctors. Conclusions: The definition of deterministic and formally correct methods, from a functional to a structural point of view, guarantees the achievement of optimized and functionally correct results. The excellent performance of our deterministic method (NeuronalAlg) to segment cells and nuclei from fluorescence images was measured with quantitative indicators and compared with those achieved by three published ML approaches.",
        "references": [
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "e33f7928a45e395f1bc2a1aa1dd9349bc725ba24",
            "10e5d5962562703099611e4b711a6de9a710a895",
            "6cf6f0f14d2c58ccf92db17e8b64eaee05ca40e3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "848a875695c1c8c8599962d944f9ca20cda00741",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "6ab709787933bf9b765ba77c066abed29747ee17"
        ],
        "related_topics": [
            "Digital Pathology",
            "Indirect Immunofluorescence",
            "Machine Learning",
            "Parameters",
            "Image Size"
        ],
        "reference_count": "0",
        "citation_count": "62"
    },
    {
        "Id": "cf8f6507d9c4580d749b370507b10104765d075a",
        "title": "NeuronAlg: An Innovative Neuronal Computational Model for Immunofluorescence Image Segmentation",
        "authors": [
            "G. Giacopelli",
            "Michele Migliore",
            "Domenico Tegolo"
        ],
        "date": "1 May 2023",
        "abstract": "The excellent performance of the deterministic method (NeuronalAlg) in segmenting cells and nuclei from fluorescence images was measured with quantitative indicators and compared with those achieved by three published ML approaches. Background: Image analysis applications in digital pathology include various methods for segmenting regions of interest. Their identification is one of the most complex steps and therefore of great interest for the study of robust methods that do not necessarily rely on a machine learning (ML) approach. Method: A fully automatic and optimized segmentation process for different datasets is a prerequisite for classifying and diagnosing indirect immunofluorescence (IIF) raw data. This study describes a deterministic computational neuroscience approach for identifying cells and nuclei. It is very different from the conventional neural network approaches but has an equivalent quantitative and qualitative performance, and it is also robust against adversative noise. The method is robust, based on formally correct functions, and does not suffer from having to be tuned on specific data sets. Results: This work demonstrates the robustness of the method against variability of parameters, such as image size, mode, and signal-to-noise ratio. We validated the method on three datasets (Neuroblastoma, NucleusSegData, and ISBI 2009 Dataset) using images annotated by independent medical doctors. Conclusions: The definition of deterministic and formally correct methods, from a functional and structural point of view, guarantees the achievement of optimized and functionally correct results. The excellent performance of our deterministic method (NeuronalAlg) in segmenting cells and nuclei from fluorescence images was measured with quantitative indicators and compared with those achieved by three published ML approaches.",
        "references": [
            "10e5d5962562703099611e4b711a6de9a710a895",
            "848a875695c1c8c8599962d944f9ca20cda00741",
            "e33f7928a45e395f1bc2a1aa1dd9349bc725ba24",
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "a3ff2b4a748b1ab37c4643e8fda87249f62868ca",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "f28e387d4229c5f690ce4570a391c0f47e7155c7",
            "712e32e2da67428ba6c6add1605410e1c3792883"
        ],
        "related_topics": [
            "Digital Pathology",
            "Image Segmentation",
            "Indirect Immunofluorescence",
            "Parameters",
            "Image Size",
            "Machine Learning"
        ],
        "reference_count": "0",
        "citation_count": "94"
    },
    {
        "Id": "d582d503e24fadc4e431921370a056c6e61c7dc1",
        "title": "Development of Autoencoder Based Models for High Accuracy Nuclear Segmentation in Fluorescent Microscope Systems",
        "authors": [
            "S{\\&quot;u}meyye Nur Emi\u0307r",
            "Sibel Dani\u015fmaz",
            "Hulya Dogan",
            "Ramazan Ozgur Dogan"
        ],
        "date": "15 September 2023",
        "abstract": "Mikroskobik sistemlerde doku veya h\u00fccre numunelerinde n\u00fckleer morfoloji veya biyolojik belirte\u00e7ler gibi b\u00f6l\u00fcmleri g\u00f6rselle\u015ftirmek i\u00e7in hematoksilen ve eozin (Hematoxylin and eosin - H&E) histolojik boyamalar, imm\u00fcnohistokimyasal (immunohistovhemical - IHC) ve imm\u00fcnofloresan (immunofluorescence - IF) boyama yakla\u015f\u0131mlar\u0131 geli\u015ftirilmi\u015ftir. H&E veya IHC boyamalar ile kar\u015f\u0131la\u015ft\u0131r\u0131ld\u0131\u011f\u0131nda, IF boyamalar\u0131n say\u0131sala aktar\u0131lmas\u0131 uzmanlar i\u00e7in daha zorlu ve zaman al\u0131c\u0131 olmaktad\u0131r. Fakat, IF boyama yakla\u015f\u0131mlar\u0131nda daha fazla h\u00fccresel veya h\u00fccre alt\u0131 belirte\u00e7 g\u00f6r\u00fcnt\u00fclenebilmektedir. Floresan mikroskoplardan elde edilmi\u015f n\u00fckleer segmentasyonunun y\u00fcksek do\u011frulukla otomatik ger\u00e7ekle\u015ftirilmesi IF boyama yakla\u015f\u0131mlar\u0131ndaki h\u00fccreler hakk\u0131nda daha fazla bilgi elde edilmesini sa\u011flamaktad\u0131r. Literat\u00fcrde di\u011fer mikroskobik sistemlerden elde edilmi\u015f g\u00f6r\u00fcnt\u00fclerde h\u00fccre veya doku segmentasyonu i\u00e7in bir\u00e7ok \u00e7al\u0131\u015fma geli\u015ftirilmi\u015f ve y\u00fcksek do\u011fruluklu sonu\u00e7lar elde edilmi\u015ftir. Fakat di\u011fer alanlarda ger\u00e7ekle\u015ftirilen bu ba\u015far\u0131, floresan mikroskoplardan elde edilmi\u015f g\u00f6r\u00fcnt\u00fclerdeki n\u00fckleer segmentasyonu i\u00e7in elde edilmemi\u015ftir. Bu kapsamda, \u00e7al\u0131\u015fmada floresan mikroskop sistemlerinde n\u00fckleer segmentasyonu i\u00e7in y\u00fcksek do\u011fruluklu otomatik kodlay\u0131c\u0131 modelleri geli\u015ftirilmektedir. Geli\u015ftirilen otomatik kodlay\u0131c\u0131 modellerinin analizi uzman ki\u015filer taraf\u0131ndan i\u015faretlenmi\u015f, floresan mikroskop g\u00f6r\u00fcnt\u00fclerinden olu\u015fan veri seti kullan\u0131larak ger\u00e7ekle\u015ftirilmektedir. \u00c7al\u0131\u015fmada kullan\u0131lan performans de\u011ferlendirme prosed\u00fcrleri a\u00e7\u0131s\u0131ndan, ger\u00e7ekle\u015ftirilen otomatik kodlay\u0131c\u0131 modellerinin ba\u015far\u0131lar\u0131n\u0131n otomatik n\u00fckleer segmentasyon i\u00e7in tatmin edici oldu\u011fu a\u00e7\u0131k\u00e7a g\u00f6r\u00fclmektedir.",
        "references": [
            "866eef39af3ccfa7d7f69c57bf6ec834461b6702",
            "1ef2f0faed2d9b1d819b130f950a143009816b2f",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "e9324f71051b11fe40f594e992c918a7d572bbc5",
            "081fbe9f5a7f0674edca9c1e040f13373971d56b",
            "da318d257b9c010ab139f4db2ed9a349fe0f4e8a",
            "3d8e0fe8c0f5c8648d59d72a87a85b4af45e8f0d",
            "8d4ee285ea54841b8b9953aa7c325b96d636d6d7",
            "5d90bf78c959be9821ad4e0fcb83d575e98d495c",
            "e33f7928a45e395f1bc2a1aa1dd9349bc725ba24"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "16"
    },
    {
        "Id": "e9324f71051b11fe40f594e992c918a7d572bbc5",
        "title": "An annotated fluorescence image dataset for training nuclear segmentation methods",
        "authors": [
            "Florian Kromp",
            "Eva Bozsaky",
            "Fikret Rifatbegovic",
            "Lukas Fischer",
            "M. I. Ambros",
            "Maria Berneder",
            "Tamara Weiss",
            "Daria Lazic",
            "Wolfgang Doerr",
            "Allan Hanbury",
            "Klaus Beiske",
            "Peter F. Ambros",
            "Inge M. Ambros",
            "Sabine Taschner-Mandl"
        ],
        "date": "11 August 2020",
        "abstract": "A comprehensive, annotated dataset including tightly aggregated nuclei of multiple tissues for the training of machine learning-based nuclear segmentation algorithms is presented and the heterogeneity of the dataset is demonstrated with respect to multiple parameters such as magnification, modality, signal-to-noise ratio and diagnosis. Fully-automated nuclear image segmentation is the prerequisite to ensure statistically significant, quantitative analyses of tissue preparations,applied in digital pathology or quantitative microscopy. The design of segmentation methods that work independently of the tissue type or preparation is complex, due to variations in nuclear morphology, staining intensity, cell density and nuclei aggregations. Machine learning-based segmentation methods can overcome these challenges, however high quality expert-annotated images are required for training. Currently, the limited number of annotated fluorescence image datasets publicly available do not cover a broad range of tissues and preparations. We present a comprehensive, annotated dataset including tightly aggregated nuclei of multiple tissues for the training of machine learning-based nuclear segmentation algorithms. The proposed dataset covers sample preparation methods frequently used in quantitative immunofluorescence microscopy. We demonstrate the heterogeneity of the dataset with respect to multiple parameters such as magnification, modality, signal-to-noise ratio and diagnosis. Based on a suggested split into training and test sets and additional single-nuclei expert annotations, machine learning-based image segmentation methods can be trained and evaluated. Measurement(s) nucleus \u2022 Annotation \u2022 Frozen Section \u2022 Neuroblastoma \u2022 Touch Prep Slide \u2022 Centrifuged Smear Slide \u2022 cells grown on slide \u2022 Ganglioneuroblastoma \u2022 Wilms Tumor \u2022 HaCaT cell Technology Type(s) Fluorescence Imaging \u2022 machine learning Factor Type(s) nucleus segmentation Sample Characteristic - Organism Homo sapiens Measurement(s) nucleus \u2022 Annotation \u2022 Frozen Section \u2022 Neuroblastoma \u2022 Touch Prep Slide \u2022 Centrifuged Smear Slide \u2022 cells grown on slide \u2022 Ganglioneuroblastoma \u2022 Wilms Tumor \u2022 HaCaT cell Technology Type(s) Fluorescence Imaging \u2022 machine learning Factor Type(s) nucleus segmentation Sample Characteristic - Organism Homo sapiens Machine-accessible metadata file describing the reported data: https://doi.org/10.6084/m9.figshare.12570854",
        "references": [
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "b4a0cb0837c47d325ccb0a8bded7baaf23753932",
            "13de33ee941f1ebf3ed185c20fb4453a07302c30",
            "311b041f89a07fea9354dc88d0e2380beb43eb6d",
            "70a1b232764460758101f4da2dbfe89b406dd0cb",
            "8c66e4dbc28730f185b3c9395f78d8649a2b830f",
            "bd898f483476e3dcacf83cd85efc64e6319da0e1",
            "2807ed15a4ec156e714e8688c07ca7ec7e2be5f6",
            "0e8b061e08eb1ac8968e44edc0e54da658afad0e",
            "2606fe6df5973e60509cd6b4637c0231e430cd40"
        ],
        "related_topics": [
            "Digital Pathology",
            "Annotated"
        ],
        "reference_count": "27",
        "citation_count": "46"
    },
    {
        "Id": "c89bfd998b0a6c656010b629814ab0cad3cff72e",
        "title": "Evaluation of Deep Learning Strategies for Nucleus Segmentation in Fluorescence Images",
        "authors": [
            "Juan C. Caicedo",
            "Jonathan F Roth",
            "Allen Goodman",
            "Tim Becker",
            "Kyle W. Karhohs",
            "Claire McQuin",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "31 May 2018",
        "abstract": "This work presents an evaluation framework to measure accuracy, types of errors, and computational efficiency; and uses it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. Identifying nuclei is often a critical first step in analyzing microscopy images of cells, and classical image processing algorithms are most commonly used for this task. Recent developments in deep learning can yield superior accuracy, but typical evaluation metrics for nucleus segmentation do not satisfactorily capture error modes that are relevant in cellular images. Besides, large image data sets with ground truth for evaluation have been limiting. We present an evaluation framework to measure accuracy, types of errors, and computational efficiency; and use it to compare two deep learning strategies (U-Net and DeepCell) alongside a classical approach implemented in CellProfiler. We publicly release a set of 23,165 manually annotated nuclei and source code to reproduce experiments. Our results show that U-Net outperforms both pixel-wise classification networks and classical algorithms. Also, our evaluation framework shows that deep learning improves accuracy and reduces the number of biologically relevant errors by half.",
        "references": [
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "cdecac6e2f578cfc56140e00aaa74a78f864fea2",
            "318f82a3e593e391cfd0da7964b16d83299aa943",
            "2d8d74c7fd9375b17e9d6123773919b2aa512c56",
            "1268de7bda769e651dc6c089d006c7edbe37f563",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "36748de338909976f72ffbadaf097470ec040da0"
        ],
        "related_topics": [
            "DeepCell",
            "Broad Bioimage Benchmark Collection",
            "Deep Learning",
            "Nucleus Segmentation",
            "International Society For Advancement Of Cytometry",
            "Cytometry"
        ],
        "reference_count": "53",
        "citation_count": "226"
    },
    {
        "Id": "0ea3e5215ac2676a15bef2354b2938704a0789a3",
        "title": "Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images",
        "authors": [
            "Lipeng Xie",
            "Jin Qi",
            "Lili Pan",
            "Samad Wali"
        ],
        "date": "1 February 2020",
        "abstract": "Semantic Scholar extracted view of \"Integrating deep convolutional neural networks with marker-controlled watershed for overlapping nuclei segmentation in histopathology images\" by Lipeng Xie et al.",
        "references": [
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "78db21fabea962592ac0aef54624251550929109",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "3ba787154b2411fe89d88b6ab50ea84aa62dea04",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "0aa9c29e32cb13b2d011e5730bd4955c7962083b"
        ],
        "related_topics": [
            "Marker-Controlled Watershed",
            "Nuclei Segmentation",
            "Histopathology Images",
            "Instance Segmentation"
        ],
        "reference_count": "44",
        "citation_count": "43"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
        "title": "A Dataset and a Technique for Generalized Nuclear Segmentation for Computational Pathology",
        "authors": [
            "Neeraj Kumar",
            "Ruchika Verma",
            "Sanuj Sharma",
            "S. K. Bhargava",
            "Abhishek Vahadane",
            "Amit Sethi"
        ],
        "date": "6 March 2017",
        "abstract": "A large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries is introduced, whose quality was validated by a medical doctor. Nuclear segmentation in digital microscopic tissue images can enable extraction of high-quality features for nuclear morphometrics and other analysis in computational pathology. Conventional image processing techniques, such as Otsu thresholding and watershed segmentation, do not work effectively on challenging cases, such as chromatin-sparse and crowded nuclei. In contrast, machine learning-based segmentation can generalize across various nuclear appearances. However, training machine learning algorithms requires data sets of images, in which a vast number of nuclei have been annotated. Publicly accessible and annotated data sets, along with widely agreed upon metrics to compare techniques, have catalyzed tremendous innovation and progress on other image classification problems, particularly in object recognition. Inspired by their success, we introduce a large publicly accessible data set of hematoxylin and eosin (H&E)-stained tissue images with more than 21000 painstakingly annotated nuclear boundaries, whose quality was validated by a medical doctor. Because our data set is taken from multiple hospitals and includes a diversity of nuclear appearances from several patients, disease states, and organs, techniques trained on it are likely to generalize well and work right out-of-the-box on other H&E-stained images. We also propose a new metric to evaluate nuclear segmentation results that penalizes object- and pixel-level errors in a unified manner, unlike previous metrics that penalize only one type of error. We also propose a segmentation technique based on deep learning that lays a special emphasis on identifying the nuclear boundaries, including those between the touching or overlapping nuclei, and works well on a diverse set of test images.",
        "references": [
            "e312652daf82ed144d1696aae7ab412030d4f7eb",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "ee07d8530c080caa5056a60bcc176569544a8927",
            "6c1108c63693a08da9d8fb418b14289bae73cc66",
            "13de33ee941f1ebf3ed185c20fb4453a07302c30",
            "930b28f5255a6633ec89bdafb9f72af8e9148fac",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "856a3c76453a798556096cd23848402d1351c1f9"
        ],
        "related_topics": [
            "Nuclear Segmentation",
            "Digital Microscopic Tissue Images",
            "Nuclear Morphometrics",
            "Nuclear Boundaries",
            "Computational Pathology",
            "Overlapping Nuclei",
            "Machine Learning",
            "Eosin",
            "Deep Learning",
            "Object Recognition"
        ],
        "reference_count": "46",
        "citation_count": "663"
    },
    {
        "Id": "5986547c7380f5a8fb6028093f827b3662f838a2",
        "title": "Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images",
        "authors": [
            "Simon Graham",
            "Quoc Dang Vu",
            "Shan E. Ahmed Raza",
            "Ayesha S Azam",
            "Yee-Wah Tsang",
            "Jin Tae Kwak",
            "Nasir M. Rajpoot"
        ],
        "date": "16 December 2018",
        "abstract": "Semantic Scholar extracted view of \"Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images\" by S. Graham et al.",
        "references": [
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "e312652daf82ed144d1696aae7ab412030d4f7eb",
            "26d5ad3849f97cfa9fa61f68330e3fa31f261668",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "78db21fabea962592ac0aef54624251550929109",
            "9eb4f09a7de530a4b6ee1bd8bafed229275e4527",
            "114a42b24290829c16550781ec1a2d6c09fc969e",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102"
        ],
        "related_topics": [
            "HoVer-Net",
            "Nuclear Pixels",
            "CoNSeP",
            "Nuclear Instance Segmentation",
            "CPM-17",
            "Nuclear Segmentation",
            "Computational Pathology",
            "Nuclear Instances",
            "Nuclear Type",
            "Nuclear Classification"
        ],
        "reference_count": "49",
        "citation_count": "623"
    },
    {
        "Id": "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
        "title": "Test-time augmentation for deep learning-based cell segmentation on microscopy images",
        "authors": [
            "Nikita Moshkov",
            "Botond M{\\&#x27;a}th{\\&#x27;e}",
            "Attila Kert{\\&#x27;e}sz-Farkas",
            "R{\\&#x27;e}ka Hollandi",
            "P{\\&#x27;e}ter Horv{\\&#x27;a}th"
        ],
        "date": "23 October 2019",
        "abstract": "This paper describes how the test-time argumentation prediction method is incorporated into two major segmentation approaches utilized in the single-cell analysis of microscopy images, and shows that even if only simple test- time augmentations are applied, TTA can significantly improve prediction accuracy. Recent advancements in deep learning have revolutionized the way microscopy images of cells are processed. Deep learning network architectures have a large number of parameters, thus, in order to reach high accuracy, they require a massive amount of annotated data. A common way of improving accuracy builds on the artificial increase of the training set by using different augmentation techniques. A less common way relies on test-time augmentation (TTA) which yields transformed versions of the image for prediction and the results are merged. In this paper we describe how we have incorporated the test-time argumentation prediction method into two major segmentation approaches utilized in the single-cell analysis of microscopy images. These approaches are semantic segmentation based on the U-Net, and instance segmentation based on the Mask R-CNN models. Our findings show that even if only simple test-time augmentations (such as rotation or flipping and proper merging methods) are applied, TTA can significantly improve prediction accuracy. We have utilized images of tissue and cell cultures from the Data Science Bowl (DSB) 2018 nuclei segmentation competition and other sources. Additionally, boosting the highest-scoring method of the DSB with TTA, we could further improve prediction accuracy, and our method has reached an ever-best score at the DSB.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "41c9bfb05a7c3eeafe3f749eae1acf6afd873194",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "6ab709787933bf9b765ba77c066abed29747ee17",
            "172df6d55b81f184ab0042c49634ccf9b72ed253",
            "4ee0a11ad31d26eed38d1193772d12900fbfabe2"
        ],
        "related_topics": [
            "Test-time Augmentation",
            "Deep Learning",
            "Transport-Triggered Architecture",
            "U-Net",
            "Training Set",
            "Instance Segmentation",
            "Cell Segmentation",
            "Mask R-CNN Model",
            "Parameters",
            "Semantic Segmentation"
        ],
        "reference_count": "22",
        "citation_count": "140"
    },
    {
        "Id": "7b83fd62774adc795464ca3ad9b3c238c9ce5daf",
        "title": "Structure-Preserving Multi-Domain Stain Color Augmentation using Style-Transfer with Disentangled Representations",
        "authors": [
            "Sophia J. Wagner",
            "Nadieh Khalili",
            "Raghav Sharma",
            "Melanie Boxberg",
            "Carsten Marr",
            "Walter de Back",
            "Tingying Peng"
        ],
        "date": "26 July 2021",
        "abstract": "This work proposes a novel color augmentation technique, HistAuGAN, that can simulate a wide variety of realistic histology stain colors, thus making neural networks stain-invariant when applied during training and outperforms conventional color augmented techniques on a classification task on the publicly available dataset Camelyon17. In digital pathology, different staining procedures and scanners cause substantial color variations in whole-slide images (WSIs), especially across different laboratories. These color shifts result in a poor generalization of deep learning-based methods from the training domain to external pathology data. To increase test performance, stain normalization techniques are used to reduce the variance between training and test domain. Alternatively, color augmentation can be applied during training leading to a more robust model without the extra step of color normalization at test time. We propose a novel color augmentation technique, HistAuGAN, that can simulate a wide variety of realistic histology stain colors, thus making neural networks stain-invariant when applied during training. Based on a generative adversarial network (GAN) for image-to-image translation, our model disentangles the content of the image, i.e., the morphological tissue structure, from the stain color attributes. It can be trained on multiple domains and, therefore, learns to cover different stain colors as well as other domain-specific variations introduced in the slide preparation and imaging process. We demonstrate that HistAuGAN outperforms conventional color augmentation techniques on a classification task on the publicly available dataset Camelyon17 and show that it is able to mitigate present batch effects.",
        "references": [
            "eca28fd9bb5233da0df3e1579a6dceb4157bce6c",
            "455c14205e23d9cb709371ceb605f92e4fbbda2f",
            "d6e975989345b69f539dbb8f22cb3437f5cc5039",
            "e926486ab0dfd772d7da41489b47da0db935b3d8",
            "d357531a42611f9b9db253fd22c77e5709157448",
            "fd31706da5fdd11d0bd28718a51f2337cd1fcf82",
            "a243c7bbd2affd548e18a35dadf0313c1b2198c2",
            "2729d2918978d5ed602aa843fbdd027d83e0036f",
            "3d5d9d8e74b215609eabba80ef79a35ebf460e49",
            "c42faad6f2ab45bb3a1cf27408cae32f575632b6"
        ],
        "related_topics": [
            "Color Augmentation",
            "Multi-Domain",
            "Staining Procedures",
            "Style Transfer",
            "Generative Adversarial Networks",
            "WSIs",
            "Classification Task",
            "Batch Effects",
            "Multiple Domains",
            "Digital Pathology"
        ],
        "reference_count": "19",
        "citation_count": "31"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "authors": [
            "Olaf Ronneberger",
            "Philipp Fischer",
            "Thomas Brox"
        ],
        "date": "18 May 2015",
        "abstract": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "references": [
            "6fc6803df5f9ae505cae5b2f178ade4062c768d0",
            "09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "eb42cf88027de515750f230b23b1a057dc782108",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "4f03888450fde9e8234f616badaae499740e57a4",
            "3b2ccc97f1433cf8750a2ad5a05555ccd10e9cdf",
            "3bffa23a16c273ac2228a13e65dade6766ce7777"
        ],
        "related_topics": [
            "U-Net",
            "Contracting Path",
            "ISBI Cell Tracking Challenge 2015",
            "Sliding-window Convolutional Network",
            "Neuronal Structures",
            "Expansive Path",
            "Biomedical Segmentation",
            "ISBI 2012",
            "U-net Architecture",
            "Drosophila First Instar Larva Ventral Nerve Cord"
        ],
        "reference_count": "18",
        "citation_count": "56,807"
    },
    {
        "Id": "1a0912bb76777469295bb2c059faee907e7f3258",
        "title": "Mask R-CNN",
        "authors": [
            "Kaiming He",
            "Georgia Gkioxari",
            "Piotr Doll{\\&#x27;a}r",
            "Ross B. Girshick"
        ],
        "date": "20 March 2017",
        "abstract": "This work presents a conceptually simple, flexible, and general framework for object instance segmentation, which extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.",
        "references": [
            "342786659379879f58bf5c4ff43c84c83a6a7389",
            "888ddf8f543b2e0794f4a021d80a23d4eb05c8af",
            "b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "2a94c84383ee3de5e6211d43d16e7de387f68878",
            "7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "6b8d0df903496699e52b4daee5d1815b7b784cf7",
            "361b19d2c00d086fa8ef860374f5e1d862fd2f30",
            "3ad998a9b2c071c4a1971048f8a2d754530f08e8"
        ],
        "related_topics": [
            "Mask R-CNN",
            "Bounding Box Recognition",
            "Instance Segmentation",
            "RoIAlign",
            "RoIPool",
            "ResNet-101-FPN",
            "Faster R-CNN",
            "Object Instance Segmentation",
            "ResNet-50-FPN",
            "RoI Features"
        ],
        "reference_count": "41",
        "citation_count": "19,499"
    },
    {
        "Id": "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852",
        "title": "Deep learning for cellular image analysis",
        "authors": [
            "Erick Moen",
            "Dylan Bannon",
            "Takamasa Kudo",
            "William Graf",
            "Markus W. Covert",
            "David Van Valen"
        ],
        "date": "27 May 2019",
        "abstract": "The intersection between deep learning and cellular image analysis is reviewed and an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists are provided. Recent advances in computer vision and machine learning underpin a collection of algorithms with an impressive ability to decipher the content of images. These deep learning algorithms are being applied to biological images and are transforming the analysis and interpretation of imaging data. These advances are positioned to render difficult analyses routine and to enable researchers to carry out new, previously impossible experiments. Here we review the intersection between deep learning and cellular image analysis and provide an overview of both the mathematical mechanics and the programming frameworks of deep learning that are pertinent to life scientists. We survey the field\u2019s progress in four key applications: image classification, image segmentation, object tracking, and augmented microscopy. Last, we relay our labs\u2019 experience with three key aspects of implementing deep learning in the laboratory: annotating training data, selecting and training a range of neural network architectures, and deploying solutions. We also highlight existing datasets and implementations for each surveyed application. A Review on applications of deep machine learning in image analysis that offers practical guidance for biologists.",
        "references": [
            "08dc94471605308669c8d3d8284ba94fcc93e345",
            "0cbc480e0d380bbaa04bfb21a396c9e8da6e930e",
            "d1f788e9ac58de5d2dc6a3928939b411cf9ec5a6",
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "efd68f3724942c9de5dc804d3c7cb3f70f42234b",
            "7dae942104dc8283504ce7a492c9ca12fa119189",
            "9f7a89bc9b8ebb7152acacc95a84daead92d8f2c",
            "c051d469f9e9d5550d4633deecdb1ea8b11862dd",
            "f140829b7c37fb5d51713f631f720b1c7746d5e7"
        ],
        "related_topics": [
            "Cellular Image Analysis",
            "Augmented Microscopy",
            "Deep Learning",
            "Machine Learning",
            "Image Segmentation",
            "Object Tracking",
            "Computer Vision"
        ],
        "reference_count": "186",
        "citation_count": "696"
    },
    {
        "Id": "3d5d9d8e74b215609eabba80ef79a35ebf460e49",
        "title": "DRIT++: Diverse Image-to-Image Translation via Disentangled Representations",
        "authors": [
            "Hsin-Ying Lee",
            "Hung-Yu Tseng",
            "Jia-Bin Huang",
            "Maneesh Kumar Singh",
            "Ming-Hsuan Yang"
        ],
        "date": "2 May 2019",
        "abstract": "This work presents an approach based on disentangled representation for generating diverse outputs without paired training images that can generate diverse and realistic images on a wide range of tasks without pairedTraining data. Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: (1) lack of aligned training pairs and (2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Fr\u00e9chet inception distance, and measure diversity with the perceptual distance metric, Jensen\u2013Shannon divergence, and number of statistically-different bins.",
        "references": [
            "60bc358296ae11ac8f11286bba0a49ac7e797d26",
            "4070bf3a68b70ab52de35a076a17941543001be2",
            "c43d954cf8133e6254499f3d68e45218067e4941",
            "0042b3ae1985873d4fb9e32c7225e4024a326a92",
            "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
            "7242666492464a5408fc072f5c4f51502e0d0e5b",
            "60104351ac65115503c9e92e856bcab6a13b0ce8",
            "4a5bd1f2935f9a3c33f1a103a1d43377e61265a0",
            "b69badabc3fddc9710faa44c530473397303b0b9",
            "7bb804466057c5ae57c58a8aff4e0c96fe3f6c18"
        ],
        "related_topics": [
            "DRIT++",
            "Domain-specific Attribute Space",
            "Domain-invariant Content Space",
            "Introduction Image To Image",
            "Cross-cycle Consistency Loss",
            "Image-to-Image Translation",
            "Diverse Outputs",
            "Disentangled Representation Framework",
            "Latent Regression Loss",
            "Content Discriminator"
        ],
        "reference_count": "67",
        "citation_count": "999"
    },
    {
        "Id": "bd898f483476e3dcacf83cd85efc64e6319da0e1",
        "title": "Histopathological Image Analysis: A Review",
        "authors": [
            "Metin N. Gurcan",
            "Laura E. Boucheron",
            "Ali Can",
            "Anant Madabhushi",
            "Nasir M. Rajpoot",
            "B{\\&quot;u}lent Yener"
        ],
        "date": "30 October 2009",
        "abstract": "The recent state of the art CAD technology for digitized histopathology is reviewed and the development and application of novel image analysis technology for a few specific histopathological related problems being pursued in the United States and Europe are described. Over the past decade, dramatic increases in computational power and improvement in image analysis algorithms have allowed the development of powerful computer-assisted analytical approaches to radiological data. With the recent advent of whole slide digital scanners, tissue histopathology slides can now be digitized and stored in digital image form. Consequently, digitized tissue histopathology has now become amenable to the application of computerized image analysis and machine learning techniques. Analogous to the role of computer-assisted diagnosis (CAD) algorithms in medical imaging to complement the opinion of a radiologist, CAD algorithms have begun to be developed for disease detection, diagnosis, and prognosis prediction to complement the opinion of the pathologist. In this paper, we review the recent state of the art CAD technology for digitized histopathology. This paper also briefly describes the development and application of novel image analysis technology for a few specific histopathology related problems being pursued in the United States and Europe.",
        "references": [
            "ec60b6361d200a837601ac432a7a087c9d8abf83",
            "31e53eaf795ff31ea949f23a276d7db2668f6fbc",
            "1673c83bee30d3e918129fc4afd9c1913aad82a7",
            "1e8825249b6dfffd8b1fbe938cd96e2de2f48a53",
            "ca4ba745d8b2670282627828af7b8b51d7ed972c",
            "66d829baf896f7475e5e6345d4b6d6687d0bfc64",
            "e977c59be9e7490a985fccfda35cf2cf69a22e4e",
            "66783753253af1ed5bc2b3410035b7c6b27f925d",
            "d0ad4142e4e7fb052935b349dc67bac288c83ff3",
            "b5052fa21c9d3a662f9f2494e6893c4e1371568b"
        ],
        "related_topics": [
            "Whole Slide Digital Scanners",
            "Histopathological Image Analysis",
            "Pathologist",
            "Digitized Histopathology",
            "Radiologist",
            "Medical Imaging",
            "Cylindrical Algebraic Decomposition"
        ],
        "reference_count": "142",
        "citation_count": "1,632"
    },
    {
        "Id": "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
        "title": "Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl",
        "authors": [
            "Juan C. Caicedo",
            "Allen Goodman",
            "Kyle W. Karhohs",
            "Beth A. Cimini",
            "Jeanelle Ackerman",
            "Marzieh Haghighi",
            "Cherkeng Heng",
            "Tim Becker",
            "Minh Doan",
            "Claire McQuin",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "21 October 2019",
        "abstract": "The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Segmenting the nuclei of cells in microscopy images is often the first step in the quantitative analysis of imaging data for biological and biomedical applications. Many bioimage analysis tools can segment nuclei in images but need to be selected and configured for every experiment. The 2018 Data Science Bowl attracted 3,891 teams worldwide to make the first attempt to build a segmentation method that could be applied to any two-dimensional light microscopy image of stained nuclei across experiments, with no human interaction. Top participants in the challenge succeeded in this task, developing deep-learning-based models that identified cell nuclei across many image types and experimental conditions without the need to manually adjust segmentation parameters. This represents an important step toward configuration-free bioimage analysis software tools. The 2018 Data Science Bowl challenged competitors to develop an accurate tool for segmenting stained nuclei from diverse light microscopy images. The winners deployed innovative deep-learning strategies to realize configuration-free segmentation.",
        "references": [
            "c89bfd998b0a6c656010b629814ab0cad3cff72e",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7",
            "e1b3ad532f9346d51578c32dc6b070c9744a8d88",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "68f30bd22817a17adc837eb285e51c9628f00e8d",
            "31a27c6500b6652d7ecc055c9b08457ad90128c1",
            "cee3035635eafa1c412934419d7563fdb06d73ae",
            "48cc41c7b2fac21d7bbd2988c5c6a2c5f9744852"
        ],
        "related_topics": [
            "Data Science Bowl",
            "Nucleus Segmentation"
        ],
        "reference_count": "53",
        "citation_count": "428"
    },
    {
        "Id": "a195a2c234ef7a72cc448cb9b58404ff83077f65",
        "title": "Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy",
        "authors": [
            "Tim Scherr",
            "Katharina L{\\&quot;o}ffler",
            "Moritz B{\\&quot;o}hland",
            "Ralf Mikut"
        ],
        "date": "3 April 2020",
        "abstract": "This paper presents a method for the segmentation of touching cells in microscopy images using a novel representation of cell borders, inspired by distance maps, which is capable to utilize not only touching cells but also close cells in the training process. The accurate segmentation and tracking of cells in microscopy image sequences is an important task in biomedical research, e.g., for studying the development of tissues, organs or entire organisms. However, the segmentation of touching cells in images with a low signal-to-noise-ratio is still a challenging problem. In this paper, we present a method for the segmentation of touching cells in microscopy images. By using a novel representation of cell borders, inspired by distance maps, our method is capable to utilize not only touching cells but also close cells in the training process. Furthermore, this representation is notably robust to annotation errors and shows promising results for the segmentation of microscopy images containing in the training data underrepresented or not included cell types. For the prediction of the proposed neighbor distances, an adapted U-Net convolutional neural network (CNN) with two decoder paths is used. In addition, we adapt a graph-based cell tracking algorithm to evaluate our proposed method on the task of cell tracking. The adapted tracking algorithm includes a movement estimation in the cost function to re-link tracks with missing segmentation masks over a short sequence of frames. Our combined tracking by detection method has proven its potential in the IEEE ISBI 2020 Cell Tracking Challenge (http://celltrackingchallenge.net/) where we achieved as team KIT-Sch-GE multiple top three rankings including two top performances using a single segmentation model for the diverse data sets.",
        "references": [
            "084d3d1f2e8284f75aa6a5d4b550ba191ad12084",
            "f7eac8600e31bbd1371601a694b188858b657af0",
            "01dd8c73df2b37b2e02a9e25612ed86a06f25cc5",
            "3bffa23a16c273ac2228a13e65dade6766ce7777",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "2f62e40170265ffb1d0c79b6539d71457456880b",
            "f3bdd04e9b71c76ee4ae8f476740685e65ee0f0e",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "23e78e98e17d21d9b1c57cbe3e24ce47b1e4048c",
            "a1af04fa0a581a9f134a734363b1786ab2c355b7"
        ],
        "related_topics": [
            "Convolutional Neural Network",
            "Sequence",
            "Cell Segmentation",
            "Cost Functions"
        ],
        "reference_count": "42",
        "citation_count": "49"
    },
    {
        "Id": "bcf7a61d248a29fb647d232c57e69520a1f715c5",
        "title": "On Improving an Already Competitive Segmentation Algorithm for the Cell Tracking Challenge - Lessons Learned",
        "authors": [
            "Tim Scherr",
            "Katharina L{\\&quot;o}ffler",
            "Oliver Neumann",
            "Ralf Mikut"
        ],
        "date": "28 June 2021",
        "abstract": "The fine-tuned segmentation in combination with an improved tracking enabled to further improve the performance in the 6th edition of the Cell Tracking Challenge 2021 as team KIT-Sch-GE. The virtually error-free segmentation and tracking of densely packed cells and cell nuclei is still a challenging task. Especially in low-resolution and low signal-to-noise-ratio microscopy images erroneously merged and missing cells are common segmentation errors making the subsequent cell tracking even more difficult. In 2020, we successfully participated as team KIT-Sch-GE (1) in the 5th edition of the ISBI Cell Tracking Challenge. With our deep learning-based distance map regression segmentation and our graph-based cell tracking, we achieved multiple top 3 rankings on the diverse data sets. In this manuscript, we show how our approach can be further improved by using another optimizer and by fine-tuning training data augmentation parameters, learning rate schedules, and the training data representation. The fine-tuned segmentation in combination with an improved tracking enabled to further improve our performance in the 6th edition of the Cell Tracking Challenge 2021 as team KIT-Sch-GE (2).",
        "references": [
            "a195a2c234ef7a72cc448cb9b58404ff83077f65",
            "f4f9c33d94c7fbb4b43988d1e81490b7078f49c5",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "7b5be0cdec2a1b36cd8b61d161cff716b3594846",
            "82c5d56fb0f4aef16766bdf99c4ad0d81aa86ccf",
            "3bffa23a16c273ac2228a13e65dade6766ce7777",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1"
        ],
        "related_topics": [],
        "reference_count": "27",
        "citation_count": "6"
    },
    {
        "Id": "60ebb13073843322f2927edcf69ae214710d6448",
        "title": "Dual U-Net for the Segmentation of Overlapping Glioma Nuclei",
        "authors": [
            "Xieli Li",
            "Yuanyuan Wang",
            "Qi-sheng Tang",
            "Zhen Fan",
            "Jinhua Yu"
        ],
        "date": "2019",
        "abstract": "A U-Net-based multi-task learning network in which the boundary and region information is utilized to improve the segmentation accuracy of glioma nuclei, especially overlapping ones is proposed. The morphology and surroundings of cells have been routinely used by pathologists to diagnose the pathological subtypes of gliomas and to assess the malignancy of tumors. Thanks to the advent and development of digital pathology technology, it is possible to automatically analyze whole slides of tissue and focus on the nucleus in order to derive a quantitative assessment that can be used for grading, classification, and diagnosis. During the process of computer-assisted diagnosis, the accurate location and segmentation of nuclei from hematoxylin and eosin (H&E)-stained histopathological images is an important step. In this paper, we proposed a U-Net-based multi-task learning network in which the boundary and region information is utilized to improve the segmentation accuracy of glioma nuclei, especially overlapping ones. To refine the segmentation, a classification model is used to predict the boundary, a regression model is used to predict the distance map, and the final segmentation is obtained by using the fusion layers. The proposed approach was compared with other specially designed boundary-aware methods by using a pathological section dataset that consists of 320 glioma cases from the Huashan Hospital at Fudan University. Both the pixel-level and object-level evaluations showed that the structural modification is effective in segmentation with an F1-score of 0.82, a Hausdorff distance (HD) of 3.95, and an aggregated Jaccard index (AJI) of 0.66 (+0.46%, \u22123.75%, and +4.09% compared with the unimproved methods, respectively). In addition, comparative experiments on multi-organ nuclei segmentation (MoNuSeg) open dataset proved the advanced nature of the proposed method in the field of nuclei segmentation, especially separating touching objects. The proposed method obtains an AJI of 0.59 and an F1-score of 0.79.",
        "references": [
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "13de33ee941f1ebf3ed185c20fb4453a07302c30",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "2de5e5dd4460ba1ff00720e882c82a48f337ecc0",
            "ecd9ed863f2cff19cb0097d3ccbd07b584bfdc0e",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "930b28f5255a6633ec89bdafb9f72af8e9148fac",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "91b7b1ad437c888e63a3d175c35eb2d501bbd7d0"
        ],
        "related_topics": [
            "Dual U-Net",
            "Aggregated Jaccard Index",
            "Histopathological Images",
            "Multi-Task Learning Network",
            "Tumors",
            "Hausdorff Distance",
            "Pathologist",
            "Glioma",
            "Multi-Organ-Nuclei-Segmentation",
            "Eosin"
        ],
        "reference_count": "42",
        "citation_count": "35"
    },
    {
        "Id": "24b8a0b02bcb7934967757fc59d273a71ba67e30",
        "title": "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation",
        "authors": [
            "Jieneng Chen",
            "Yongyi Lu",
            "Qihang Yu",
            "Xiangde Luo",
            "Ehsan Adeli",
            "Yan Wang",
            "Le Lu",
            "Alan Loddon Yuille",
            "Yuyin Zhou"
        ],
        "date": "8 February 2021",
        "abstract": "It is argued that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",
        "references": [
            "a6876ea89e677a7cc42dd43f27165ff6fd414de5",
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "e3c836f4945396dac7f13f6f72af0bed0b299a0b",
            "f356edffeb43bc2e1050071e59cd718f5b0f60a5",
            "2a2bdf5cf0d73bc333423a8fd246593f4bf65322",
            "a86d7289c76d832e83c99539859b7b186e4ea6c8",
            "ae1c89817a3a239e5344293138bdd80293983460",
            "d29430adccb805ab57b349afa8553954347b3197",
            "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a"
        ],
        "related_topics": [
            "TransUNet",
            "Medical Image Segmentation",
            "Synapse Dataset",
            "Hybrid CNN-Transformer Architecture",
            "Multi-organ Segmentation",
            "Abdominal CT Scans",
            "Dice-Sorensen Coefficient",
            "Clinical CT Images",
            "U-shaped Architecture",
            "Medical Image Segmentation Tasks"
        ],
        "reference_count": "21",
        "citation_count": "1,704"
    },
    {
        "Id": "ae1c89817a3a239e5344293138bdd80293983460",
        "title": "Attention U-Net: Learning Where to Look for the Pancreas",
        "authors": [
            "Ozan Oktay",
            "Jo Schlemper",
            "Lo{\\&quot;i}c Le Folgoc",
            "M. J. Lee",
            "Mattias P. Heinrich",
            "Kazunari Misawa",
            "Kensaku Mori",
            "Steven G. McDonagh",
            "Nils Y. Hammerla",
            "Bernhard Kainz",
            "Ben Glocker",
            "Daniel Rueckert"
        ],
        "date": "11 April 2018",
        "abstract": "A novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes is proposed to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.",
        "references": [
            "aac368016f2540683ad2f611eb0cd889d350ff72",
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "3d44a1a97b6c768ae3080adf717326167457b0ad",
            "65147d0652741e886243549123dab142699e07eb",
            "c70218603f0af1be5d063056cbe629e042141a86",
            "1acdfdf6e24dc6c89f61aa7600648b38870bbc9b",
            "01556b9fbade335e0b58812fa75023a6dd409ce1",
            "b1135c3ba94839082c91c5b2600181d251b2634b",
            "569977bdb3f31d4b7c78ab3834fd34b370330e4e"
        ],
        "related_topics": [
            "Attention Gate",
            "Attention U-Net",
            "Pancreas",
            "Target Structures",
            "Dice Similarity Coefficients",
            "U-Net",
            "Gating Vector",
            "CT Pancreas Segmentation",
            "Segmentation Performance",
            "Pancreas Segmentation"
        ],
        "reference_count": "41",
        "citation_count": "3,199"
    },
    {
        "Id": "6c783eb64db6016fe80c0682709f4d24350120c3",
        "title": "A novel generic dictionary-based denoising method for improving noisy and densely packed nuclei segmentation in 3D time-lapse fluorescence microscopy images",
        "authors": [
            "Lamees Nasser",
            "Thomas Boudier"
        ],
        "date": "18 July 2018",
        "abstract": "A novel denoising algorithm is proposed that can both enhance very faint and noisy nuclei signal but simultaneously detect nuclei position accurately and is based on a limited number of parameters, with only one being critical, which is the approximate size of the objects of interest. Time-lapse fluorescence microscopy is an essential technique for quantifying various characteristics of cellular processes, i.e. cell survival, migration, and differentiation. To perform high-throughput quantification of cellular processes, nuclei segmentation and tracking should be performed in an automated manner. Nevertheless, nuclei segmentation and tracking are challenging tasks due to embedded noise, intensity inhomogeneity, shape variation as well as a weak boundary of nuclei. Although several nuclei segmentation approaches have been reported in the literature, dealing with embedded noise remains the most challenging part of any segmentation algorithm. We propose a novel denoising algorithm, based on sparse coding, that can both enhance very faint and noisy nuclei signal but simultaneously detect nuclei position accurately. Furthermore our method is based on a limited number of parameters, with only one being critical, which is the approximate size of the objects of interest. We also show that our denoising method coupled with classical segmentation method works properly in the context of the most challenging cases. To evaluate the performance of the proposed method, we tested our method on two datasets from the cell tracking challenge. Across all datasets, the proposed method achieved satisfactory results with 96:96% recall for the C. elegans dataset. Besides, in the Drosophila dataset, our method achieved very high recall (99:3%).",
        "references": [
            "852f4ebcbcf27f7b6835626d611097465cee5eed",
            "fa23031d601678b001aaa62e41f7fb34afe60f4a",
            "a2e06c347c192c94bcae153c36199d1272f7408f",
            "e8923e57b0466bbec0386fc4f533b10298e86f0d",
            "c9e59e4c26618e9b63d9caf13803ca5c7b20b296",
            "f3a3254d7e233ae36baef93ebe5008c21151f467",
            "2f74bd9dac23d050bfe9e3afe404b7c8dda0540b",
            "4546c36058306b587165ab63175d3639376db972",
            "34be0927155b5823e8e92bf2ec67a7d3796a090c",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a"
        ],
        "related_topics": [],
        "reference_count": "58",
        "citation_count": "10"
    },
    {
        "Id": "82367164f016d25d581527cb141f775e016b9c89",
        "title": "Accurate Nuclear Segmentation with Center Vector Encoding",
        "authors": [
            "Jiahui Li",
            "Zhiqiang Hu",
            "Shuang Yang"
        ],
        "date": "2 June 2019",
        "abstract": "A novel bottom-up method for nuclear segmentation is presented, and the concepts of Center Mask and Center Vector are introduced to better depict the relationship between pixels and nuclear instances. Nuclear segmentation is important and frequently demanded for pathology image analysis, yet is also challenging due to nuclear crowdedness and possible occlusion. In this paper, we present a novel bottom-up method for nuclear segmentation. The concepts of Center Mask and Center Vector are introduced to better depict the relationship between pixels and nuclear instances. The instance differentiation process are thus largely simplified and easier to understand. Experiments demonstrate the effectiveness of Center Vector Encoding, where our method outperforms state-of-the-arts by a clear margin.",
        "references": [
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "d1bef2553557375dfe85c6bbfcd6f7898f28615b",
            "cbf0a2dd3282b7bd5779b54f83c3e5396465a5ee",
            "1ec20454b36b1181062af2587831b49d85253245",
            "02ce1ff1290b351fc9b0a457c774bea26ae00198",
            "4b790b2baa71a24c09beabae98e5312782240a58",
            "f3f7fb350d776077e4b5b0d6c965adcced2c5065",
            "be9c59ec7ca1363828091cdd5e003e4e6e51b600"
        ],
        "related_topics": [
            "Nuclear Segmentation",
            "Nuclear Instances",
            "Pixel",
            "Center-Mask",
            "Bottom-up Method"
        ],
        "reference_count": "30",
        "citation_count": "14"
    },
    {
        "Id": "5773cf42590f11c51ca4b36c13d5c2cd2bcd9482",
        "title": "NucleiNet: A convolutional encoder-decoder network for bio-image denoising",
        "authors": [
            "Zichuan Liu",
            "Yifei Hu",
            "Hang Xu",
            "Lamees Nasser",
            "Philippe Coquet",
            "Thomas Boudier",
            "Hao Yu"
        ],
        "date": "11 July 2017",
        "abstract": "Using a convolutional encoder-decoder network, one can provide a scalable bio-image platform, called NucleiNet, to automatically segment, classify and track cell nuclei, which means that over 99% of nuclei can be successfully detected with no merging nuclei found. Generic and scalable data analysis procedures are highly demanded by the increasing number of multi-dimensional biomedical data. However, especially for time-lapse biological data, the high level of noise prevents for automated high-throughput analysis methods. The rapid developing of machine-learning methods and particularly deep-learning methods provide new tools and methodologies that can help in the denoising of such data. Using a convolutional encoder-decoder network, one can provide a scalable bio-image platform, called NucleiNet, to automatically segment, classify and track cell nuclei. The proposed method can achieve 0.99 F-score and 0.99 pixel-wise accuracy on C. elegans dataset, which means that over 99% of nuclei can be successfully detected with no merging nuclei found.",
        "references": [
            "0bde8d9367d1004c7396dd69cb27ed97dc2f8d77",
            "cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "e8923e57b0466bbec0386fc4f533b10298e86f0d",
            "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "26c708915f028d78734e44aa7416ee382b27ecea",
            "dbd5335a258ae6c4473c4dde643feda985e85111",
            "933392bae254c94d88b6c0a63b20d11199b3ccda",
            "a74a028f44e17a03becef819a48fb7a1bb6af323",
            "2e0292cd0dd06adc9649aace75210f4ae6a280cf",
            "9c3c5f169a40c3bb723562f57abf0c1f8eb5ece7"
        ],
        "related_topics": [
            "Pixel-wise Accuracy",
            "Denoising"
        ],
        "reference_count": "13",
        "citation_count": "7"
    },
    {
        "Id": "7d78b5bfb749eca31a37340e1d6e55360e877914",
        "title": "J Regularization Improves Imbalanced Multiclass Segmentation",
        "authors": [
            "Fidel A. Guerrero-Pe{\\~n}a",
            "Pedro D. Marrero-Fern{\\&#x27;a}ndez",
            "Paul T. Tarr",
            "Ing Ren Tsang",
            "Elliot M. Meyerowitz",
            "Alexandre Cunha"
        ],
        "date": "22 October 2019",
        "abstract": "This work proposes a new loss formulation to further advance the multiclass segmentation of cluttered cells under weakly supervised conditions and improves the separation of touching and immediate cells, obtaining sharp segmentation boundaries with high adequacy when adding a Youden's $J$ statistic regularization term to the cross entropy loss. We propose a new loss formulation to further advance the multiclass segmentation of cluttered cells under weakly supervised conditions. When adding a Youden's $J$ statistic regularization term to the cross entropy loss we improve the separation of touching and immediate cells, obtaining sharp segmentation boundaries with high adequacy. This regularization intrinsically supports class imbalance thus eliminating the necessity of explicitly using weights to balance training. Simulations demonstrate this capability and show how the regularization leads to correct results by helping advancing the optimization when cross entropy stagnates. We build upon our previous work on multiclass segmentation by adding yet another training class representing gaps between adjacent cells. This addition helps the classifier identify narrow gaps as background and no longer as touching regions. We present results of our methods for 2D and 3D images, from bright field images to confocal stacks containing different types of cells, and we show that they accurately segment individual cells after training with a limited number of images, some of which are poorly annotated.",
        "references": [
            "5896bd31724bd2f57630473e1e466ea08c3b0c34",
            "66c211884f5c8580200c3a99ece3da825c73772d",
            "a1e392a596bc2f34fe34dddf79ee11c20fe5d00e",
            "2d8edc4e38bf9907170238726ec902cb3739393b",
            "c7afd747b5c6b77dc22eaa87a8b22888243842b6",
            "6bf187cf239e66767688ed7dd88f6a408bf465f0",
            "e8d2045404f4a2775a88674c7b9645d7339d8a3b",
            "6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "cced09655a03c64a849c354bc534086f468264d5",
            "50004c086ffd6a201a4b782281aaa930fbfe6ecf"
        ],
        "related_topics": [
            "Cross Entropy Loss",
            "Annotated Images",
            "Imbalanced",
            "Classifier",
            "Optimization",
            "Class Imbalance"
        ],
        "reference_count": "26",
        "citation_count": "21"
    },
    {
        "Id": "33d94d93b59f16494b640bffa3c6198df46ecf95",
        "title": "An Integration Convolutional Neural Network for Nuclei Instance Segmentation",
        "authors": [
            "Aiping Qu",
            "Zhiming Cheng",
            "Xiaofeng He",
            "Yue Li"
        ],
        "date": "16 December 2020",
        "abstract": "The experiments demonstrate that the proposed nuclei instance segmentation approach outperforms prior state-of-the-art methods, and could be generalized across variety of nuclear type, magnification and imaging modality. Automated detection and segmentation of every nucleus in microscopy images is a fundamental task in biomedical research and clinical practice, including nuclear morphology analysis, cancer grading and prognosis predicting. However, this task is still challenging because of the nuclear atypical in shape, size and internal organisation. In this paper, we propose a nuclei instance segmentation method with the aim of jointing detection and segmentation simultaneously. The method builds on a two-stream Convolutional Neural Network(CNN) architecture that explicit utilize a single shot multi-box detector with Feature Pyramid Network in one stream for obtaining the nuclear location information and keep an U-net in the other stream for nuclei instance segmentation. Furthermore, the recurrent, residual and attention mechanisms are integrated for focusing on the useful information. The approach utilizes the strengths of the Recurrent Convolutional Neural Network (RCNN), the Attention Network and the Residual Network. The experiments demonstrate that the proposed nuclei instance segmentation approach outperforms prior state-of-the-art methods, and could be generalized across variety of nuclear type, magnification and imaging modality.",
        "references": [
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "7de166105087f15973ae78b554fb559e806dfa78",
            "54f218f3566942adec81265b21eb511ec8eecd63",
            "1b184ae321f37415b14db71ceb1dd585b9f39077",
            "27c761258329eddb90b64d52679ff190cb4527b5",
            "5986547c7380f5a8fb6028093f827b3662f838a2",
            "92cb73fa80d2c69eb2d46a00a2307d19d7d3b48b",
            "929c9a90e2ceac3f6684f2b29e6579c40b4ce3c1",
            "87c0dd990287d92796c7dc83edba6f52a2f52e21",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb"
        ],
        "related_topics": [
            "Nuclei Instance Segmentation",
            "Recurrent Convolutional Neural Networks",
            "U-Net",
            "Residual Networks",
            "Nuclear Type",
            "Cancer Grading",
            "Convolutional Neural Network",
            "Feature Pyramid Networks"
        ],
        "reference_count": "34",
        "citation_count": "One"
    },
    {
        "Id": "58843e76cd1d4a4fbab02ebfd02b71e1cd145ece",
        "title": "BAWGNet: Boundary aware wavelet guided network for the nuclei segmentation in histopathology images",
        "authors": [
            "Tamjid Imtiaz",
            "Shaikh Anowarul Fattah",
            "S. Y. Kung"
        ],
        "date": "1 August 2023",
        "abstract": "Semantic Scholar extracted view of \"BAWGNet: Boundary aware wavelet guided network for the nuclei segmentation in histopathology images\" by Tamjid Imtiaz et al.",
        "references": [
            "5938b820125a91e19dd05de6c897a835992f3799",
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "0721885821f099d1e3a6382835dc01769fdada12",
            "92713358f5748548b03c3130336b04b5f9028561",
            "ea3114b0ca20f392714eb62f04d281b869f624b1",
            "29a862a252a0a8bc8cea8fe5d4be4f7b19abc281",
            "75bdaacf355d0a6d52d9a4a8790e1fdc936eb867",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "d8c9934ecefd4e7b515bc9b6562278e8ec12a017"
        ],
        "related_topics": [],
        "reference_count": "40",
        "citation_count": "2"
    },
    {
        "Id": "a2aa5a08ad6d5e6432f0468e077e523430cf46a1",
        "title": "Meta multi-task nuclei segmentation with fewer training samples",
        "authors": [
            "Chu Han",
            "Huasheng Yao",
            "Bingchao Zhao",
            "Zhenhui Li",
            "Zhenwei Shi",
            "Lei Wu",
            "Xin Chen",
            "Jin Qu",
            "Ke Zhao",
            "Rushi Lan",
            "Changhong Liang",
            "Xipeng Pan",
            "Zaiyi Liu"
        ],
        "date": "1 May 2022",
        "abstract": "Semantic Scholar extracted view of \"Meta multi-task nuclei segmentation with fewer training samples\" by Chu Han et al.",
        "references": [
            "2e71e26bb816c6729e955e3c9e2b1c14906e73b4",
            "b893086a41be6fbe191cc04540a7c6c17bc80b71",
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "ba124bbde8d114a9103eb5751036e2e4710e2fff",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb",
            "a89cf9c6aa4cad62a78421916726b8b16c0cb9f2",
            "f77ae2f69b7432d9505d737c6209661f7be1eeb4",
            "a5782ff434f20c13e6ce689b698dfe3446db3a30",
            "99ebeb354c9538539e48c605446739007e9f4926"
        ],
        "related_topics": [
            "Model Agnostic Meta Learning",
            "Generalization",
            "State-of-the-art Models",
            "Fast Adaptation",
            "Pathologist",
            "Nuclei Segmentation",
            "Meta Multi-Task Learning",
            "Interaction Blocks",
            "Unseen Domains"
        ],
        "reference_count": "47",
        "citation_count": "15"
    },
    {
        "Id": "fef6868c31533988a7194982e36e2ae4a3d7e5c8",
        "title": "ConDANet: Contourlet Driven Attention Network for Automatic Nuclei Segmentation in Histopathology Images",
        "authors": [
            "Tamjid Imtiaz",
            "Shaikh Anowarul Fattah",
            "Mohammad Saquib"
        ],
        "date": "2023",
        "abstract": "A contourlet driven attention network, namely ConDANet, is developed which incorporates a novel attention mechanism along with a content-preserving sampling strategy which preserves the textural content of the nucleus in histopathology images and prevents the loss of information in the subsequent sampling operation of the encoder-decoder part of the network. Cell nuclei segmentation, the task of identifying the precise boundary of the nucleus in each cell in a histopathology image, is a rudimentary task prior to single-cell analysis. While addressing this task, two of the major challenges are the precise delineation of the small-shaped nucleus structure and the characterization of the edge region of the nucleus for both the regular and deformed nucleus. To overcome these challenges, a contourlet driven attention network, namely ConDANet, is developed which incorporates a novel attention mechanism along with a content-preserving sampling strategy. Instead of using conventional spatial or channel-wise attention, a controlling signal driven attention mechanism is proposed, which is capable of extracting fine edge details of the nuclei regions by focusing on the relevant edge or boundary information of the nuclei structures. The contourlet transform based controlling signal generation scheme is proposed, which not only exploits the advantage of the multi-scale time frequency localization properties of wavelets but also provides a high degree of directionality. Additionally, the wavelet pooling strategy is incorporated to the network which preserves the textural content of the nucleus in histopathology images and prevents the loss of information in the subsequent sampling operation of the encoder-decoder part of the network. Finally, the proposed method is employed for analyzing three publicly available histopathology datasets to manifest its effectiveness in segmenting nuclei from cellular images extracted from a wide variety of organs and patients. Dice scores obtained in these three datasets are 88.9%, 81.71%, and 75.12% which are found to be superior compared to some state-of-the-art methods.",
        "references": [
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "9a5356d74076b7e1b8a7bcb0e459fc923abafb27",
            "0ede258f3007863ade1bb2c7316b4b57925485ac",
            "63a373063d51489b31e07ee639ab74b6cf586240",
            "5e1335f1cf45fb4c29a0697f65c1e5c8e26d7dd4",
            "5d1d45c89ec13f3b0a83078759bb1dfbac695102",
            "75bdaacf355d0a6d52d9a4a8790e1fdc936eb867",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "f3ec8c29a34d0cc1bb7c7892ba4cae86347914e8",
            "b79ca6fd3df135a9bcf778844be625b764fbcfb3"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "31"
    },
    {
        "Id": "c96757792971d93b7a5a3cff3bcc9fbe6e7fa97b",
        "title": "DenseRes-Unet: Segmentation of overlapped/clustered nuclei from multi organ histopathology images",
        "authors": [
            "Iqra Kiran",
            "Basit Raza",
            "Areesha Ijaz",
            "Muazzam Ali Khan Khattak"
        ],
        "date": "1 January 2022",
        "abstract": "Semantic Scholar extracted view of \"DenseRes-Unet: Segmentation of overlapped/clustered nuclei from multi organ histopathology images\" by Iqra Kiran et al.",
        "references": [
            "92713358f5748548b03c3130336b04b5f9028561",
            "60ebb13073843322f2927edcf69ae214710d6448",
            "4a91c15880a788711c0a7e00ba3968580e3052a5",
            "07341eaa7ea8d8261a15c946127ce5eebb0d463b",
            "9a5356d74076b7e1b8a7bcb0e459fc923abafb27",
            "d6785c954cc5562838a57e185e99d0496b5fd5a2",
            "505e72b8d60e987e89b93fdd98859b857ca94207",
            "5938b820125a91e19dd05de6c897a835992f3799",
            "9987d9b7fc3a5b3c9d35f47be5cc5d9eede9bf4b",
            "30d4a6588647db444228bd799381dcd7cdd90eb3"
        ],
        "related_topics": [
            "Aggregated Jaccard Index",
            "Multi-Organ Nucleus Segmentation",
            "Center Points",
            "Nuclei Segmentation",
            "Pixel",
            "Contour Problem",
            "Histopathology Images",
            "Fully Convolutional Networks",
            "Residual Connections",
            "Skip Connections"
        ],
        "reference_count": "52",
        "citation_count": "36"
    },
    {
        "Id": "aabd9c8f5ef73211d079be8627ee9532922267c4",
        "title": "Nuclei and glands instance segmentation in histology images: a narrative review",
        "authors": [
            "Esha Sadia Nasir",
            "Arshi Parvaiz",
            "Muhammad Moazam Fraz"
        ],
        "date": "26 August 2022",
        "abstract": "In this survey, 126 papers illustrating the AI-based methods for nuclei and glands instance segmentation published in the last five years are deeply analyzed, and the limitations of current approaches and the open challenges are discussed. Examination of tissue biopsy and quantification of the various characteristics of cellular processes are clinical benchmarks in cancer diagnosis. Nuclei and glands instance segmentation greatly assists the high-throughput quantification of cellular process and accurate appraisal of tissue biopsy. It subsequently makes a significant improvement to the computational pathology process for cancer diagnosis, treatment planning, and survival analysis. Recent advancements in the field of computer vision have automated the manual, laborious, and time-consuming histopathological analysis process. Automated image analysis of histopathological images for cells and tissues to trace the entirety of the ultrastructures, has been an active area of research in medical informatics for decades. The developments in computers, microscopy hardware, and the availability of large-scale public datasets have further fastened the development in this field. And the realization that scientific and diagnostic pathology calls for fresh ways to undertake, automated image analysis of histopathological images has captivated contemporary attention. In this survey, 126 papers illustrating the AI-based methods for nuclei and glands instance segmentation published in the last five years (2017\u20132022) are deeply analyzed, and the limitations of current approaches and the open challenges are discussed. Moreover, the potential future research direction is presented, and the contribution of state-of-the-art methods is summarized. Further, a generalized summary of publicly available datasets and detailed insights on the grand challenges illustrating the top-performing methods specific to each challenge is also provided. Besides, we intended to give the reader the current state of existing research and pointers to the future directions in developing methods that can be used in clinical practice enabling improved diagnosis, grading, prognosis, and treatment planning of cancer. To the best of our knowledge, no previous work has reviewed the instance segmentation in histology images focusing on nuclei and glands instance segmentation.",
        "references": [
            "e0b984f5fce8535ba1306cd4d735aa0ed9ef7e62",
            "c9a372fc3b30bb2da051e941cab44c3e5ba31065",
            "348eea0020bdf24329331d9df21da7033eb59bf3",
            "78db21fabea962592ac0aef54624251550929109",
            "ac158a0fa7ae122cc5e3785382aa364762c26839",
            "d23d38e8d400ed14bd6fbfe8867f7b06b8b9fbd6",
            "8ecc6ecd5b77665b261cce1a96d431d9f43bdaea",
            "60ebb13073843322f2927edcf69ae214710d6448",
            "7058f9b5e86fe98d5014ff74400cb4e7cc1e5af8",
            "737142a5c402034df9825904f7f5d5379bd1384a"
        ],
        "related_topics": [
            "Instance Segmentation",
            "Histology Images",
            "Glands",
            "Treatment Planning",
            "Computational Pathology",
            "Survival Analysis"
        ],
        "reference_count": "168",
        "citation_count": "2"
    },
    {
        "Id": "ab3ad2d3985eb36b793a79a4a129e7ee8be86e20",
        "title": "SMILE: Cost-sensitive multi-task learning for nuclear segmentation and classification with imbalanced annotations",
        "authors": [
            "Xipeng Pan",
            "Jijun Cheng",
            "Feihu Hou",
            "Rushi Lan",
            "Cheng Lu",
            "Lingqiao Li",
            "Zhengyun Feng",
            "Huadeng Wang",
            "Changhong Liang",
            "Zhenbing Liu",
            "Xin Chen",
            "Chu Han",
            "Zaiyi Liu"
        ],
        "date": "12 June 2023",
        "abstract": "Semantic Scholar extracted view of \"SMILE: Cost-sensitive multi-task learning for nuclear segmentation and classification with imbalanced annotations\" by Xipeng Pan et al.",
        "references": [
            "a2aa5a08ad6d5e6432f0468e077e523430cf46a1",
            "a4387fb99253e13ef998446e33b0737f0903b269",
            "87c0dd990287d92796c7dc83edba6f52a2f52e21",
            "30d4a6588647db444228bd799381dcd7cdd90eb3",
            "3e358c3033908a9506e7f1e3cf29283e359f43d6",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb",
            "8a370a1bce09d8c9d6376b064a4090f0085b8c4c",
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "34c062e2b8a3f6421b9f4ff22f115a36d4aba823",
            "5986547c7380f5a8fb6028093f827b3662f838a2"
        ],
        "related_topics": [
            "Classification",
            "Nuclear Segmentation",
            "Minority Classes",
            "Spatial Multiplexing Of Local Elements",
            "Nuclei Segmentation",
            "CoNSeP",
            "WSIs",
            "Cost-sensitive",
            "Whole Slide Images",
            "Feature Interactions"
        ],
        "reference_count": "48",
        "citation_count": "9"
    },
    {
        "Id": "05a2d3b1cf591873002274625f5f2f7eb3a72b4d",
        "title": "A deep learning approach for nucleus segmentation and tumor classification from lung histopathological images",
        "authors": [
            "S. M. Jaisakthi",
            "Karthik Desingu",
            "Palaniappan Mirunalini",
            "S. Pavya",
            "N. Priyadharshini"
        ],
        "date": "7 May 2023",
        "abstract": "It is posits that segmenting and retaining the nuclear regions in the input image to the tumor type classifier suppresses the importance of less relevant portions of the image during model training, pronounces nuclear region boundaries to highlight shape features, and reduces the overall computation cost of training. Lung cancer is the leading cause of death worldwide. Early diagnosis is crucial to improve patients\u2019 chance of survival. Automated detection and analysis of cancer types can significantly improve the diagnosis process. It can aid treatment through follow-up analyses. This paper proposes a deep learning based pipeline for multi-class classification of lung tumor type (Benign (B), ADenoCarcinoma (ADC) and Squamous-Cell Carcinoma (SCC)) from histopathological images. A baseline classification method, the $$P_{dir}$$ P dir pipeline, is proposed where Whole Slide Histopathological Image (WSHI) patches are classified using the proposed Deep Convolutional Neural Network (DCNN) classifier. Since each cancer type is characterized by the difference in the structure of the nuceli, this research work proposes to improve the performance of classification by segmenting the nuclei. The $$P_{seg}$$ P seg pipeline is proposed to extract the nuclear regions from the WSHI patches using an Xception-style UNet based neural network, and this segmented region is then categorised into tumor types using the same downstream DCNN architecture. The classification system showed an accuracy of 95.40% and 99.60% using the $$P_{dir}$$ P dir and $$P_{seg}$$ P seg pipelines, respectively. The classification performed through $$P_{seg}$$ P seg pipeline exhibits significant improvement compared to the $$P_{dir}$$ P dir pipeline, supporting our hypothesis that nucleus segmentation improves classification performance. This paper posits that segmenting and retaining the nuclear regions in the input image to the tumor type classifier suppresses the importance of less relevant portions of the image during model training, pronounces nuclear region boundaries to highlight shape features, and reduces the overall computation cost of training. Ultimately, it induces a positive impact on classification performance. The enhanced performance obtained using the proposed $$P_{seg}$$ P seg pipeline supports our hypothesis.",
        "references": [
            "b54530acc5c3d979222cda72ace2506fbc81cfe1",
            "7256088eece603df2e5675025e8bed90c0f21171",
            "ba307a871bc5120c4f40989aebddd4d33d8ea3f2",
            "480183bb8eaf87bfb1b42eff56d4769742d5b316",
            "16658d471952e7a10608313b663965907f6a07f2",
            "7ce1e90790d4e720a125903d9bf0a090b9da8db1",
            "769149c0dc0ed308eca8bc916f4326b2e2f57a1f",
            "77e07e0576d27cb60e0b0eafbf4a2e3997eab250",
            "49ba2b5c3a9914780998666ed5523bbba94ebc10",
            "02d977705be208739398108bba1c87f37c13d7b0"
        ],
        "related_topics": [
            "Deep Learning",
            "Histopathological Images",
            "Nucleus Segmentation",
            "Multi-class Classification",
            "Squamous Cell Carcinoma",
            "Neural Network",
            "Classification"
        ],
        "reference_count": "0",
        "citation_count": "75"
    },
    {
        "Id": "b008724915a987b5eb9b31dfcb41ee4907f4b1cf",
        "title": "Synthetic-to-real: instance segmentation of clinical cluster cells with unlabeled synthetic training",
        "authors": [
            "Meng Zhao",
            "Siyu Wang",
            "Fan Shi",
            "Chen Jia",
            "Xuguo Sun",
            "Shengyong Chen"
        ],
        "date": "24 June 2022",
        "abstract": "A contour constraint instance segmentation framework (CC framework) for cluster cells based on a cluster cell combination enhancement module to alleviate over- and under-segmentation among individual cell-instance boundaries is proposed. Abstract Motivation The presence of tumor cell clusters in pleural effusion may be a signal of cancer metastasis. The instance segmentation of single cell from cell clusters plays a pivotal role in cluster cell analysis. However, current cell segmentation methods perform poorly for cluster cells due to the overlapping/touching characters of clusters, multiple instance properties of cells, and the poor generalization ability of the models. Results In this article, we propose a contour constraint instance segmentation framework (CC framework) for cluster cells based on a cluster cell combination enhancement module. The framework can accurately locate each instance from cluster cells and realize high-precision contour segmentation under a few samples. Specifically, we propose the contour attention constraint module to alleviate over- and under-segmentation among individual cell-instance boundaries. In addition, to evaluate the framework, we construct a pleural effusion cluster cell dataset including 197 high-quality samples. The quantitative results show that the numeric result of APmask is > 90%, a more than 10% increase compared with state-of-the-art semantic segmentation algorithms. From the qualitative results, we can observe that our method rarely has segmentation errors.",
        "references": [
            "bbffe75a1b5c5bc02f3a13bbc4f6700b17686102",
            "1b184ae321f37415b14db71ceb1dd585b9f39077",
            "4cb5f6dfa8144d5235df4e530743043e1637e49b",
            "6581196d938739cae9b54f7da85274a149ddbbbf",
            "547d47be440ea686cdbe118a32c1491653ddb4e1",
            "5fd490e5ceed129a83d16dbda29ab61fe4aa1acb",
            "ab0e039f03059e5d143b90b83324712827d16fad",
            "c09c5aaadf3e43f8b684058652ba9c404fd13cb3",
            "7cb249ab16be6a6f58eef20f7549bf48254ccd62",
            "5986547c7380f5a8fb6028093f827b3662f838a2"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "e186a36749bcf6d10d30b99482af206c8848d666",
        "title": "A Novel Heteromorphous Convolutional Neural Network for Automated Assessment of Tumors in Colon and Lung Histopathology Images",
        "authors": [
            "Saeed Iqbal",
            "Adnan Qureshi",
            "Musaed A. Alhussein",
            "Khursheed Aurangzeb",
            "Seifedine Kadry"
        ],
        "date": "1 August 2023",
        "abstract": "A new method called ColonNet, a heteromorphous convolutional neural network (CNN) with a feature grafting methodology categorically configured for analyzing mitotic nuclei in colon and lung histopathology images, highlighting its potential as a valuable tool to support pathologists in diagnostic activities. The automated assessment of tumors in medical image analysis encounters challenges due to the resemblance of colon and lung tumors to non-mitotic nuclei and their heteromorphic characteristics. An accurate assessment of tumor nuclei presence is crucial for determining tumor aggressiveness and grading. This paper proposes a new method called ColonNet, a heteromorphous convolutional neural network (CNN) with a feature grafting methodology categorically configured for analyzing mitotic nuclei in colon and lung histopathology images. The ColonNet model consists of two stages: first, identifying potential mitotic patches within the histopathological imaging areas, and second, categorizing these patches into squamous cell carcinomas, adenocarcinomas (lung), benign (lung), benign (colon), and adenocarcinomas (colon) based on the model\u2019s guidelines. We develop and employ our deep CNNs, each capturing distinct structural, textural, and morphological properties of tumor nuclei, to construct the heteromorphous deep CNN. The execution of the proposed ColonNet model is analyzed by its comparison with state-of-the-art CNNs. The results demonstrate that our model surpasses others on the test set, achieving an impressive F1 score of 0.96, sensitivity and specificity of 0.95, and an area under the accuracy curve of 0.95. These outcomes underscore our hybrid model\u2019s superior performance, excellent generalization, and accuracy, highlighting its potential as a valuable tool to support pathologists in diagnostic activities.",
        "references": [
            "f3ba4efd54601fee417ac9805097fb1c671ccc9d",
            "16658d471952e7a10608313b663965907f6a07f2",
            "2741098be8505e37b4280e97cae0d177e8b2ede7",
            "d97e70d8fa6cbe4fcac9096491ce6e6a5e974e88",
            "eb23900d3dcaa3968968e882ea10bd2922fbc9f1",
            "6184e4069b8dc3d70af4b93aaa1868a2edf2ac61",
            "fa9804e72294daa7b76e52c7e7c50b86fb2e1207",
            "016dc879654cb4faa6430f9740fb9480ef8d702a",
            "59cfbc953c6e33616e5cb7e51c5385c62a181b92",
            "5bfa3d3a6dfae0d13818a4b778ed8405c6bba791"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "44"
    },
    {
        "Id": "41747cbdbed84762dfbfc305254c97021279dc6e",
        "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "6 November 2019",
        "abstract": "A powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images by trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2910bec6d4de87e22be5119cef3c488d2ae50e2a",
            "68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [
            "Uninformed Students",
            "Anomaly-free Images",
            "Per-region-overlap",
            "MVTec Anomaly Detection Dataset",
            "Unsupervised Anomaly Segmentation",
            "Anomaly-free Training Data",
            "Anomalous Regions",
            "MVTec AD",
            "Spatial Anomaly Map",
            "OCGAN"
        ],
        "reference_count": "37",
        "citation_count": "376"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "3b3aefbbdb64e5812f133f220b3f129a36a30065",
        "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
        "authors": [
            "Hanqiu Deng",
            "Xingyu Li"
        ],
        "date": "26 January 2022",
        "abstract": "A novel T-S model consisting of a teacher encoder and a student decoder is proposed and a simple yet effective \u201creverse distillation\u201d paradigm is introduced, demonstrating the proposed approach's effectiveness and generalizability. Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective \u201creverse distillation\u201d paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model's one-class embedding as input and targets to restore the teacher's multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but aban-dons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach's effectiveness and generalizability.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "599fd051c9438011ec5b581983c89e8922b4a5e6",
            "30895c61bb836f2cae7ef5ba6516886f746a7153",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [
            "Reverse Distillation",
            "Student Decoder",
            "One-Class Embedding",
            "CutPaste",
            "Multiresolution Knowledge Distillation",
            "Large Defects",
            "Anomaly-free Samples",
            "Pseudo Anomalies",
            "WideResnet-50",
            "Localize Anomaly"
        ],
        "reference_count": "52",
        "citation_count": "161"
    },
    {
        "Id": "5fdd9cfb9b22b9e575d36e98e5d6f52a83729e42",
        "title": "Reconstructed Student-Teacher and Discriminative Networks for Anomaly Detection",
        "authors": [
            "Shinji Yamada",
            "Satoshi Kamiya",
            "Kazuhiro Hotta"
        ],
        "date": "14 October 2022",
        "abstract": "A powerful anomaly detection method is proposed based on student-teacher feature pyramid matching (STPM), which consists of a student and teacher network and a discriminative network, which displayed high accuracy on the MVTec anomaly detection dataset. Anomaly detection is an important problem in computer vision; however, the scarcity of anomalous samples makes this task difficult. Thus, recent anomaly detection methods have used only \u201cnormal images\u201d with no abnormal areas for training. In this work, a powerful anomaly detection method is proposed based on student-teacher feature pyramid matching (STPM), which consists of a student and teacher network. Generative models are another approach to anomaly detection. They reconstruct normal images from an input and compute the difference between the predicted normal and the input. Unfortunately, STPM does not have the ability to generate normal images. To improve the accuracy of STPM, this work uses a student network, as in generative models, to reconstruct normal features. This improves the accuracy; however, the anomaly maps for normal images are not clean because STPM does not use anomaly images for training, which decreases the accuracy of the image-level anomaly detection. To further improve accuracy, a discriminative network trained with pseudo-anomalies from anomaly maps is used in our method, which consists of two pairs of student-teacher networks and a discriminative network. The method displayed high accuracy on the MVTec anomaly detection dataset.",
        "references": [
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "95a26eafabf06b1fc5dec6c460a927cf5964e97e",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "5f61089d3d548a515f01b473f0119137d1f340d4",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "9277dc70c74bcadf80dab11c28ead83fd085deec"
        ],
        "related_topics": [
            "Anomaly Detection",
            "Normal Images",
            "STPM",
            "Anomaly Map",
            "Generative Models",
            "Student Network",
            "MVTec Anomaly Detection Dataset",
            "Pseudo Anomalies",
            "Image-level Anomaly Detection",
            "Teacher Network"
        ],
        "reference_count": "35",
        "citation_count": "16"
    },
    {
        "Id": "02805f18989b7e77f30ee13defd6fecfcd0f499f",
        "title": "Student-Teacher Feature Pyramid Matching for Anomaly Detection",
        "authors": [
            "Guodong Wang",
            "Shumin Han",
            "Errui Ding",
            "Di Huang"
        ],
        "date": "7 March 2021",
        "abstract": "This paper proposes a simple yet powerful approach to anomaly detection, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Anomaly detection is a challenging task and usually formulated as an one-class learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efficiency. Given a strong model pre-trained on image classification as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature matching enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on the MVTec anomaly detection dataset, superior to the state of the art ones.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "2528a82dd2266600d4ee2b54165556a984de94d4",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "Student-Teacher Feature Pyramid Matching",
            "Anomaly Localization",
            "Per-region-overlap",
            "PaDiM",
            "MVTec AD",
            "Feature Pyramids",
            "Student Network",
            "Anomaly Detection",
            "Scoring Function",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "47",
        "citation_count": "83"
    },
    {
        "Id": "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
        "title": "MVTec AD \u2014 A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "1 June 2019",
        "abstract": "This work introduces the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories, and conducts a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolved neural networks. The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the \ufb01eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the \ufb01rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.",
        "references": [
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "732c21998e251d64cd58b6a86886ee5907efeaa5",
            "9c24454b071bc8e96ea46c5064a7bddf07cca464",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "5d90f06bb70a0a3dced62413346235c02b1aa086"
        ],
        "related_topics": [
            "MVTec AD",
            "MVTec Anomaly Detection",
            "UnSupervised Anomaly Detection",
            "Texture Categories",
            "Metal Nut",
            "Anomalous Images",
            "MVTec AD Dataset",
            "Anomalous Regions",
            "Spatial Anomaly Map",
            "Anomaly-free Images"
        ],
        "reference_count": "29",
        "citation_count": "748"
    },
    {
        "Id": "95a26eafabf06b1fc5dec6c460a927cf5964e97e",
        "title": "DR\u00c6M \u2013 A discriminatively trained reconstruction embedding for surface anomaly detection",
        "authors": [
            "Vitjan Zavrtanik",
            "Matej Kristan",
            "Danijel Sko{\\vc}aj"
        ],
        "date": "17 August 2021",
        "abstract": "The proposed DR\u00c6M method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. Visual surface anomaly detection aims to detect local image regions that significantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DR\u00c6M). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DR\u00c6M outperforms the current state-of-the-art unsupervised methods by a large margin and even de-livers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy. Code at github.com/VitjanZ/DRAEM.",
        "references": [
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "f93bdba4177051d3cb285e65dc911dc77d332d11",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "17d3f90cb63fbac50a5e49b8a46e633ec1f526fd",
            "2e8d62277e40d465343e8dfb32ecc246f320540e",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "41747cbdbed84762dfbfc305254c97021279dc6e"
        ],
        "related_topics": [
            "DRAEM",
            "Surface Anomaly Detection",
            "Anomaly Localization",
            "Anomaly-free Images",
            "Synthetic Anomalies",
            "Discriminative Sub-network",
            "Anomaly Score Map",
            "Simulated Anomalies",
            "Anomalous Images",
            "Image-level AUROC"
        ],
        "reference_count": "34",
        "citation_count": "226"
    },
    {
        "Id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
        "title": "Deep Residual Learning for Image Recognition",
        "authors": [
            "Kaiming He",
            "X. Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "date": "10 December 2015",
        "abstract": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "references": [
            "eb42cf88027de515750f230b23b1a057dc782108",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "fb91db6aa4f710814f8aec28a7f3ecbc4e5ad4fd",
            "ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649",
            "d6f2f611da110b5b5061731be3fc4c7f45d8ee23",
            "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "b8ef1230a5cc9ea7cd8358f1ae7d1af97813ba14",
            "8ad35df17ae4064dd174690efb04d347428f1117"
        ],
        "related_topics": [
            "Deep Residual Learning",
            "Residual Functions",
            "ImageNet Localization",
            "COCO Segmentation",
            "Stacked Layers",
            "Residual Learning",
            "VGG Nets",
            "ImageNet",
            "Layers",
            "Image Recognition"
        ],
        "reference_count": "54",
        "citation_count": "152,373"
    },
    {
        "Id": "9277dc70c74bcadf80dab11c28ead83fd085deec",
        "title": "Sub-Image Anomaly Detection with Deep Pyramid Correspondences",
        "authors": [
            "Niv Cohen",
            "Yedid Hoshen"
        ],
        "date": "5 May 2020",
        "abstract": "This work presents a novel anomaly segmentation approach based on alignment between an anomalous image and a constant number of the similar normal images, which is shown to achieve state-of-the-art performance on unsupervised anomaly detection and localization while requiring virtually no training time. Nearest neighbor (kNN) methods utilizing deep pre-trained features exhibit very strong anomaly detection performance when applied to entire images. A limitation of kNN methods is the lack of segmentation map describing where the anomaly lies inside the image. In this work we present a novel anomaly segmentation approach based on alignment between an anomalous image and a constant number of the similar normal images. Our method, Semantic Pyramid Anomaly Detection (SPADE) uses correspondences based on a multi-resolution feature pyramid. SPADE is shown to achieve state-of-the-art performance on unsupervised anomaly detection and localization while requiring virtually no training time.",
        "references": [
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "f008d9b244fcb393ceb57b42ea165e58a31286bd",
            "1f528877c4d8d5df3b3abbfa64379677d451956b",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "e021d59638966a6fbb36854cc2cf1045de7a62d2",
            "fef6f1e04fa64f2f26ac9f01cd143dd19e549790",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "04513c7c0b3a63fde81a996dae064a28d453c17a",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900"
        ],
        "related_topics": [
            "Anomalous Images",
            "Per-region-overlap",
            "Normal Images",
            "Spatially-adaptive Denormalization",
            "Anomaly Segmentation",
            "Visual Anomaly Detection",
            "MVTec Dataset",
            "Pixel-level Anomaly Detection",
            "ShanghaiTech Campus",
            "Image-level Anomaly Detection"
        ],
        "reference_count": "41",
        "citation_count": "264"
    },
    {
        "Id": "3b3aefbbdb64e5812f133f220b3f129a36a30065",
        "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
        "authors": [
            "Hanqiu Deng",
            "Xingyu Li"
        ],
        "date": "26 January 2022",
        "abstract": "A novel T-S model consisting of a teacher encoder and a student decoder is proposed and a simple yet effective \u201creverse distillation\u201d paradigm is introduced, demonstrating the proposed approach's effectiveness and generalizability. Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective \u201creverse distillation\u201d paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model's one-class embedding as input and targets to restore the teacher's multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but aban-dons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach's effectiveness and generalizability.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "599fd051c9438011ec5b581983c89e8922b4a5e6",
            "30895c61bb836f2cae7ef5ba6516886f746a7153",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [
            "Reverse Distillation",
            "Student Decoder",
            "One-Class Embedding",
            "CutPaste",
            "Multiresolution Knowledge Distillation",
            "Large Defects",
            "Anomaly-free Samples",
            "Pseudo Anomalies",
            "WideResnet-50",
            "Localize Anomaly"
        ],
        "reference_count": "52",
        "citation_count": "161"
    },
    {
        "Id": "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
        "title": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection",
        "authors": [
            "Guodong Wang",
            "Shumin Han",
            "Errui Ding",
            "Di Huang"
        ],
        "date": "2021",
        "abstract": "This paper proposes a simple yet powerful approach to anomaly detection, implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Anomaly detection is a challenging task and usually for-mulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Given a strong model pre-trained on image classi\ufb01cation as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, signi\ufb01cantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256 \u00d7 256), at least dozens of times faster than the latest counterparts.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "388645c44061f6e88fff0ecdad2f622936207d67",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2528a82dd2266600d4ee2b54165556a984de94d4"
        ],
        "related_topics": [
            "Student-Teacher Feature Pyramid Matching",
            "Pixel-level Anomaly Detection",
            "Anomaly-free Images",
            "Image-level Anomaly Detection",
            "MVTec AD Dataset",
            "Per-region-overlap",
            "Student Network",
            "Feature Pyramids",
            "Unsupervised Learning",
            "Scoring Function"
        ],
        "reference_count": "45",
        "citation_count": "63"
    },
    {
        "Id": "41747cbdbed84762dfbfc305254c97021279dc6e",
        "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "6 November 2019",
        "abstract": "A powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images by trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2910bec6d4de87e22be5119cef3c488d2ae50e2a",
            "68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [
            "Uninformed Students",
            "Anomaly-free Images",
            "Per-region-overlap",
            "MVTec Anomaly Detection Dataset",
            "Unsupervised Anomaly Segmentation",
            "Anomaly-free Training Data",
            "Anomalous Regions",
            "MVTec AD",
            "Spatial Anomaly Map",
            "OCGAN"
        ],
        "reference_count": "37",
        "citation_count": "376"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
        "title": "Memorizing Normality to Detect Anomaly: Memory-Augmented Deep Autoencoder for Unsupervised Anomaly Detection",
        "authors": [
            "Dong Gong",
            "Lingqiao Liu",
            "Vuong Le",
            "Budhaditya Saha",
            "Moussa Reda Mansour",
            "Svetha Venkatesh",
            "Anton van den Hengel"
        ],
        "date": "4 April 2019",
        "abstract": "The proposed memory-augmented autoencoder called MemAE is free of assumptions on the data type and thus general to be applied to different tasks and proves the excellent generalization and high effectiveness of the proposed MemAE. Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder \"generalizes\" so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE firstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be fixed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.",
        "references": [
            "2b75ba7f75170b73d913c515cc0deefef6c88f5f",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "fef6f1e04fa64f2f26ac9f01cd143dd19e549790",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "99dff291f260b3cc3ff190106b0c2e3e685223a4",
            "afac99b10a5c7e531f73e4a4866d6ee3c9e86cd4",
            "bbd0e204f48a45735e1065c8b90b298077b73192"
        ],
        "related_topics": [
            "MemAE",
            "Memory-augmented Autoencoder",
            "Prototypical Normal Patterns",
            "Normal Data",
            "Memory Module",
            "Temporally-coherent Sparse Coding",
            "UnSupervised Anomaly Detection",
            "UCSD Ped2",
            "Video Anomaly Detection",
            "Prototypical Elements"
        ],
        "reference_count": "54",
        "citation_count": "854"
    },
    {
        "Id": "4dd78b8d466b4cfe55a1bbdc694291197ce62541",
        "title": "DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation",
        "authors": [
            "Jie Yang",
            "Yong Shi",
            "Zhiquan Qi"
        ],
        "date": "26 November 2020",
        "abstract": "Semantic Scholar extracted view of \"DFR: Deep Feature Reconstruction for Unsupervised Anomaly Segmentation\" by Jie Yang et al.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "8bedee52bf14e326d7fe2b1f1c1d0cb256aa53f6",
            "75a838cbc1541858b9c484001cade327640dc280",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "1bbf746cca4bcafd274d197ac9fae82b245bf97b",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "d9d7ab13ce305ccee309c989a2341d72b1252070",
            "fe09f7a379944444201552e952b910188c0aeaca",
            "9c24454b071bc8e96ea46c5064a7bddf07cca464",
            "8da55e685a7bef9c897788ab519a8710c695c419"
        ],
        "related_topics": [
            "Deep Feature Reconstruction",
            "Anomalous Regions",
            "Unsupervised Anomaly Segmentation",
            "Benchmark Dataset",
            "Digital Forensic Readiness",
            "Anomaly Detection"
        ],
        "reference_count": "42",
        "citation_count": "60"
    },
    {
        "Id": "1c0165247ce1d56a9de7be50ca6c4a49f0db4a82",
        "title": "Self-Supervised Masking for Unsupervised Anomaly Detection and Localization",
        "authors": [
            "Chaoqin Huang",
            "Qinwei Xu",
            "Yanfeng Wang",
            "Yu Wang",
            "Ya Zhang"
        ],
        "date": "13 May 2022",
        "abstract": "A self-supervised learning approach through random masking and then restoring, named SSM for unsupervised anomaly detection and localization, that enhances the training of the inpainting network but also leads to great improvement in the efficiency of mask prediction at inference. Recently, anomaly detection and localization in multimedia data have received significant attention among the machine learning community. In real-world applications such as medical diagnosis and industrial defect detection, anomalies only present in a fraction of the images. To extend the reconstruction-based anomaly detection architecture to the localized anomalies, we propose a self-supervised learning approach through random masking and then restoring, named Self-Supervised Masking (SSM) for unsupervised anomaly detection and localization. SSM not only enhances the training of the inpainting network but also leads to great improvement in the efficiency of mask prediction at inference. Through random masking, each image is augmented into a diverse set of training triplets, thus enabling the autoencoder to learn to reconstruct with masks of various sizes and shapes during training. To improve the efficiency and effectiveness of anomaly detection and localization at inference, we propose a novel progressive mask refinement approach that progressively uncovers the normal regions and finally locates the anomalous regions. The proposed SSM method outperforms several state-of-the-arts for both anomaly detection and anomaly localization, achieving 98.3% AUC on Retinal-OCT and 93.9% AUC on MVTec AD, respectively.",
        "references": [
            "e13d3f39cb9d03c57fef1344a825c163160dd8e7",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "8ee35ed698527d9695c872e3b76715fec4ef69ad",
            "badbd3a3df684fc8f7032c4577fb92fb1a743243",
            "d743c1b674ae539ef387252b8400a8b06c3ecf20",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "6259b02912cebc224f3a2b1324e811a152a0177d",
            "9a679663be4981d99b79f28dc946ac24344935d6"
        ],
        "related_topics": [
            "Anomaly Detection",
            "Area Under The ROC Curve",
            "UnSupervised Anomaly Detection",
            "Random Masking",
            "Inference",
            "Inpainting Network",
            "Retinal OCT",
            "Anomalous Regions",
            "Self-Supervised Learning",
            "MVTec AD"
        ],
        "reference_count": "85",
        "citation_count": "33"
    },
    {
        "Id": "6517f92d519fc126cc18924231bafd8945a554d1",
        "title": "Reconstruction Student with Attention for Student-Teacher Pyramid Matching",
        "authors": [
            "Shinji Yamada",
            "Kazuhiro Hotta"
        ],
        "date": "30 November 2021",
        "abstract": "A powerful method which compensates for the shortcomings of Student-Teacher Feature Pyramid Matching (STPM), which can be trained from only normal images with small number of epochs is proposed. Anomaly detection and localization are important problems in computer vision. Recently, Convolutional Neural Network (CNN) has been used for visual inspection. In particular, the scarcity of anomalous samples increases the difficulty of this task, and unsupervised leaning based methods are attracting attention. We focus on Student-Teacher Feature Pyramid Matching (STPM) which can be trained from only normal images with small number of epochs. Here we proposed a powerful method which compensates for the shortcomings of STPM. Proposed method consists of two students and two teachers that a pair of student-teacher network is the same as STPM. The other student-teacher network has a role to reconstruct the features of normal products. By reconstructing the features of normal products from an abnormal image, it is possible to detect abnormalities with higher accuracy by taking the difference between them. The new student-teacher network uses attention modules and different teacher network from the original STPM. Attention mechanism acts to successfully reconstruct the normal regions in an input image. Different teacher network prevents looking at the same regions as the original STPM. Six anomaly maps obtained from the two student-teacher networks are used to calculate the final anomaly map. Student-teacher network for reconstructing features improved AUC scores for pixel level and image level in comparison with the original STPM.",
        "references": [
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "82527ee075d2f7bf731da80edd8d4a92b01c2b8b",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "e8874d7d585ae1c355e186efdcc9f704b3d43b49",
            "ad655c25e052fa4eeed53421344aca6f239c4c9d",
            "a8f3dc53e321fbb2565f5925def4365b9f68d1af",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "31f9eb39d840821979e5df9f34a6e92dd9c879f2"
        ],
        "related_topics": [
            "STPM",
            "Convolutional Neural Network",
            "Image Level",
            "Pixel Level",
            "Attention Modules",
            "Anomaly Map",
            "Computer Vision",
            "Normal Images",
            "AUC Scores",
            "Anomaly Detection"
        ],
        "reference_count": "33",
        "citation_count": "17"
    },
    {
        "Id": "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
        "title": "Anomaly Detection using One-Class Neural Networks",
        "authors": [
            "Raghavendra Chalapathy",
            "Aditya Krishna Menon",
            "Sanjay Chawla"
        ],
        "date": "18 February 2018",
        "abstract": "A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and PFAM), OC-NN significantly outperforms existing state-of-the-art anomaly detection methods. We propose a one-class neural network (OC-NN) model to detect anomalies in complex data sets. OC-NN combines the ability of deep networks to extract a progressively rich representation of data with the one-class objective of creating a tight envelope around normal data. The OC-NN approach breaks new ground for the following crucial reason: data representation in the hidden layer is driven by the OC-NN objective and is thus customized for anomaly detection. This is a departure from other approaches which use a hybrid approach of learning deep features using an autoencoder and then feeding the features into a separate anomaly detection method like one-class SVM (OC-SVM). The hybrid OC-SVM approach is sub-optimal because it is unable to influence representational learning in the hidden layers. A comprehensive set of experiments demonstrate that on complex data sets (like CIFAR and GTSRB), OC-NN performs on par with state-of-the-art methods and outperformed conventional shallow methods in some scenarios.",
        "references": [
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "1d4ec24a6da3be62dc5d7efbae2a101c63f187e8",
            "2b75ba7f75170b73d913c515cc0deefef6c88f5f",
            "88f761749f5ac789f84b19ed0cff75c131dd8a29",
            "c53352a4239568cc915ad968aff51c49924a3072",
            "00a1077d298f2917d764eb729ab1bc86af3bd241",
            "081651b38ff7533550a3adfc1c00da333a8fe86c",
            "995c5f5e62614fcb4d2796ad2faab969da51713e",
            "527cc8cd2af06a9ac2e5cded806bab5c3faad9cf"
        ],
        "related_topics": [
            "One-Class Neural Networks",
            "OC-SVM",
            "Anomaly Detection",
            "Hidden Layer",
            "Deep Network",
            "CIFAR",
            "Autoencoders",
            "One-Class SVMs",
            "German Traffic Sign Recognition Benchmark"
        ],
        "reference_count": "46",
        "citation_count": "353"
    },
    {
        "Id": "82998db067c4c7fe9c7586202bf5e4249cd8f00b",
        "title": "Mixed Attention Auto Encoder for Multi-class Industrial Anomaly Detection",
        "authors": [
            "Jiangqi Liu",
            "Feng Wang"
        ],
        "date": "22 September 2023",
        "abstract": "A unified mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection with a single model and deliver remarkable performances on the benchmark dataset compared with the state-of-the-art methods. Most existing methods for unsupervised industrial anomaly detection train a separate model for each object category. This kind of approach can easily capture the category-specific feature distributions, but results in high storage cost and low training efficiency. In this paper, we propose a unified mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection with a single model. To alleviate the performance degradation due to the diverse distribution patterns of different categories, we employ spatial attentions and channel attentions to effectively capture the global category information and model the feature distributions of multiple classes. Furthermore, to simulate the realistic noises on features and preserve the surface semantics of objects from different categories which are essential for detecting the subtle anomalies, we propose an adaptive noise generator and a multi-scale fusion module for the pre-trained features. MAAE delivers remarkable performances on the benchmark dataset compared with the state-of-the-art methods.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "3d2b77469dc6c12bad63ce675def923b1f2a2628",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "c47fb40c71a70242c1320804139e1640e940e6c0",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "2e8d62277e40d465343e8dfb32ecc246f320540e",
            "ac62ed8a9b77d613189b63004f4a5d4c5cc082fe",
            "2d8c97db4bae00ff243d122b957091a236a697a7"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "20"
    },
    {
        "Id": "e59cd738bce1f415acd62c9ced1d120e573de69f",
        "title": "LafitE: Latent Diffusion Model with Feature Editing for Unsupervised Multi-class Anomaly Detection",
        "authors": [
            "Haonan Yin",
            "Guanlong Jiao",
            "Qianhui Wu",
            "B{\\&quot;o}rje F. Karlsson",
            "Biqing Huang",
            "Chin-Yew Lin"
        ],
        "date": "16 July 2023",
        "abstract": "A unified model to detect anomalies from objects belonging to multiple classes when only normal data is accessible is developed, and the proposed LafitE, \\ie, Latent Diffusion Model with Feature Editing, outperforms state-of-art methods by a significant margin in terms of average AUROC. In the context of flexible manufacturing systems that are required to produce different types and quantities of products with minimal reconfiguration, this paper addresses the problem of unsupervised multi-class anomaly detection: develop a unified model to detect anomalies from objects belonging to multiple classes when only normal data is accessible. We first explore the generative-based approach and investigate latent diffusion models for reconstruction to mitigate the notorious ``identity shortcut'' issue in auto-encoder based methods. We then introduce a feature editing strategy that modifies the input feature space of the diffusion model to further alleviate ``identity shortcuts'' and meanwhile improve the reconstruction quality of normal regions, leading to fewer false positive predictions. Moreover, we are the first who pose the problem of hyperparameter selection in unsupervised anomaly detection, and propose a solution of synthesizing anomaly data for a pseudo validation set to address this problem. Extensive experiments on benchmark datasets MVTec-AD and MPDD show that the proposed LafitE, \\ie, Latent Diffusion Model with Feature Editing, outperforms state-of-art methods by a significant margin in terms of average AUROC. The hyperparamters selected via our pseudo validation set are well-matched to the real test set.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "d78f2e0d5e3040ad62d5bcd0abca8a8507bff209",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "20f6fce7726e7b3ab4ca45ef40d92b79f093f825",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [],
        "reference_count": "65",
        "citation_count": "One"
    },
    {
        "Id": "ecfccca99ebb4edfb4d505d7faeedb3eb88decd0",
        "title": "DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection",
        "authors": [
            "Haoyang He",
            "Jiangning Zhang",
            "Hongxu Chen",
            "Xuhai Chen",
            "Zhishan Li",
            "Xu Chen",
            "Yabiao Wang",
            "Chengjie Wang",
            "Lei Xie"
        ],
        "date": "11 December 2023",
        "abstract": "A Difusion-based Anomaly Detection (DiAD) framework for multi-class anomaly detection, which consists of a pixel-space autoencoder, a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion's denoising network, and a feature-space pre-trained feature extractor. Reconstruction-based approaches have achieved remarkable outcomes in anomaly detection. The exceptional image reconstruction capabilities of recently popular diffusion models have sparked research efforts to utilize them for enhanced reconstruction of anomalous images. Nonetheless, these methods might face challenges related to the preservation of image categories and pixel-wise structural integrity in the more practical multi-class setting. To solve the above problems, we propose a Difusion-based Anomaly Detection (DiAD) framework for multi-class anomaly detection, which consists of a pixel-space autoencoder, a latent-space Semantic-Guided (SG) network with a connection to the stable diffusion's denoising network, and a feature-space pre-trained feature extractor. Firstly, The SG network is proposed for reconstructing anomalous regions while preserving the original image's semantic information. Secondly, we introduce Spatial-aware Feature Fusion (SFF) block to maximize reconstruction accuracy when dealing with extensively reconstructed areas. Thirdly, the input and reconstructed images are processed by a pre-trained feature extractor to generate anomaly maps based on features extracted at different scales. Experiments on MVTec-AD and VisA datasets demonstrate the effectiveness of our approach which surpasses the state-of-the-art methods, e.g., achieving 96.8/52.6 and 97.2/99.0 (AUROC/AP) for localization and detection respectively on multi-class MVTec-AD dataset. Code will be available at https://lewandofskee.github.io/projects/diad.",
        "references": [
            "28367cd3b68489ebd819ddfc3fb042b301abd3c7",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "55d49aa6bd3e6605c6510a147c1fb5bdd7af0b12",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "20f6fce7726e7b3ab4ca45ef40d92b79f093f825",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "015e11f1862ccb808e701123dbe1a84f0bead671",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f"
        ],
        "related_topics": [],
        "reference_count": "59",
        "citation_count": "2"
    },
    {
        "Id": "c911d99205cbeaea42e0690916a119b84ae0aaf5",
        "title": "MLAD: A Unified Model for Multi-system Log Anomaly Detection",
        "authors": [
            "Runqiang Zang",
            "Hongcheng Guo",
            "Jian Yang",
            "Jiaheng Liu",
            "Zhoujun Li",
            "Tieqiao Zheng",
            "Xu Shi",
            "Liangfan Zheng",
            "Bo Zhang"
        ],
        "date": "15 January 2024",
        "abstract": "This work proposes MLAD, a novel anomaly detection model that incorporates semantic relational reasoning across multiple systems, and employs Sentence-bert to capture the similarities between log sequences and convert them into highly-dimensional learnable semantic vectors. In spite of the rapid advancements in unsupervised log anomaly detection techniques, the current mainstream models still necessitate specific training for individual system datasets, resulting in costly procedures and limited scalability due to dataset size, thereby leading to performance bottlenecks. Furthermore, numerous models lack cognitive reasoning capabilities, posing challenges in direct transferability to similar systems for effective anomaly detection. Additionally, akin to reconstruction networks, these models often encounter the\"identical shortcut\"predicament, wherein the majority of system logs are classified as normal, erroneously predicting normal classes when confronted with rare anomaly logs due to reconstruction errors. To address the aforementioned issues, we propose MLAD, a novel anomaly detection model that incorporates semantic relational reasoning across multiple systems. Specifically, we employ Sentence-bert to capture the similarities between log sequences and convert them into highly-dimensional learnable semantic vectors. Subsequently, we revamp the formulas of the Attention layer to discern the significance of each keyword in the sequence and model the overall distribution of the multi-system dataset through appropriate vector space diffusion. Lastly, we employ a Gaussian mixture model to highlight the uncertainty of rare words pertaining to the\"identical shortcut\"problem, optimizing the vector space of the samples using the maximum expectation model. Experiments on three real-world datasets demonstrate the superiority of MLAD.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "687d0ebe2c89670f190f9202449005beadcda377",
            "c08a65e47b13c52744b6564e39c0e7c8f32a2074",
            "3fb7f11028819d6a6913174b69aca3b2229e54f0",
            "d20ac00f8d7139a087ee67d705ee5ff0ca5d5b19",
            "fefeded74334e5eafa47c5df6de2837fe3b7502d",
            "5fbea83bafab0a51c173c9230e272905702c8978",
            "a7c60fda14de91095d2a205d7efc3ef4c46a0f6a",
            "ae23ed6d405e7ab31c15e4fdf3a9fafadfee41c2",
            "6a0cc3de8c81a7cff55fb637478509dd9cfb72ef"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "50"
    },
    {
        "Id": "78873db8417583041c4fe5de25ad7a699c126a66",
        "title": "OmniAL: A Unified CNN Framework for Unsupervised Anomaly Localization",
        "authors": [
            "Ying Zhao"
        ],
        "date": "1 June 2023",
        "abstract": "A unified CNN framework for unsupervised anomaly localization, named OmniAL, is proposed that surpasses the state-of-the-art of unified models and makes the first attempt to conduct a comprehensive study on the robustness of unsuper supervised anomaly localization and detection methods against different level adversarial attacks. Unsupervised anomaly localization and detection is crucial for industrial manufacturing processes due to the lack of anomalous samples. Recent unsupervised advances on industrial anomaly detection achieve high performance by training separate models for many different categories. The model storage and training time cost of this paradigm is high. Moreover, the setting of one-model-N-classes leads to fearful degradation of existing methods. In this paper, we propose a unified CNN framework for unsupervised anomaly localization, named OmniAL. This method conquers aforementioned problems by improving anomaly synthesis, reconstruction and localization. To prevent the model learning identical reconstruction, it trains the model with proposed panel-guided synthetic anomaly data rather than directly using normal data. It increases anomaly reconstruction error for multi-class distribution by using a network that is equipped with proposed Dilated Channel and Spatial Attention (DCSA) blocks. To better localize the anomaly regions, it employs proposed DiffNeck between reconstruction and localization sub-networks to explore multi-level differences. Experiments on 15-class MVTecAD and 12-class VisA datasets verify the advantage of proposed OmniAL that surpasses the state-of-the-art of unified models. On 15-class-MVTecAD/12-class-VisA, its single unified model achieves 97.2/87.8 image-AUROC, 98.3/96.6 pixel-AUROC and 73.4/41.7 pixel-AP for anomaly detection and localization respectively. Besides that, we make the first attempt to conduct a comprehensive study on the robustness of unsupervised anomaly localization and detection methods against different level adversarial attacks. Experiential results show OmniAL has good application prospects for its superior performance.",
        "references": [
            "a6a237a7625f2c3175e44070ee9bad6e917798b0",
            "51ba3b33f445199d9f3cddb5b00c7e2927199b0c",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "2e8d62277e40d465343e8dfb32ecc246f320540e",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "57e0b6a1a47afe3d0cee58d387a7cfcd3bdd5e73",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "fc086bf5f6d1627153b68abdd5a4450e141b4ca3",
            "95a26eafabf06b1fc5dec6c460a927cf5964e97e"
        ],
        "related_topics": [
            "Unsupervised Anomaly Localization",
            "Spatial Attention",
            "VisA Dataset",
            "Adversarial Attacks",
            "Anomalous Samples",
            "Anomaly Detection",
            "Convolutional Neural Network",
            "Pixel AUROC"
        ],
        "reference_count": "45",
        "citation_count": "9"
    },
    {
        "Id": "d2ad908b75063cbfde78e99c3b1f44c60a1ae311",
        "title": "Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection",
        "authors": [
            "Ruiying Lu",
            "YuJie Wu",
            "Long Tian",
            "Dongsheng Wang",
            "Bo Chen",
            "Xiyang Liu",
            "Ruimin Hu"
        ],
        "date": "22 October 2023",
        "abstract": "This paper proposes a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework, and investigates an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns. Unsupervised image Anomaly Detection (UAD) aims to learn robust and discriminative representations of normal samples. While separate solutions per class endow expensive computation and limited generalizability, this paper focuses on building a unified framework for multiple classes. Under such a challenging setting, popular reconstruction-based networks with continuous latent representation assumption always suffer from the\"identical shortcut\"issue, where both normal and abnormal samples can be well recovered and difficult to distinguish. To address this pivotal issue, we propose a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework. First, instead of learning the continuous representations, we preserve the typical normal patterns as discrete iconic prototypes, and confirm the importance of Vector Quantization in preventing the model from falling into the shortcut. The vector quantized iconic prototype is integrated into the Transformer for reconstruction, such that the abnormal data point is flipped to a normal data point.Second, we investigate an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns. Third, a prototype-oriented optimal transport method is proposed to better regulate the prototypes and hierarchically evaluate the abnormal score. By evaluating on MVTec-AD and VisA datasets, our model surpasses the state-of-the-art alternatives and possesses good interpretability. The code is available at https://github.com/RuiyingLu/HVQ-Trans.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "78873db8417583041c4fe5de25ad7a699c126a66",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "fc086bf5f6d1627153b68abdd5a4450e141b4ca3",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "cfc21dbbdcb4c50c553d1e1fb0dbc69f61ce1d30",
            "57e0b6a1a47afe3d0cee58d387a7cfcd3bdd5e73",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "51"
    },
    {
        "Id": "336f4126e59eb7f9d167f9b76fe379280029d651",
        "title": "A Novel MAE-Based Self-Supervised Anomaly Detection and Localization Method",
        "authors": [
            "Yibo Chen",
            "Haolong Peng",
            "Le Huang",
            "Jianming Zhang",
            "Wei Jiang"
        ],
        "date": "2023",
        "abstract": "A novel end-to-end approach for multi-class anomaly detection: self-supervised Mask-pretrained Anomaly Localization Autoencoder (MALA) that demonstrates its applicability across diverse styles of industrial products. Despite significant advancements in self-supervised anomaly detection, multi-class anomaly detection tasks still pose substantial challenges. Most existing methods require individual network training for each category of objects. This paper presents a novel end-to-end approach for multi-class anomaly detection: self-supervised Mask-pretrained Anomaly Localization Autoencoder (MALA). Firstly, the masked autoencoder (MAE) and Pseudo Label Prediction Module (PLPM) are utilized to recover and perceive normal image patterns. Subsequently, the encoder weights are frozen for further end-to-end network training to predict anomalous maps directly. Token Balance Module(TBM) facilitates anomalous perception and improves anomaly segmentation. By utilizing the Visual Transformer and employing image inpainting as a proxy task, remarkable generalization results are achieved. The proposed method demonstrates its applicability across diverse styles of industrial products. Experiments are conducted on MVTech AD, VisA, KolektorSDD2, and MT datasets, achieving state-of-the-art results in multi-task anomaly detection and segmentation tasks. Specifically, we obtain image AUROC of 98.% and pixel AUROC of 97.1% on the MVTech AD dataset, pixel AUROC of 97.1% on the VisA dataset, and pixel AUROC of 98.7% on the KolektorSDD2 dataset.",
        "references": [
            "21484ffc19b1c5e41cbd49aff061db4dfb5286c2",
            "2c1837be2e4e6c4f47d51c42773e398897c372a6",
            "a4b4b63968260f142b0b5a10d8868f18d6930f4c",
            "1a7a28740c6eec40f9d8a8c3d987a8fa1b1437af",
            "78873db8417583041c4fe5de25ad7a699c126a66",
            "e29734b4945611c4eeb49bd176086881e71ba25a",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "57e0b6a1a47afe3d0cee58d387a7cfcd3bdd5e73",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "18abe29e2c50fa0b5c113a2e9458b89fb1197a8d"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "74"
    },
    {
        "Id": "60264ea13394896aeb7bb514f761a287cab1a54d",
        "title": "UniFormaly: Towards Task-Agnostic Unified Framework for Visual Anomaly Detection",
        "authors": [
            "Yujin Lee",
            "Harin Lim",
            "Seoyoon Jang",
            "Hyunsoo Yoon"
        ],
        "date": "24 July 2023",
        "abstract": "Back Patch Masking (BPM) and top k-ratio feature matching are introduced and UniFormaly is presented, a universal and powerful anomaly detection framework that achieves outstanding results on various tasks and datasets. Visual anomaly detection aims to learn normality from normal images, but existing approaches are fragmented across various tasks: defect detection, semantic anomaly detection, multi-class anomaly detection, and anomaly clustering. This one-task-one-model approach is resource-intensive and incurs high maintenance costs as the number of tasks increases. We present UniFormaly, a universal and powerful anomaly detection framework. We emphasize the necessity of our off-the-shelf approach by pointing out a suboptimal issue in online encoder-based methods. We introduce Back Patch Masking (BPM) and top k-ratio feature matching to achieve unified anomaly detection. BPM eliminates irrelevant background regions using a self-attention map from self-supervised ViTs. This operates in a task-agnostic manner and alleviates memory storage consumption, scaling to tasks with large-scale datasets. Top k-ratio feature matching unifies anomaly levels and tasks by casting anomaly scoring into multiple instance learning. Finally, UniFormaly achieves outstanding results on various tasks and datasets. Codes are available at https://github.com/YoojLee/Uniformaly.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "23ad8fc48530ce366f8192dfb48d0f7df1dba277",
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
            "41747cbdbed84762dfbfc305254c97021279dc6e"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "44"
    },
    {
        "Id": "83aa8ce07aeda545ce3092acfaf071b6626f1903",
        "title": "Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection",
        "authors": [
            "Jiangning Zhang",
            "Xuhai Chen",
            "Yabiao Wang",
            "Chengjie Wang",
            "Yong Liu",
            "Xiangtai Li",
            "Ming-Hsuan Yang",
            "Dacheng Tao"
        ],
        "date": "12 December 2023",
        "abstract": "Plain ViT architecture for MUAD is explored, which achieves state-of-the-art results and efficiency on the MVTec AD and VisA datasets without bells and whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and a comprehensive and fair evaluation benchmark on eight metrics for the MUAD task is proposed. This work studies the recently proposed challenging and practical Multi-class Unsupervised Anomaly Detection (MUAD) task, which only requires normal images for training while simultaneously testing both normal/anomaly images for multiple classes. Existing reconstruction-based methods typically adopt pyramid networks as encoders/decoders to obtain multi-resolution features, accompanied by elaborate sub-modules with heavier handcraft engineering designs for more precise localization. In contrast, a plain Vision Transformer (ViT) with simple architecture has been shown effective in multiple domains, which is simpler, more effective, and elegant. Following this spirit, this paper explores plain ViT architecture for MUAD. Specifically, we abstract a Meta-AD concept by inducing current reconstruction-based methods. Then, we instantiate a novel and elegant plain ViT-based symmetric ViTAD structure, effectively designed step by step from three macro and four micro perspectives. In addition, this paper reveals several interesting findings for further exploration. Finally, we propose a comprehensive and fair evaluation benchmark on eight metrics for the MUAD task. Based on a naive training recipe, ViTAD achieves state-of-the-art (SoTA) results and efficiency on the MVTec AD and VisA datasets without bells and whistles, obtaining 85.4 mAD that surpasses SoTA UniAD by +3.0, and only requiring 1.1 hours and 2.3G GPU memory to complete model training by a single V100 GPU. Source code, models, and more results are available at https://zhangzjn.github.io/projects/ViTAD.",
        "references": [
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "ecfccca99ebb4edfb4d505d7faeedb3eb88decd0",
            "78873db8417583041c4fe5de25ad7a699c126a66",
            "7f12319e9b0cdbef09f720bd34dacb8422504df6",
            "a09cbcaac305884f043810afc4fa4053099b5970",
            "5533bf9f2385ebece563fea35b19e998db64e597",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "886fa942f73fa0c047c5a5ab87d9770dda3c8119",
            "c9319fe84d32f00fe8e10dcf2310181236c2629a",
            "7db5da4321d526539ac567fb56cd8900def4b1e5"
        ],
        "related_topics": [],
        "reference_count": "113",
        "citation_count": "One"
    },
    {
        "Id": "553b25f1e371ae6bd7126af54206444043ee7da3",
        "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
        "authors": [
            "Qihang Zhou",
            "Guansong Pang",
            "Yu Tian",
            "Shibo He",
            "Jiming Chen"
        ],
        "date": "29 October 2023",
        "abstract": "A novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains, to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \\eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",
        "references": [
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "4b182347b943548fe6479393bb24adac21740675",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "aa207668318fec38d60b79f407fb64982e46fce9",
            "7d90243c5a46430a36c5ba88627b5d254450a1e1",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
            "2c1837be2e4e6c4f47d51c42773e398897c372a6",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "8e37b3b30a1fce1a3f41374ceaf168d2e79bf53f"
        ],
        "related_topics": [],
        "reference_count": "59",
        "citation_count": "6"
    },
    {
        "Id": "3b3aefbbdb64e5812f133f220b3f129a36a30065",
        "title": "Anomaly Detection via Reverse Distillation from One-Class Embedding",
        "authors": [
            "Hanqiu Deng",
            "Xingyu Li"
        ],
        "date": "26 January 2022",
        "abstract": "A novel T-S model consisting of a teacher encoder and a student decoder is proposed and a simple yet effective \u201creverse distillation\u201d paradigm is introduced, demonstrating the proposed approach's effectiveness and generalizability. Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective \u201creverse distillation\u201d paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model's one-class embedding as input and targets to restore the teacher's multi-scale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but aban-dons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach's effectiveness and generalizability.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "599fd051c9438011ec5b581983c89e8922b4a5e6",
            "30895c61bb836f2cae7ef5ba6516886f746a7153",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "9fe3cebb4454abc5d3bcfcad9c3228fbacdbdb08",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9"
        ],
        "related_topics": [
            "Reverse Distillation",
            "Student Decoder",
            "One-Class Embedding",
            "CutPaste",
            "Multiresolution Knowledge Distillation",
            "Large Defects",
            "Anomaly-free Samples",
            "Pseudo Anomalies",
            "WideResnet-50",
            "Localize Anomaly"
        ],
        "reference_count": "52",
        "citation_count": "161"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
        "title": "MVTec AD \u2014 A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "1 June 2019",
        "abstract": "This work introduces the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories, and conducts a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolved neural networks. The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the \ufb01eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the \ufb01rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.",
        "references": [
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "732c21998e251d64cd58b6a86886ee5907efeaa5",
            "9c24454b071bc8e96ea46c5064a7bddf07cca464",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "5d90f06bb70a0a3dced62413346235c02b1aa086"
        ],
        "related_topics": [
            "MVTec AD",
            "MVTec Anomaly Detection",
            "UnSupervised Anomaly Detection",
            "Texture Categories",
            "Metal Nut",
            "Anomalous Images",
            "MVTec AD Dataset",
            "Anomalous Regions",
            "Spatial Anomaly Map",
            "Anomaly-free Images"
        ],
        "reference_count": "29",
        "citation_count": "748"
    },
    {
        "Id": "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
        "title": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection",
        "authors": [
            "Guodong Wang",
            "Shumin Han",
            "Errui Ding",
            "Di Huang"
        ],
        "date": "2021",
        "abstract": "This paper proposes a simple yet powerful approach to anomaly detection, implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Anomaly detection is a challenging task and usually for-mulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Given a strong model pre-trained on image classi\ufb01cation as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, signi\ufb01cantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256 \u00d7 256), at least dozens of times faster than the latest counterparts.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "388645c44061f6e88fff0ecdad2f622936207d67",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2528a82dd2266600d4ee2b54165556a984de94d4"
        ],
        "related_topics": [
            "Student-Teacher Feature Pyramid Matching",
            "Pixel-level Anomaly Detection",
            "Anomaly-free Images",
            "Image-level Anomaly Detection",
            "MVTec AD Dataset",
            "Per-region-overlap",
            "Student Network",
            "Feature Pyramids",
            "Unsupervised Learning",
            "Scoring Function"
        ],
        "reference_count": "45",
        "citation_count": "63"
    },
    {
        "Id": "8ee35ed698527d9695c872e3b76715fec4ef69ad",
        "title": "Learning Semantic Context from Normal Samples for Unsupervised Anomaly Detection",
        "authors": [
            "Xudong Yan",
            "Huaidong Zhang",
            "Xuemiao Xu",
            "Xiaowei Hu",
            "Pheng-Ann Heng"
        ],
        "date": "18 May 2021",
        "abstract": "This work presents a Semantic Context based Anomaly Detection Network, SCADN, for unsupervised anomaly detection by learning the semantic context from the normal samples by generating multi-scale striped masks to remove a part of regions from thenormal samples and training a generative adversarial network to reconstruct the unseen regions. Unsupervised anomaly detection aims to identify data samples that have low probability density from a set of input samples, and only the normal samples are provided for model training. The inference of abnormal regions on the input image requires an understanding of the surrounding semantic context. This work presents a Semantic Context based Anomaly Detection Network, SCADN, for unsupervised anomaly detection by learning the semantic context from the normal samples. To achieve this, we first generate multi-scale striped masks to remove a part of regions from the normal samples, and then train a generative adversarial network to reconstruct the unseen regions. Note that the masks are designed in multiple scales and stripe directions, and various training examples are generated to obtain the rich semantic context . In testing, we obtain an error map by computing the difference between the reconstructed image and the input image for all samples, and infer the abnormal samples based on the error maps. Finally, we perform various experiments on three public benchmark datasets and a new dataset LaceAD collected by us, and show that our method clearly outperforms the current state-of-the-art methods.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "18e9e01f6cff97b9ac35c4300761cfc61a04ad8a",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "70f9968a356d840040a1c9207906f60376dc6bd4",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803"
        ],
        "related_topics": [
            "Normal Samples",
            "UnSupervised Anomaly Detection",
            "Abnormal Samples",
            "Generative Adversarial Networks"
        ],
        "reference_count": "46",
        "citation_count": "74"
    },
    {
        "Id": "18f207d8dab7357f4f674211ec4f150de1c93a0e",
        "title": "Learning Memory-Guided Normality for Anomaly Detection",
        "authors": [
            "Hyunjong Park",
            "Jongyoun Noh",
            "Bumsub Ham"
        ],
        "date": "30 March 2020",
        "abstract": "This work proposes to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data, boosting the discriminative power of both memory items and deeply learned features from normal data and lessening the representation capacity of CNNs. We address the problem of anomaly detection, that is, detecting anomalous events in a video sequence. Anomaly detection methods based on convolutional neural networks (CNNs) typically leverage proxy tasks, such as reconstructing input video frames, to learn models describing normality without seeing anomalous samples at training time, and quantify the extent of abnormalities using the reconstruction error at test time. The main drawbacks of these approaches are that they do not consider the diversity of normal patterns explicitly, and the powerful representation capacity of CNNs allows to reconstruct abnormal video frames. To address this problem, we present an unsupervised learning approach to anomaly detection that considers the diversity of normal patterns explicitly, while lessening the representation capacity of CNNs. To this end, we propose to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data. We also present novel feature compactness and separateness losses to train the memory, boosting the discriminative power of both memory items and deeply learned features from normal data. Experimental results on standard benchmarks demonstrate the effectiveness and efficiency of our approach, which outperforms the state of the art.",
        "references": [
            "7a89447be0a176368926f1ef108512f4df5e27be",
            "fef6f1e04fa64f2f26ac9f01cd143dd19e549790",
            "792250ae660b7c25f85eeea7dcae623e4301d97c",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "e5366a704ffa3b41aacd385f3c087ec3fd566934",
            "094ac7510d1723cb9c2da01db47291322aa29025",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "8a6acba7fb2aad1299fcf35701417e063d410ed4",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "527cc8cd2af06a9ac2e5cded806bab5c3faad9cf"
        ],
        "related_topics": [
            "Separateness Losses",
            "Normal Patterns",
            "CUHK Avenue",
            "Memory Module",
            "UCSD Ped2",
            "Prototypical Patterns",
            "MemAE",
            "Memory Items",
            "Abnormal Frames",
            "Memory-augmented Autoencoder"
        ],
        "reference_count": "52",
        "citation_count": "414"
    },
    {
        "Id": "7b7087b7452adc2fe8a874678049f591c1342c0f",
        "title": "Deep Morphological Anomaly Detection Based on Angular Margin Loss",
        "authors": [
            "Taehyeon Kim",
            "Eun Gi Hong",
            "Yoonsik Choe"
        ],
        "date": "16 July 2021",
        "abstract": "The main idea behind the proposed algorithm is to train a multiclass model to classify hundreds of morphological transformation cases applied to all the given data, which outperforms several recent anomaly detection methodologies in various datasets. Deep anomaly detection aims to identify \u201cabnormal\u201d data by utilizing a deep neural network trained on a normal training dataset. In general, industrial visual anomaly detection systems distinguish between normal and \u201cabnormal\u201d data through small morphological differences such as cracks and stains. Nevertheless, most existing algorithms emphasize capturing the semantic features of normal data rather than the morphological features. Therefore, they yield poor performance on real-world visual inspection, although they show their superiority in simulations with representative image classification datasets. To address this limitation, we propose a novel deep anomaly detection algorithm based on the salient morphological features of normal data. The main idea behind the proposed algorithm is to train a multiclass model to classify hundreds of morphological transformation cases applied to all the given data. To this end, the proposed algorithm utilizes a self-supervised learning strategy, making unsupervised learning straightforward. Additionally, to enhance the performance of the proposed algorithm, we replaced the cross-entropy-based loss function with the angular margin loss function. It is experimentally demonstrated that the proposed algorithm outperforms several recent anomaly detection methodologies in various datasets.",
        "references": [
            "0dccbb7902ddda1d32599b65f8b34d90c44dc718",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "f7174c5c29c3904cc2d23f26be2b896a5bc715b4",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803"
        ],
        "related_topics": [],
        "reference_count": "38",
        "citation_count": "2"
    },
    {
        "Id": "38ca689c2f916c648ea3ecb1043facbc4bea0d4f",
        "title": "Puzzle-AE: Novelty Detection in Images through Solving Puzzles",
        "authors": [
            "Mohammadreza Salehi",
            "Ainaz Eftekhar",
            "Niousha Sadjadi",
            "Mohammad Hossein Rohban",
            "Hamid R. Rabiee"
        ],
        "date": "29 August 2020",
        "abstract": "This work proposes adversarial robust training as an effective automatic shortcut removal and achieves competitive or superior results compared to the State of the Art (SOTA) anomaly detection methods on various toy and real-world datasets. Autoencoder, as an essential part of many anomaly detection methods, is lacking flexibility on normal data in complex datasets. U-Net is proved to be effective for this purpose but overfits on the training data if trained by just using reconstruction error similar to other AE-based frameworks. Puzzle-solving, as a pretext task of self-supervised learning (SSL) methods, has earlier proved its ability in learning semantically meaningful features. We show that training U-Nets based on this task is an effective remedy that prevents overfitting and facilitates learning beyond pixel-level features. Shortcut solutions, however, are a big challenge in SSL tasks, including jigsaw puzzles. We propose adversarial robust training as an effective automatic shortcut removal. We achieve competitive or superior results compared to the State of the Art (SOTA) anomaly detection methods on various toy and real-world datasets. Unlike many competitors, the proposed framework is stable, fast, data-efficient, and does not require unprincipled early stopping.",
        "references": [
            "10b219619e88931fabb674037bbb633682775136",
            "c2b733a79db700b971327a58ef42699fe8a416aa",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "6d4a87759917132913319960389f17fa1fe8b630",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "70f9968a356d840040a1c9207906f60376dc6bd4",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "599fd051c9438011ec5b581983c89e8922b4a5e6"
        ],
        "related_topics": [
            "Puzzle-AE",
            "Latent Space Autoregression",
            "U-Net",
            "Anomaly Detection",
            "Semi-Supervised Learning",
            "Data Efficient",
            "Early Stopping",
            "Self-Supervised Learning",
            "Overfitting",
            "Novelty Detection"
        ],
        "reference_count": "66",
        "citation_count": "34"
    },
    {
        "Id": "205a1c89058ea55fe536c6484a62213d1d0f0160",
        "title": "An Improved Reverse Distillation Model for Unsupervised Anomaly Detection",
        "authors": [
            "Van-Duc Nguyen",
            "Hoang Huu Bach",
            "Le Hong Trang"
        ],
        "date": "3 January 2023",
        "abstract": "Testing results obtained on benchmarks for AD and one-class novelty detection showed that the proposed model outperforms the SOTA ones, proving the utility and applicability of the suggested strategy. Using knowledge distillation for unsupervised anomaly detection problems is more efficient. Recently, a reverse distillation (RD) model has been presented a novel teacher-student (T-S) model for the problem [7]. In the model, the student network uses the one-class embedding from the teacher model as input with the goal of restoring the teacher's rep-resentations. The knowledge distillation starts with high-level abstract presentations and moves down to low-level aspects using a model called one-class bottleneck embedding (OCBE). Although its performance is expressive, it still leverages the power of transforming input images before applying this architecture. Instead of only using raw images, in this paper, we transform them using augmentation techniques. The teacher will encode raw and transformed inputs to get raw representation (encoded from raw inputs) and transformed representation (encoded from transformed inputs). The student must restore the transformed representation from the bottleneck to the raw representation. Testing results obtained on benchmarks for AD and one-class novelty detection showed that our proposed model outperforms the SOTA ones, proving the utility and applicability of the suggested strategy.",
        "references": [
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e"
        ],
        "related_topics": [
            "Knowledge Distillation",
            "Reverse Distillation",
            "UnSupervised Anomaly Detection",
            "Student Network",
            "One-Class Embedding",
            "One-class Novelty Detection",
            "Self-organizing Tree Algorithm"
        ],
        "reference_count": "0",
        "citation_count": "24"
    },
    {
        "Id": "cec282840ed7992af45400472fa545c94a6e3f7d",
        "title": "ADPS: Asymmetric Distillation Post-Segmentation for Image Anomaly Detection",
        "authors": [
            "Peng Xing",
            "Hao Tang",
            "Jinhui Tang",
            "Zechao Li"
        ],
        "date": "19 October 2022",
        "abstract": "This work proposes an innovative approach called Asymmetric Distillation Post-Segmentation (ADPS), which employs an asymmetric distillation paradigm that takes distinct forms of the same image as the input of the teacher-student networks, driving the student network to learn discriminating representations for anomalous regions. Knowledge Distillation-based Anomaly Detection (KDAD) methods rely on the teacher-student paradigm to detect and segment anomalous regions by contrasting the unique features extracted by both networks. However, existing KDAD methods suffer from two main limitations: 1) the student network can effortlessly replicate the teacher network's representations, and 2) the features of the teacher network serve solely as a ``reference standard\"and are not fully leveraged. Toward this end, we depart from the established paradigm and instead propose an innovative approach called Asymmetric Distillation Post-Segmentation (ADPS). Our ADPS employs an asymmetric distillation paradigm that takes distinct forms of the same image as the input of the teacher-student networks, driving the student network to learn discriminating representations for anomalous regions. Meanwhile, a customized Weight Mask Block (WMB) is proposed to generate a coarse anomaly localization mask that transfers the distilled knowledge acquired from the asymmetric paradigm to the teacher network. Equipped with WMB, the proposed Post-Segmentation Module (PSM) is able to effectively detect and segment abnormal regions with fine structures and clear boundaries. Experimental results demonstrate that the proposed ADPS outperforms the state-of-the-art methods in detecting and segmenting anomalies. Surprisingly, ADPS significantly improves Average Precision (AP) metric by 9% and 20% on the MVTec AD and KolektorSDD2 datasets, respectively.",
        "references": [
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "93040f8a5d10e8fde279e18d353aa3dca2873900"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "66"
    },
    {
        "Id": "61840de4d9610558d510cfcf32986e93511a4cef",
        "title": "Asymmetric Distillation Post-Segmentation Method for Image Anomaly Detection",
        "authors": [
            "Peng-Fei Xing",
            "Zechao Li"
        ],
        "date": "2022",
        "abstract": "A novel Asymmetric Distillation Post-Segmentation (ADPS) method to effectively explore the asymmetric structure of the input and the discriminative features of the teacher network is proposed and achieves state-of-the-art anomaly segmentation results. Knowledge distillation-based anomaly detection methods generate same outputs for unknown classes due to the symmetric form of the input and ignore the powerful semantic information of the output of the teacher network since it is only used as a \u201creference standard\u201d. Towards this end, this work proposes a novel Asymmetric Distillation Post-Segmentation (ADPS) method to effectively explore the asymmetric structure of the input and the discriminative features of the teacher network. Speci\ufb01cally, a simple yet effective asymmetric input approach is proposed to make different data \ufb02ows through the teacher and student networks. The student network enables to have different inductive and expressive abilities, which can generate different outputs in anomalous regions. Besides, to further explore the semantic information of the teacher network and obtain effective discriminative boundaries, the Weight Mask Block (WMB) and the post-segmentation module are proposede. WMB leverages a weighted strategy by exploring teacher-student feature maps to highlight anomalous features. The post-segmentation module further learns the anomalous features and obtains valid discriminative boundaries. Experimental results on three benchmark datasets demonstrate that the proposed ADPS achieves state-of-the-art anomaly segmentation results.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "2b32b46f346d9b13268f0e74e5242a10a712a352",
            "8e180ffb0c4bfe4db41a245637042a28fc98d891",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "6517f92d519fc126cc18924231bafd8945a554d1",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f"
        ],
        "related_topics": [
            "Teacher Network",
            "Student Network",
            "Semantic Information",
            "Weighted Mini-bucket",
            "Unknown Classes",
            "Image Anomaly Detection",
            "Anomalous Regions",
            "Knowledge Distillation",
            "Anomaly Detection",
            "Benchmark Dataset"
        ],
        "reference_count": "0",
        "citation_count": "52"
    },
    {
        "Id": "5c04ce7f8510af40f2931535feeaf220832ab548",
        "title": "SLSG: Industrial Image Anomaly Detection by Learning Better Feature Embeddings and One-Class Classification",
        "authors": [
            "Minghui Yang",
            "Jing Liu",
            "Zhiwei Yang",
            "Zhaoyang Wu"
        ],
        "date": "30 April 2023",
        "abstract": "A network based on self-supervised learning and self-attentive graph convolution (SLSG) for anomaly detection, which comprehensively models the dense and sparse relationships among elements in the image, which further strengthens the detection of logical anomalies. Industrial image anomaly detection under the setting of one-class classification has significant practical value. However, most existing models struggle to extract separable feature representations when performing feature embedding and struggle to build compact descriptions of normal features when performing one-class classification. One direct consequence of this is that most models perform poorly in detecting logical anomalies which violate contextual relationships. Focusing on more effective and comprehensive anomaly detection, we propose a network based on self-supervised learning and self-attentive graph convolution (SLSG) for anomaly detection. SLSG uses a generative pre-training network to assist the encoder in learning the embedding of normal patterns and the reasoning of position relationships. Subsequently, SLSG introduces the pseudo-prior knowledge of anomaly through simulated abnormal samples. By comparing the simulated anomalies, SLSG can better summarize the normal features and narrow down the hypersphere used for one-class classification. In addition, with the construction of a more general graph structure, SLSG comprehensively models the dense and sparse relationships among elements in the image, which further strengthens the detection of logical anomalies. Extensive experiments on benchmark datasets show that SLSG achieves superior anomaly detection performance, demonstrating the effectiveness of our method.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "355b4e74774798c177c82943eef925d66a2bb2ce",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "e5349e937545d3f3d18d254bd21d695e7350ea8e",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "7f12319e9b0cdbef09f720bd34dacb8422504df6"
        ],
        "related_topics": [
            "One-class Classification",
            "Anomaly Detection",
            "Logical Anomalies",
            "Feature Embeddings",
            "Normal Patterns",
            "Simulated Anomalies",
            "Hypersphere",
            "Embeddings",
            "Self-Supervised Learning",
            "Benchmark Dataset"
        ],
        "reference_count": "0",
        "citation_count": "57"
    },
    {
        "Id": "2013f18832e0fb11c7aaf1f4f8da453aebda488c",
        "title": "Pull & Push: Leveraging Differential Knowledge Distillation for Efficient Unsupervised Anomaly Detection and Localization",
        "authors": [
            "Qihang Zhou",
            "Shibo He",
            "Haoyu Liu",
            "Tao Chen",
            "Jiming Chen"
        ],
        "date": "1 May 2023",
        "abstract": "This work designs an efficient teacher-student model for anomaly detection and localization, which maximizes pixel-wise discrepancies for anomalous regions approximated by data augmentation and simultaneously minimizes discrepancies for pixel- wise normal regions between these two networks. Recently, much attention has been paid to segmenting subtle unknown defect regions by knowledge distillation in an unsupervised setting. Most previous studies concentrated on guiding the student network to learn the same representations on the normality, neglecting the different behaviors of the abnormality. This leads to a high probability of false detection of subtle defects. To address such an issue, we propose to push representations on abnormal areas of the teacher and student network as far as possible while pulling representations on normal areas as close as possible. Based on this idea, we design an efficient teacher-student model for anomaly detection and localization, which maximizes pixel-wise discrepancies for anomalous regions approximated by data augmentation and simultaneously minimizes discrepancies for pixel-wise normal regions between these two networks. The explicit differential knowledge distillation enlarges the margin between normal representations and abnormal ones in favour of discriminating them. Then, the appropriate small student network is not only efficient, but more importantly, helps inhibit the generalization ability of anomalous patterns when learning normal patterns, facilitating the precise decision boundary. The experimental results on the MVTec AD, Fashion-MNIST, and CIFAR-10 datasets demonstrate that our proposed method achieves better performance than current state-of-the-art (SOTA) approaches. Especially, For the MVTec AD dataset with high resolution images, we achieve 98.1 AUROC% and 93.6 AUPRO% in anomaly localization, outperforming knowledge distillation based SOTA methods by 1.1 AUROC% and 1.5 AUPRO% with a lightweight model.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "4dd78b8d466b4cfe55a1bbdc694291197ce62541",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "7d90243c5a46430a36c5ba88627b5d254450a1e1",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "23ad8fc48530ce366f8192dfb48d0f7df1dba277"
        ],
        "related_topics": [
            "Student Network",
            "Knowledge Distillation",
            "MVTec AD",
            "Self-organizing Tree Algorithm",
            "Area Under The Receiver Operating Characteristic Curve",
            "Margin",
            "Student Model",
            "CIFAR-10",
            "UnSupervised Anomaly Detection",
            "Anomaly Detection"
        ],
        "reference_count": "64",
        "citation_count": "8"
    },
    {
        "Id": "fa5aaa7c45e4cd727226a75f5b1b8e5d33460a87",
        "title": "Contextual Affinity Distillation for Image Anomaly Detection",
        "authors": [
            "J. Zhang",
            "Masanori Suganuma",
            "Takayuki Okatani"
        ],
        "date": "6 July 2023",
        "abstract": "The global context condensing block (GCCB) is designed and a contextual affinity loss for the student training and anomaly scoring is proposed and achieves a new state-of-the-art performance on the MVTec LOCO AD dataset. Previous works on unsupervised industrial anomaly detection mainly focus on local structural anomalies such as cracks and color contamination. While achieving significantly high detection performance on this kind of anomaly, they are faced with logical anomalies that violate the long-range dependencies such as a normal object placed in the wrong position. In this paper, based on previous knowledge distillation works, we propose to use two students (local and global) to better mimic the teacher's behavior. The local student, which is used in previous studies mainly focuses on structural anomaly detection while the global student pays attention to logical anomalies. To further encourage the global student's learning to capture long-range dependencies, we design the global context condensing block (GCCB) and propose a contextual affinity loss for the student training and anomaly scoring. Experimental results show the proposed method doesn't need cumbersome training techniques and achieves a new state-of-the-art performance on the MVTec LOCO AD dataset.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "11709bfadfd6bbb371f4077bccb7c26d93c39cdd",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "23ad8fc48530ce366f8192dfb48d0f7df1dba277",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "e26851c89afcfb1a0fe00292590c9f6e830c4ec0"
        ],
        "related_topics": [
            "Logical Anomalies",
            "Long-range Dependencies",
            "Image Anomaly Detection",
            "Knowledge Distillation",
            "Anomaly Scoring"
        ],
        "reference_count": "40",
        "citation_count": "One"
    },
    {
        "Id": "98e3b4be394a4bc137ac3707c3c90b8df505b1d0",
        "title": "Remembering Normality: Memory-guided Knowledge Distillation for Unsupervised Anomaly Detection",
        "authors": [
            "Zhihao Gu",
            "Liang Liu",
            "Xu Chen",
            "Ran Yi",
            "Jiangning Zhang",
            "Yabiao Wang",
            "Chengjie Wang",
            "Annan Shu",
            "Guannan Jiang",
            "Lizhuang Ma"
        ],
        "date": "1 October 2023",
        "abstract": "A novel Memory-guided Knowledge-Distillation framework that adaptively modulates the normality of student features in detecting anomalies is introduced and a normality recall memory is proposed to strengthen the normality of student-generated features by recalling the stored normal information. Knowledge distillation (KD) has been widely explored in unsupervised anomaly detection (AD). The student is assumed to constantly produce representations of typical patterns within trained data, named \"normality\", and the representation discrepancy between the teacher and student model is identified as anomalies. However, it suffers from the \"normality forgetting\" issue. Trained on anomaly-free data, the student still well reconstructs anomalous representations for anomalies and is sensitive to fine patterns in normal data, which also appear in training. To mitigate this issue, we introduce a novel Memory-guided Knowledge-Distillation (MemKD) framework that adaptively modulates the normality of student features in detecting anomalies. Specifically, we first propose a normality recall memory (NR Memory) to strengthen the normality of student-generated features by recalling the stored normal information. In this sense, representations will not present anomalies and fine patterns will be well described. Subsequently, we employ a normality embedding learning strategy to promote information learning for the NR Memory. It constructs a normal exemplar set so that the NR Memory can memorize prior knowledge in anomaly-free data and later recall them from the query feature. Consequently, comprehensive experiments demonstrate that the proposed MemKD achieves promising results on five benchmarks.",
        "references": [
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "655158947fd2a3b1e77703d9f4e951e7c583f894",
            "93040f8a5d10e8fde279e18d353aa3dca2873900",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "11709bfadfd6bbb371f4077bccb7c26d93c39cdd",
            "9775d372bfaf889a395dc714e283b6a179e62537"
        ],
        "related_topics": [],
        "reference_count": "32",
        "citation_count": "One"
    },
    {
        "Id": "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
        "title": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection",
        "authors": [
            "Guodong Wang",
            "Shumin Han",
            "Errui Ding",
            "Di Huang"
        ],
        "date": "2021",
        "abstract": "This paper proposes a simple yet powerful approach to anomaly detection, implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Anomaly detection is a challenging task and usually for-mulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Given a strong model pre-trained on image classi\ufb01cation as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, signi\ufb01cantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256 \u00d7 256), at least dozens of times faster than the latest counterparts.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "388645c44061f6e88fff0ecdad2f622936207d67",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2528a82dd2266600d4ee2b54165556a984de94d4"
        ],
        "related_topics": [
            "Student-Teacher Feature Pyramid Matching",
            "Pixel-level Anomaly Detection",
            "Anomaly-free Images",
            "Image-level Anomaly Detection",
            "MVTec AD Dataset",
            "Per-region-overlap",
            "Student Network",
            "Feature Pyramids",
            "Unsupervised Learning",
            "Scoring Function"
        ],
        "reference_count": "45",
        "citation_count": "63"
    },
    {
        "Id": "41747cbdbed84762dfbfc305254c97021279dc6e",
        "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "6 November 2019",
        "abstract": "A powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images by trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2910bec6d4de87e22be5119cef3c488d2ae50e2a",
            "68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [
            "Uninformed Students",
            "Anomaly-free Images",
            "Per-region-overlap",
            "MVTec Anomaly Detection Dataset",
            "Unsupervised Anomaly Segmentation",
            "Anomaly-free Training Data",
            "Anomalous Regions",
            "MVTec AD",
            "Spatial Anomaly Map",
            "OCGAN"
        ],
        "reference_count": "37",
        "citation_count": "376"
    },
    {
        "Id": "8e180ffb0c4bfe4db41a245637042a28fc98d891",
        "title": "Learning Unsupervised Metaformer for Anomaly Detection",
        "authors": [
            "Jhih-Ciang Wu",
            "Ding-Jie Chen",
            "Chiou-Shann Fuh",
            "Tyng-Luh Liu"
        ],
        "date": "1 October 2021",
        "abstract": "This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap, and uses an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions. Anomaly detection (AD) aims to address the task of classification or localization of image anomalies. This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap. The former generalizes an AD model to tackling a broad range of object categories, while the latter provides useful clues for localizing abnormal regions. At the core of our method is an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions, i.e., to explore the reconstruction gap at those regions of interest. We justify the effectiveness of our method with SOTA results on the MVTec AD dataset of industrial images and highlight the adaptation flexibility of the universal Metaformer with multi-class and few-shot scenarios.",
        "references": [
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "bf79f0b8b68d7ba67420aa6f88dc7aa5eddd1d73",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "Reconstruction Gap",
            "Instance-aware Attention",
            "MetaFormer",
            "Anomaly Detection",
            "Industrial Images",
            "Object Categories",
            "Classification",
            "MVTec AD Dataset",
            "Few-shot Scenario"
        ],
        "reference_count": "39",
        "citation_count": "42"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "d08775cf2bebcffa05c6fa506f687ef56953f128",
        "title": "AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning",
        "authors": [
            "Jou Won Song",
            "Kyeongbo Kong",
            "Ye In Park",
            "Seonggyun Kim",
            "Suk-Ju Kang"
        ],
        "date": "7 October 2021",
        "abstract": "The experiments show that the proposed AnoSeg outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset and compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks. Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.",
        "references": [
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "1a00dc525da31292e3734cbae2de681f114e30b1",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "3447d8b47a8cf7ce9f04ede314f0ded8172fa470",
            "e13d3f39cb9d03c57fef1344a825c163160dd8e7"
        ],
        "related_topics": [
            "AnoSeg",
            "Synthetic Anomaly Data",
            "Anomaly Segmentation",
            "Self-Supervised Learning",
            "Normal Data",
            "Anomaly Detection",
            "Intersection Over Union",
            "Adversarial Loss",
            "MVTec AD Dataset",
            "Segmentation Task"
        ],
        "reference_count": "27",
        "citation_count": "30"
    },
    {
        "Id": "363c81a08858df8dd7d1bde79c6e002e3b19f900",
        "title": "Attribute Restoration Framework for Anomaly Detection",
        "authors": [
            "Fei Ye",
            "Chaoqin Huang",
            "Jinkun Cao",
            "Maosen Li",
            "Ya Zhang",
            "Cewu Lu"
        ],
        "date": "25 November 2019",
        "abstract": "This work proposes to break information equivalence among input and supervision for reconstruction tasks by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "7c4528f0ff263b6e7a6cee0e959f0e3615b3c65e",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "3447d8b47a8cf7ce9f04ede314f0ded8172fa470",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7"
        ],
        "related_topics": [
            "MVTec AD",
            "Anomaly Detection",
            "Anomalous Data",
            "Supervision",
            "ImageNet",
            "Restoration Tasks",
            "Reconstruction-based Methods",
            "Area Under The Receiver Operating Characteristic Curve",
            "Normal Data",
            "Benchmark Dataset"
        ],
        "reference_count": "46",
        "citation_count": "127"
    },
    {
        "Id": "93040f8a5d10e8fde279e18d353aa3dca2873900",
        "title": "Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection",
        "authors": [
            "Jinlei Hou",
            "Yingying Zhang",
            "Qiaoyong Zhong",
            "Di Xie",
            "Shiliang Pu",
            "Hong Zhou"
        ],
        "date": "28 July 2021",
        "abstract": "By varying the granularity of division on feature maps, this work is able to modulate the reconstruction capability of the model for both normal and abnormal samples, and achieves state-of-the-art performance on the challenging MVTec AD dataset. Reconstruction-based methods play an important role in unsupervised anomaly detection in images. Ideally, we expect a perfect reconstruction for normal samples and poor reconstruction for abnormal samples. Since the generalizability of deep neural networks is difficult to control, existing models such as autoencoder do not work well. In this work, we interpret the reconstruction of an image as a divide-and-assemble procedure. Surprisingly, by varying the granularity of division on feature maps, we are able to modulate the reconstruction capability of the model for both normal and abnormal samples. That is, finer granularity leads to better reconstruction, while coarser granularity leads to poorer reconstruction. With proper granularity, the gap between the reconstruction error of normal and abnormal samples can be maximized. The divide-and-assemble framework is implemented by embedding a novel multi-scale block-wise memory module into an autoencoder network. Besides, we introduce adversarial learning and explore the semantic latent representation of the discriminator, which improves the detection of subtle anomaly. We achieve state-of-the-art performance on the challenging MVTec AD dataset. Remarkably, we improve the vanilla autoencoder model by 10.1% in terms of the AUROC score.",
        "references": [
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2f7af18b35d155064243c21d0818b1570a3a696e",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "3447d8b47a8cf7ce9f04ede314f0ded8172fa470",
            "7c4528f0ff263b6e7a6cee0e959f0e3615b3c65e",
            "18f207d8dab7357f4f674211ec4f150de1c93a0e",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "DAAD",
            "MVTec AD",
            "Abnormal Samples",
            "Autoencoders",
            "UnSupervised Anomaly Detection",
            "Normal Samples",
            "Feature Maps",
            "Reconstruction-based Methods",
            "Discriminator",
            "MVTec AD Dataset"
        ],
        "reference_count": "57",
        "citation_count": "77"
    },
    {
        "Id": "a9eed8bdee35598dba7e4133d0b08691982106dc",
        "title": "Adaptive Context-Aware Distillation for Industrial Image Anomaly Detection",
        "authors": [
            "Yuan-yuan He",
            "Hua Yang",
            "Zhouping Yin"
        ],
        "date": "2024",
        "abstract": "Extensive experiments with mainstream anomaly detection datasets show that ACAD outperforms the state-of-the-art competitors in accuracy and efficiency and the experimental results with a real-world inkjet printing organic electroluminescence display (OLED) panel dataset further demonstrate the effectiveness of the method. Image anomaly detection is extremely challenging in industrial manufacturing processes due to unforeseen and diversified anomalies. Recently, unsupervised anomaly detection methods based on knowledge distillation have been developed and have shown remarkable potential. While most existing methods are devoted to knowledge generalization, they are inadequate for the fine-grained detection task. To address this issue, we propose a novel adaptive context-aware distillation (ACAD) paradigm that gives due consideration to distillation component dependencies and knowledge transfer optimization. Technically, a novel adaptive distillation module (ADM) is proposed for optimal context-aware knowledge transfer, which consists of contrastive decoupling distillation (CDD) and masked perceiving distillation (MPD). The proposed CDD helps to constrain the distribution of different semantic patterns and strengthen the discriminative capability. Vanilla methods treat every pixel as an equal contribution and fail to focus on critical information. To this end, the MPD is proposed to weigh different contextual knowledge adaptively. Extensive experiments with mainstream anomaly detection datasets show that ACAD outperforms the state-of-the-art competitors in accuracy and efficiency. In addition, the experimental results with a real-world inkjet printing organic electroluminescence display (OLED) panel dataset further demonstrate the effectiveness of our method.",
        "references": [
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "2013f18832e0fb11c7aaf1f4f8da453aebda488c",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "2b32b46f346d9b13268f0e74e5242a10a712a352",
            "d17df33c9b6453d61d01353e94592f1757caee8a",
            "51ba3b33f445199d9f3cddb5b00c7e2927199b0c",
            "23ad8fc48530ce366f8192dfb48d0f7df1dba277",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "63"
    },
    {
        "Id": "35dfd96486c38d9a21b1d29e2381d72cd7928b2f",
        "title": "Teacher-Student Network for 3D Point Cloud Anomaly Detection with Few Normal Samples",
        "authors": [
            "Jianjian Qin",
            "Chunzhi Gu",
            "Junzhou Yu",
            "Chaoxi Zhang"
        ],
        "date": "31 October 2022",
        "abstract": "A teacher-student structured model for 3D anomaly detection that uses feature space alignment, dimension zoom, and max pooling to extract the features of the point cloud and then minimize a multi-scale loss between the feature vectors produced by the teacher and the student networks. Anomaly detection, which is a critical and popular topic in computer vision, aims to detect anomalous samples that are different from the normal (i.e., non-anomalous) ones. The current mainstream methods focus on anomaly detection for images, whereas little attention has been paid to 3D point cloud. In this paper, drawing inspiration from the knowledge transfer ability of teacher-student architecture and the impressive feature extraction capability of recent neural networks, we design a teacher-student structured model for 3D anomaly detection. Specifically, we use feature space alignment, dimension zoom, and max pooling to extract the features of the point cloud and then minimize a multi-scale loss between the feature vectors produced by the teacher and the student networks. Moreover, our method only requires very few normal samples to train the student network due to the teacher-student distillation mechanism. Once trained, the teacher-student network pair can be leveraged jointly to fulfill 3D point cloud anomaly detection based on the calculated anomaly score. For evaluation, we compare our method against the reconstruction-based method on the ShapeNet-Part dataset. The experimental results and ablation studies quantitatively and qualitatively confirm that our model can achieve higher performance compared with the state of the arts in 3D anomaly detection with very few training samples.",
        "references": [
            "7d5c0ae2985e0c665cb3713aaa169ee5c7c9794f",
            "ca6294ce6f38f3f6ff694ba67d02eaad7fc92f4e",
            "02805f18989b7e77f30ee13defd6fecfcd0f499f",
            "d997beefc0922d97202789d2ac307c55c2c52fba",
            "cd382609f0029aae042e91a5a46b3dc2ba58a321",
            "3b3aefbbdb64e5812f133f220b3f129a36a30065",
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "2013f18832e0fb11c7aaf1f4f8da453aebda488c",
            "655158947fd2a3b1e77703d9f4e951e7c583f894",
            "5fdd9cfb9b22b9e575d36e98e5d6f52a83729e42"
        ],
        "related_topics": [
            "Anomaly Detection",
            "3D Point Clouds",
            "Normal Samples",
            "Anomaly Score",
            "Max-pooling",
            "Reconstruction-based Methods",
            "ShapeNet Part Dataset",
            "Teacher-student Architecture",
            "Neural Network",
            "Computer Vision"
        ],
        "reference_count": "55",
        "citation_count": "2"
    },
    {
        "Id": "1d0274b642f852897205fc9854e0de4068f89513",
        "title": "Cascade RDN: Towards Accurate Localization in Industrial Visual Anomaly Detection With Structural Anomaly Generation",
        "authors": [
            "Jian Zhang",
            "Ge Yang",
            "Runwei Ding",
            "Yidi Li"
        ],
        "date": "1 September 2023",
        "abstract": "A novel cascade reconstruction-discriminant network (Cascade RDN), which adopts a cascade structure to obtain representative discriminative reconstruction embedding and achieves state-of-the-art anomaly classification scores on BTAD. Unsupervised visual anomaly detection uses only anomaly-free images to detect anomalous patterns, whose recent methods mainly focus on the anomaly classification sub-task but neglect to localize anomalies accurately. Existing reconstruction-based and representation-based methods yield anomaly score maps that often predict erroneous responses. The method jointly trained with the discriminative network is not suitable for anomalies other than texture types. This letter propose a novel cascade reconstruction-discriminant network (Cascade RDN), which adopts a cascade structure to obtain representative discriminative reconstruction embedding. The bi-direction channel attention module is designed to enable the two discriminative sub-networks to boost each other. Moreover, a general structural anomaly generation is presented to complement the existing textured anomaly generation to cover all types of surface anomalies. The proposed method outperforms previous anomaly localization methods by 7%-10% in AP on two challenging benchmarks MVTec AD and BTAD. Meanwhile, it achieves state-of-the-art anomaly classification scores on BTAD.",
        "references": [
            "95a26eafabf06b1fc5dec6c460a927cf5964e97e",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "9277dc70c74bcadf80dab11c28ead83fd085deec",
            "45535b86c60661dd4c4e4f375abae80937563499",
            "2c1837be2e4e6c4f47d51c42773e398897c372a6",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "41747cbdbed84762dfbfc305254c97021279dc6e"
        ],
        "related_topics": [
            "Surface Anomaly",
            "Anomaly Score Map",
            "MVTec AD",
            "Anomaly-free Images"
        ],
        "reference_count": "27",
        "citation_count": "One"
    },
    {
        "Id": "a76b4ff38181905e30c78e5da2c847a8ba1bcef9",
        "title": "Improving Vision Anomaly Detection with the Guidance of Language Modality",
        "authors": [
            "Dong Chen",
            "Kaihang Pan",
            "Guoming Wang",
            "Yueting Zhuang",
            "Siliang Tang"
        ],
        "date": "4 October 2023",
        "abstract": "This paper proposes Cross-modal Guidance (CMG), which consists of Cross-Modal Entropy Reduction (CMER) and Cross- modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. Recent years have seen a surge of interest in anomaly detection for tackling industrial defect detection, event detection, etc. However, existing unsupervised anomaly detectors, particularly those for the vision modality, face significant challenges due to redundant information and sparse latent space. Conversely, the language modality performs well due to its relatively single data. This paper tackles the aforementioned challenges for vision modality from a multimodal point of view. Specifically, we propose Cross-modal Guidance (CMG), which consists of Cross-modal Entropy Reduction (CMER) and Cross-modal Linear Embedding (CMLE), to tackle the redundant information issue and sparse space issue, respectively. CMER masks parts of the raw image and computes the matching score with the text. Then, CMER discards irrelevant pixels to make the detector focus on critical contents. To learn a more compact latent space for the vision anomaly detector, CMLE learns a correlation structure matrix from the language modality, and then the latent space of vision modality will be learned with the guidance of the matrix. Thereafter, the vision latent space will get semantically similar images closer. Extensive experiments demonstrate the effectiveness of the proposed methods. Particularly, CMG outperforms the baseline that only uses images by 16.81%. Ablation experiments further confirm the synergy among the proposed methods, as each component depends on the other to achieve optimal performance.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "1028c987f9d2dd0a1249714382e79f5c986e1804",
            "945b53ede48dae40af9870030fc4985a119cd1b8",
            "df5459473123b0e0a2fc78d812ec161f6ab0ee7e",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "2013f18832e0fb11c7aaf1f4f8da453aebda488c",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "0fe615dc0a422100e85cfb7e26c9306c481f6c75",
            "7f3bd25aab417d4a2c9ef19e18932ee6775300e8",
            "b95533611d08f00daa03fbf28a4c57456aaf2880"
        ],
        "related_topics": [],
        "reference_count": "43",
        "citation_count": "One"
    },
    {
        "Id": "58d59beb750e8c0e1d52b04dbc6c1c8b5ea3178a",
        "title": "Learning Global-Local Correspondence with Semantic Bottleneck for Logical Anomaly Detection",
        "authors": [
            "Haiming Yao",
            "Wenyong Yu",
            "Wei Luo",
            "Zhenfeng Qiang",
            "Donghao Luo",
            "Xiaotian Zhang"
        ],
        "date": "10 March 2023",
        "abstract": "This paper presents a novel framework, named Global-Local Correspondence Framework (GLCF), for visual anomaly detection with logical constraints, that consists of a local branch for detecting structural anomalies and a global Branch for detecting logical anomalies and introduces a novel semantic bottleneck enabled by the visual Transformer. This paper presents a novel framework, named Global-Local Correspondence Framework (GLCF), for visual anomaly detection with logical constraints. Visual anomaly detection has become an active research area in various real-world applications, such as industrial anomaly detection and medical disease diagnosis. However, most existing methods focus on identifying local structural degeneration anomalies and often fail to detect high-level functional anomalies that involve logical constraints. To address this issue, we propose a two-branch approach that consists of a local branch for detecting structural anomalies and a global branch for detecting logical anomalies. To facilitate local-global feature correspondence, we introduce a novel semantic bottleneck enabled by the visual Transformer. Moreover, we develop feature estimation networks for each branch separately to detect anomalies. Our proposed framework is validated using various benchmarks, including industrial datasets, Mvtec AD, Mvtec Loco AD, and the Retinal-OCT medical dataset. Experimental results show that our method outperforms existing methods, particularly in detecting logical anomalies.",
        "references": [
            "11709bfadfd6bbb371f4077bccb7c26d93c39cdd",
            "e26851c89afcfb1a0fe00292590c9f6e830c4ec0",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "6841ea2b9aead78b6335d6636f7e4ae7e33cbe4b",
            "7db5da4321d526539ac567fb56cd8900def4b1e5",
            "2c8e5a7c4643c92860497f2ff16bc9fe7fe1f046",
            "fc086bf5f6d1627153b68abdd5a4450e141b4ca3",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "1dec7a6629eb87952f3f1367eb2917d29870d4ed",
            "62b77e5cb85fc61b84edd532f6d65714be152596"
        ],
        "related_topics": [
            "Logical Anomalies",
            "Visual Anomaly Detection",
            "Semantic Bottlenecks",
            "Industrial Anomaly Detection",
            "Visual Transformers",
            "MVTec AD",
            "Industrial Datasets",
            "MVTec LOCO AD",
            "Logical Anomaly Detection"
        ],
        "reference_count": "49",
        "citation_count": "2"
    },
    {
        "Id": "e74a8db12f7514d481d3b695b8dbc661dbb26be2",
        "title": "FAIR: Frequency-aware Image Restoration for Industrial Visual Anomaly Detection",
        "authors": [
            "Tongkun Liu",
            "Bing Li",
            "Xiao Du",
            "Bingke Jiang",
            "Leqi Geng",
            "Feiyang Wang",
            "Zhuo Zhao"
        ],
        "date": "13 September 2023",
        "abstract": "Frequency-aware Image Restoration (FAIR), a novel self-supervised image restoration task that restores images from their high-frequency components, enables precise reconstruction of normal patterns while mitigating unfavorable generalization to anomalies. Image reconstruction-based anomaly detection models are widely explored in industrial visual inspection. However, existing models usually suffer from the trade-off between normal reconstruction fidelity and abnormal reconstruction distinguishability, which damages the performance. In this paper, we find that the above trade-off can be better mitigated by leveraging the distinct frequency biases between normal and abnormal reconstruction errors. To this end, we propose Frequency-aware Image Restoration (FAIR), a novel self-supervised image restoration task that restores images from their high-frequency components. It enables precise reconstruction of normal patterns while mitigating unfavorable generalization to anomalies. Using only a simple vanilla UNet, FAIR achieves state-of-the-art performance with higher efficiency on various defect detection datasets. Code: https://github.com/liutongkun/FAIR.",
        "references": [
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "2c8e5a7c4643c92860497f2ff16bc9fe7fe1f046",
            "230c875f7563abf2e11bc79c0ae8855bfa52123c",
            "e021d59638966a6fbb36854cc2cf1045de7a62d2",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "a70bc416b1124525499b0ac3d5b009637dc6c187",
            "2c1837be2e4e6c4f47d51c42773e398897c372a6",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "43"
    },
    {
        "Id": "2116df9be0019c173ec58a123aafec39a03a5827",
        "title": "Self-Supervised Visual Representation Learning via Residual Momentum",
        "authors": [
            "Trung Xuan Pham",
            "Axi Niu",
            "Zhang Kang",
            "Sultan Rizky Hikmawan Madjid",
            "Jiajing Hong",
            "Daehyeok Kim",
            "Joshua Tian Jin Tee",
            "Chang Dong Yoo"
        ],
        "date": "17 November 2022",
        "abstract": "This work highlights the importance of reducing the teacher-student intra-gap in momentum-based contrastive learning frameworks and provides a practical solution for improving the quality of learned representations. Self-supervised learning (SSL) has emerged as a promising approach for learning representations from unlabeled data. Momentum-based contrastive frameworks such as MoCo-v3 have shown remarkable success among the many SSL methods proposed in recent years. However, a significant gap in encoder representation exists between the online encoder (student) and the momentum encoder (teacher) in these frameworks, limiting the performance on downstream tasks. We identify this gap as a bottleneck often overlooked in existing frameworks and propose \u201cresidual momentum\u201d that explicitly reduces the gap during training to encourage the student to learn representations closer to the teacher\u2019s. We also reveal that a similar technique, knowledge distillation (KD), to reduce the distribution gap with cross-entropy-based loss in supervised learning is useless in the SSL context and demonstrate that the intra-representation gap measured by cosine similarity is crucial for EMA-based SSLs. Extensive experiments on different benchmark datasets and architectures demonstrate the superiority of our method compared to state-of-the-art contrastive learning baselines. Specifically, our method outperforms MoCo-v3 0.7% top-1 in ImageNet, 2.82% on CIFAR-100, 1.8% AP, and 3.0% AP75 on VOC detection pre-trained on the COCO dataset; it also improves DenseCL with 0.5% AP (800ep) and 0.6% AP75 (1600ep). Our work highlights the importance of reducing the teacher-student intra-gap in momentum-based contrastive learning frameworks and provides a practical solution for improving the quality of learned representations.",
        "references": [
            "9c1770c992c772916112507b53f3ffb9f259ba70",
            "2f399c4e23e540342e3ff6002be900833e96c8d6",
            "c3ef14ae4f90fa85b307415434c88df1028984ff",
            "4effd0f6b51b73335c3a2b7b880d168fbbe09298",
            "40b68df4635298c32725891bc46ee0201dac56c1",
            "d2b686b7480ab914c75bacb8357abbcf2a22bf00",
            "6f92dcefc5f6b4346f619ae7546a8bd2d6decade",
            "1e1e10d75c4ebabdbfb7912ca4cc06a27ffa85af",
            "8e90de490be759e15987168225f4add8e16810f8",
            "34733eaf66007516347a40ad5d9bbe1cc9dacb6b"
        ],
        "related_topics": [
            "Semi-Supervised Learning",
            "Momentum Encoders",
            "Self-Supervised Learning",
            "Benchmark Dataset",
            "Self-supervised"
        ],
        "reference_count": "91",
        "citation_count": "3"
    },
    {
        "Id": "3827ec14b1b6a1152f1b54c2339d98953dfcfae9",
        "title": "Large Language Model Guided Knowledge Distillation for Time Series Anomaly Detection",
        "authors": [
            "Chen Liu",
            "Shibo He",
            "Qihang Zhou",
            "Shizhong Li",
            "Wenchao Meng"
        ],
        "date": "26 January 2024",
        "abstract": "This work proposes AnomalyLLM, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. Self-supervised methods have gained prominence in time series anomaly detection due to the scarcity of available annotations. Nevertheless, they typically demand extensive training data to acquire a generalizable representation map, which conflicts with scenarios of a few available samples, thereby limiting their performance. To overcome the limitation, we propose \\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly detection approach where the student network is trained to mimic the features of the large language model (LLM)-based teacher network that is pretrained on large-scale datasets. During the testing phase, anomalies are detected when the discrepancy between the features of the teacher and student networks is large. To circumvent the student network from learning the teacher network's feature of anomalous samples, we devise two key strategies. 1) Prototypical signals are incorporated into the student network to consolidate the normal feature extraction. 2) We use synthetic anomalies to enlarge the representation gap between the two networks. AnomalyLLM demonstrates state-of-the-art performance on 15 datasets, improving accuracy by at least 14.5\\% in the UCR dataset.",
        "references": [
            "96086f8b0c05d7cbbd6f6ca7890dbab42f15ad75",
            "b949d5529ba08949ffc79ca82708dc6886ce4fca",
            "16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277",
            "d84cf745c534c010b8e55e5a4a04878906848dc3",
            "1725ad1d8cc0e539ac5d0a85657d5c95b4538c5e",
            "5b7f5488c380cf5085a5dd93e993ad293b225eee",
            "4ba2f8dfc1a4fccec80cf95ce3f0eeff3066f21e",
            "3082698875debe15b8e13ad3f26299243e513436",
            "18c44c6baf1d6cbc1c0f6286bf74aa3c7e5cea24",
            "e9dcd9b4a3ec96185757cc882d687553cbfa03cf"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "45"
    },
    {
        "Id": "4317714f4e427b5b7dc77870e59be00f5a89c43d",
        "title": "A Method for Image Anomaly Detection Based on Distillation and Reconstruction",
        "authors": [
            "Jiaxiang Luo",
            "Jianzhao Zhang"
        ],
        "date": "1 November 2023",
        "abstract": "A method for anomaly evaluation based on patch similarity that calculates the difference between the reconstructed image and the input image according to different regions of the image, thus improving the sensitivity and accuracy of the anomaly score. Image anomaly detection is a trending research topic in computer vision. The objective is to build models using available normal samples to detect various abnormal images without depending on real abnormal samples. It has high research significance and value for applications in the detection of defects in product appearance, medical image analysis, hyperspectral image processing, and other fields. This paper proposes an image anomaly detection algorithm based on feature distillation and an autoencoder structure, which uses the feature distillation structure of a dual-teacher network to train the encoder, thus suppressing the reconstruction of abnormal regions. This system also introduces an attention mechanism to highlight the detection objects, achieving effective detection of different defects in product appearance. In addition, this paper proposes a method for anomaly evaluation based on patch similarity that calculates the difference between the reconstructed image and the input image according to different regions of the image, thus improving the sensitivity and accuracy of the anomaly score. This paper conducts experiments on several datasets, and the results show that the proposed algorithm has superior performance in image anomaly detection. It achieves 98.8% average AUC on the SMDC-DET dataset and 98.9% average AUC on the MVTec-AD dataset.",
        "references": [
            "00ec1d0b26dd0221ee0c89a5bcb25e1855825ab3",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "cc0c5d8104cd1c1887ed192039a78b1e99fe1bb3",
            "a4b849c4537ec9f45d380a7bd1d9aa6e1baa9b53",
            "1086f9b2d3e9c13adddf99245089dbcdc72f83ef",
            "5fdd9cfb9b22b9e575d36e98e5d6f52a83729e42",
            "732750bec3b4d8c0108d6daed642500765d5c0ca",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "40"
    },
    {
        "Id": "41747cbdbed84762dfbfc305254c97021279dc6e",
        "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "6 November 2019",
        "abstract": "A powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images by trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2910bec6d4de87e22be5119cef3c488d2ae50e2a",
            "68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [
            "Uninformed Students",
            "Anomaly-free Images",
            "Per-region-overlap",
            "MVTec Anomaly Detection Dataset",
            "Unsupervised Anomaly Segmentation",
            "Anomaly-free Training Data",
            "Anomalous Regions",
            "MVTec AD",
            "Spatial Anomaly Map",
            "OCGAN"
        ],
        "reference_count": "37",
        "citation_count": "376"
    },
    {
        "Id": "5435a9ab36a308cef10bc725104e8f778ed3a328",
        "title": "Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection",
        "authors": [
            "Samet Ak\u00e7ay",
            "Amir Atapour-Abarghouei",
            "T. Breckon"
        ],
        "date": "25 January 2019",
        "abstract": "This work introduces an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Despite inherent ill-definition, anomaly detection is a research endeavour of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most significant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains at most a sub-set of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within image space and a lower-dimensional embedding vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.",
        "references": [
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "8381157eae4fbf8908d0312a9642f8e69e944449",
            "b3acb6f183b5f4b651f53c0eec5cb5c805224ac1",
            "559a52d27ff8e3ae0cdf1e7948c137ff566285c8",
            "39b8f34e71553622bb16b547211d0d769563c61d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
            "eb7ee0bc355652654990bcf9f92f124688fde493",
            "8388f1be26329fa45e5807e968a641ce170ea078"
        ],
        "related_topics": [
            "Skip-GANomaly",
            "X-ray Security Screening",
            "Anomaly Detection",
            "Adversarially Trained",
            "Machine Learning",
            "Skip Connections",
            "Visual Scene Understanding"
        ],
        "reference_count": "37",
        "citation_count": "265"
    },
    {
        "Id": "0535625be630c6a67f4c244ebf3aa61ad088fc70",
        "title": "GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training",
        "authors": [
            "Samet Ak\u00e7ay",
            "Amir Atapour-Abarghouei",
            "T. Breckon"
        ],
        "date": "17 May 2018",
        "abstract": "This work introduces a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space and shows the model efficacy and superiority over previous state-of-the-art approaches. Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insufficient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a significantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution - an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model efficacy and superiority over previous state-of-the-art approaches.",
        "references": [
            "e399a626ba21fafb19b3661603ec9724058e951b",
            "559a52d27ff8e3ae0cdf1e7948c137ff566285c8",
            "39b8f34e71553622bb16b547211d0d769563c61d",
            "c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
            "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
            "1db6e3078597386ac4222ba6c3f4f61b61f53539",
            "86ee1835a56722b76564119437070782fc90eb19",
            "7d0effebfa4bed19b6ba41f3af5b7e5b6890de87",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79"
        ],
        "related_topics": [
            "GANomaly",
            "Normal Samples",
            "AnoGAN",
            "Semi-supervised Anomaly Detection",
            "Encoder Loss",
            "Encoder-decoder-encoder Pipeline",
            "Normal Data Distribution",
            "Abnormal Samples",
            "Abnormal Images",
            "Bidirectional Generative Adversarial Networks"
        ],
        "reference_count": "55",
        "citation_count": "1,001"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "4d8abae45a5492ed2399fd5e25eeade8ac0bfa0f",
        "title": "Fence GAN: Towards Better Anomaly Detection",
        "authors": [
            "Cuong Phuc Ngo",
            "Amadeus Aristo Winarto",
            "Connie Khor Li Kou",
            "Sojeong Park",
            "Farhan Akram",
            "Hwee Kuan Lee"
        ],
        "date": "2 April 2019",
        "abstract": "With the modified GAN loss proposed, the anomaly detection method, called Fence GAN (FGAN), directly uses the discriminator score as an anomaly threshold and the experimental results show that FGAN yields the best anomaly classification accuracy compared to state-of-the-art methods. Anomaly detection is a classical problem where the aim is to detect anomalous data that do not belong to the normal data distribution. Current state-of-the-art methods for anomaly detection on complex high-dimensional data are based on the generative adversarial network (GAN). However, the traditional GAN loss is not directly aligned with the anomaly detection objective: it encourages the distribution of the generated samples to overlap with the real data and so the resulting discriminator is ineffective as an anomaly detector. In this paper, we propose modifications to the GAN loss such that the generated samples lie at the boundaries of the real data distribution. With our modified GAN loss, our anomaly detection method, called Fence GAN (FGAN), directly uses the discriminator score as an anomaly threshold. Our experimental results on the MNIST, CIFAR10 and KDD99 datasets show that FGAN yields the best anomaly classification accuracy compared to state-of-the-art methods.",
        "references": [
            "5f61089d3d548a515f01b473f0119137d1f340d4",
            "a47f8794d88c5c27123153c4eb9e08046e2b0c9d",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "b3acb6f183b5f4b651f53c0eec5cb5c805224ac1",
            "e399a626ba21fafb19b3661603ec9724058e951b",
            "732750bec3b4d8c0108d6daed642500765d5c0ca",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7"
        ],
        "related_topics": [
            "Fence GAN",
            "Anomaly Detection",
            "GAN Loss",
            "Discriminator",
            "Generative Adversarial Networks",
            "KDD99",
            "CIFAR-10",
            "Real Data Distribution",
            "Normal Data Distribution",
            "Anomaly Threshold"
        ],
        "reference_count": "34",
        "citation_count": "71"
    },
    {
        "Id": "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
        "title": "MVTec AD \u2014 A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "1 June 2019",
        "abstract": "This work introduces the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories, and conducts a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolved neural networks. The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the \ufb01eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the \ufb01rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.",
        "references": [
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "732c21998e251d64cd58b6a86886ee5907efeaa5",
            "9c24454b071bc8e96ea46c5064a7bddf07cca464",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "5d90f06bb70a0a3dced62413346235c02b1aa086"
        ],
        "related_topics": [
            "MVTec AD",
            "MVTec Anomaly Detection",
            "UnSupervised Anomaly Detection",
            "Texture Categories",
            "Metal Nut",
            "Anomalous Images",
            "MVTec AD Dataset",
            "Anomalous Regions",
            "Spatial Anomaly Map",
            "Anomaly-free Images"
        ],
        "reference_count": "29",
        "citation_count": "748"
    },
    {
        "Id": "8e180ffb0c4bfe4db41a245637042a28fc98d891",
        "title": "Learning Unsupervised Metaformer for Anomaly Detection",
        "authors": [
            "Jhih-Ciang Wu",
            "Ding-Jie Chen",
            "Chiou-Shann Fuh",
            "Tyng-Luh Liu"
        ],
        "date": "1 October 2021",
        "abstract": "This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap, and uses an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions. Anomaly detection (AD) aims to address the task of classification or localization of image anomalies. This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap. The former generalizes an AD model to tackling a broad range of object categories, while the latter provides useful clues for localizing abnormal regions. At the core of our method is an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions, i.e., to explore the reconstruction gap at those regions of interest. We justify the effectiveness of our method with SOTA results on the MVTec AD dataset of industrial images and highlight the adaptation flexibility of the universal Metaformer with multi-class and few-shot scenarios.",
        "references": [
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "bf79f0b8b68d7ba67420aa6f88dc7aa5eddd1d73",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "Reconstruction Gap",
            "Instance-aware Attention",
            "MetaFormer",
            "Anomaly Detection",
            "Industrial Images",
            "Object Categories",
            "Classification",
            "MVTec AD Dataset",
            "Few-shot Scenario"
        ],
        "reference_count": "39",
        "citation_count": "42"
    },
    {
        "Id": "98fa8f7b28f43830a22612be53bb393cf421bbc1",
        "title": "Anomaly detection with Wasserstein GAN",
        "authors": [
            "Ilyass Haloui",
            "Jayant Sen Gupta",
            "Vincent Feuillard"
        ],
        "date": "6 December 2018",
        "abstract": "W-GAN with encoder seems to produce state of the art anomaly detection scores on MNIST dataset and its usage on multi-variate time series is investigated. Generative adversarial networks are a class of generative algorithms that have been widely used to produce state-of-the-art samples. In this paper, we investigate GAN to perform anomaly detection on time series dataset. In order to achieve this goal, a bibliography is made focusing on theoretical properties of GAN and GAN used for anomaly detection. A Wasserstein GAN has been chosen to learn the representation of normal data distribution and a stacked encoder with the generator performs the anomaly detection. W-GAN with encoder seems to produce state of the art anomaly detection scores on MNIST dataset and we investigate its usage on multi-variate time series.",
        "references": [
            "b3acb6f183b5f4b651f53c0eec5cb5c805224ac1",
            "edf73ab12595c6709f646f542a0d2b33eb20a3f4",
            "559a52d27ff8e3ae0cdf1e7948c137ff566285c8",
            "13bc4e683075bdd6a3f0155241c276a772d4aa06",
            "1c5b16f980fedc25d9ad954f999e604df9f94fa7",
            "571b0750085ae3d939525e62af510ee2cee9d5ea",
            "1db6e3078597386ac4222ba6c3f4f61b61f53539",
            "488bb25e0b1777847f04c943e6dbc4f84415b712",
            "061146b1d7938d7a8dae70e3531a00fceb3c78e8",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79"
        ],
        "related_topics": [
            "Anomaly Detection",
            "Generative Adversarial Networks",
            "Wasserstein GANs",
            "MNIST Dataset",
            "Normal Data Distribution"
        ],
        "reference_count": "16",
        "citation_count": "13"
    },
    {
        "Id": "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
        "title": "Student-Teacher Feature Pyramid Matching for Unsupervised Anomaly Detection",
        "authors": [
            "Guodong Wang",
            "Shumin Han",
            "Errui Ding",
            "Di Huang"
        ],
        "date": "2021",
        "abstract": "This paper proposes a simple yet powerful approach to anomaly detection, implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Anomaly detection is a challenging task and usually for-mulated as an unsupervised learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and ef\ufb01ciency. Given a strong model pre-trained on image classi\ufb01cation as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature alignment enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on three major benchmarks, signi\ufb01cantly superior to the state of the art ones. In addition, it makes inferences at a very high speed (with 100 FPS for images of the size at 256 \u00d7 256), at least dozens of times faster than the latest counterparts.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7",
            "388645c44061f6e88fff0ecdad2f622936207d67",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced",
            "f076e4355c0facf111716dcab2837803367dd2d8",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2528a82dd2266600d4ee2b54165556a984de94d4"
        ],
        "related_topics": [
            "Student-Teacher Feature Pyramid Matching",
            "Pixel-level Anomaly Detection",
            "Anomaly-free Images",
            "Image-level Anomaly Detection",
            "MVTec AD Dataset",
            "Per-region-overlap",
            "Student Network",
            "Feature Pyramids",
            "Unsupervised Learning",
            "Scoring Function"
        ],
        "reference_count": "45",
        "citation_count": "63"
    },
    {
        "Id": "363c81a08858df8dd7d1bde79c6e002e3b19f900",
        "title": "Attribute Restoration Framework for Anomaly Detection",
        "authors": [
            "Fei Ye",
            "Chaoqin Huang",
            "Jinkun Cao",
            "Maosen Li",
            "Ya Zhang",
            "Cewu Lu"
        ],
        "date": "25 November 2019",
        "abstract": "This work proposes to break information equivalence among input and supervision for reconstruction tasks by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. With the recent advances in deep neural networks, anomaly detection in multimedia has received much attention in the computer vision community. While reconstruction-based methods have recently shown great promise for anomaly detection, the information equivalence among input and supervision for reconstruction tasks can not effectively force the network to learn semantic feature embeddings. We here propose to break this equivalence by erasing selected attributes from the original data and reformulate it as a restoration task, where the normal and the anomalous data are expected to be distinguishable based on restoration errors. Through forcing the network to restore the original image, the semantic feature embeddings related to the erased attributes are learned by the network. During testing phases, because anomalous data are restored with the attribute learned from the normal data, the restoration error is expected to be large. Extensive experiments have demonstrated that the proposed method significantly outperforms several state-of-the-arts on multiple benchmark datasets, especially on ImageNet, increasing the AUROC of the top-performing baseline by 10.1%. We also evaluate our method on a real-world anomaly detection dataset MVTec AD.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "7c4528f0ff263b6e7a6cee0e959f0e3615b3c65e",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "3447d8b47a8cf7ce9f04ede314f0ded8172fa470",
            "dbc7401e3e75c40d3c720e7db3c906d48bd742d7"
        ],
        "related_topics": [
            "MVTec AD",
            "Anomaly Detection",
            "Anomalous Data",
            "Supervision",
            "ImageNet",
            "Restoration Tasks",
            "Reconstruction-based Methods",
            "Area Under The Receiver Operating Characteristic Curve",
            "Normal Data",
            "Benchmark Dataset"
        ],
        "reference_count": "46",
        "citation_count": "127"
    },
    {
        "Id": "2b32b46f346d9b13268f0e74e5242a10a712a352",
        "title": "Self-Supervised Guided Segmentation Framework for Unsupervised Anomaly Detection",
        "authors": [
            "Peng Xing",
            "Yanpeng Sun",
            "Zechao Li"
        ],
        "date": "26 September 2022",
        "abstract": "A novel Self- Supervised Guided Segmentation Framework is proposed by jointly exploring effective generation method of forged anoma- lous samples and the normal sample features as the guidance information of segmentation for anomaly detection. \u2014Unsupervised anomaly detection is a challenging task in industrial applications since it is impracticable to col-lect suf\ufb01cient anomalous samples. In this paper, a novel Self- Supervised Guided Segmentation Framework (SGSF) is proposed by jointly exploring effective generation method of forged anoma- lous samples and the normal sample features as the guidance information of segmentation for anomaly detection. Speci\ufb01cally, to ensure that the generated forged anomaly samples are conducive to model training, the Saliency Augmentation Module (SAM) is proposed. SAM introduces a saliency map to generate saliency Perlin noise map, and develops an adaptive segmentation strategy to generate irregular masks in the saliency region. Then, the masks are utilized to generate forged anomalous samples as negative samples for training. Unfortunately, the distribution gap between forged and real anomaly samples makes it dif\ufb01cult for models trained based on forged samples to effectively locate real anomalies. Towards this end, the Self-supervised Guidance Network (SGN) is proposed. It leverages the self-supervised module to extract features that are noise-free and contain normal semantic information as the prior knowledge of the segmentation module. The segmentation module with the knowledge of normal patterns segments out the abnormal regions that are different from the guidance features. To evaluate the effectiveness of SGSF for anomaly detection, extensive experiments are conducted on three anomaly detection datasets. The experimental results show that SGSF achieves state-of-the-art anomaly detection results.",
        "references": [
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "e13d3f39cb9d03c57fef1344a825c163160dd8e7",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "363c81a08858df8dd7d1bde79c6e002e3b19f900",
            "95a26eafabf06b1fc5dec6c460a927cf5964e97e",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "c707517507873bc2cdc489b6fd9af74770468c48"
        ],
        "related_topics": [
            "Anomalous Samples",
            "Anomaly Detection",
            "Normal Patterns",
            "Negative Samples",
            "Samples",
            "Saliency Maps",
            "Anomaly Detection Datasets",
            "Irregular Masks",
            "UnSupervised Anomaly Detection",
            "Self-supervised"
        ],
        "reference_count": "51",
        "citation_count": "8"
    },
    {
        "Id": "8e180ffb0c4bfe4db41a245637042a28fc98d891",
        "title": "Learning Unsupervised Metaformer for Anomaly Detection",
        "authors": [
            "Jhih-Ciang Wu",
            "Ding-Jie Chen",
            "Chiou-Shann Fuh",
            "Tyng-Luh Liu"
        ],
        "date": "1 October 2021",
        "abstract": "This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap, and uses an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions. Anomaly detection (AD) aims to address the task of classification or localization of image anomalies. This paper addresses two pivotal issues of reconstruction-based approaches to AD in images, namely, model adaptation and reconstruction gap. The former generalizes an AD model to tackling a broad range of object categories, while the latter provides useful clues for localizing abnormal regions. At the core of our method is an unsupervised universal model, termed as Metaformer, which leverages both meta-learned model parameters to achieve high model adaptation capability and instance-aware attention to emphasize the focal regions for localizing abnormal regions, i.e., to explore the reconstruction gap at those regions of interest. We justify the effectiveness of our method with SOTA results on the MVTec AD dataset of industrial images and highlight the adaptation flexibility of the universal Metaformer with multi-class and few-shot scenarios.",
        "references": [
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "10a498003e9204f5fc1328e706510a37e514d8c7",
            "bf79f0b8b68d7ba67420aa6f88dc7aa5eddd1d73",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [
            "Reconstruction Gap",
            "Instance-aware Attention",
            "MetaFormer",
            "Anomaly Detection",
            "Industrial Images",
            "Object Categories",
            "Classification",
            "MVTec AD Dataset",
            "Few-shot Scenario"
        ],
        "reference_count": "39",
        "citation_count": "42"
    },
    {
        "Id": "41747cbdbed84762dfbfc305254c97021279dc6e",
        "title": "Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings",
        "authors": [
            "Paul Bergmann",
            "Michael Fauser",
            "David Sattlegger",
            "Carsten Steger"
        ],
        "date": "6 November 2019",
        "abstract": "A powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images by trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. We introduce a powerful student-teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of real-world datasets, including the recently introduced MVTec Anomaly Detection dataset that was specifically designed to benchmark anomaly segmentation algorithms.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "2910bec6d4de87e22be5119cef3c488d2ae50e2a",
            "68e7f5bcb2e2c628b15a96bfa72b612bd992a8e6",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "5f1e7c3c81d6a9e716eab660bb7536ecb204ef7d",
            "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "f88cfc38dec02dcf050eb1f56d2d59d90b24e04c",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "67b9c2b376a01d8757dc6d704be450d1c46c4ced"
        ],
        "related_topics": [
            "Uninformed Students",
            "Anomaly-free Images",
            "Per-region-overlap",
            "MVTec Anomaly Detection Dataset",
            "Unsupervised Anomaly Segmentation",
            "Anomaly-free Training Data",
            "Anomalous Regions",
            "MVTec AD",
            "Spatial Anomaly Map",
            "OCGAN"
        ],
        "reference_count": "37",
        "citation_count": "376"
    },
    {
        "Id": "d08775cf2bebcffa05c6fa506f687ef56953f128",
        "title": "AnoSeg: Anomaly Segmentation Network Using Self-Supervised Learning",
        "authors": [
            "Jou Won Song",
            "Kyeongbo Kong",
            "Ye In Park",
            "Seonggyun Kim",
            "Suk-Ju Kang"
        ],
        "date": "7 October 2021",
        "abstract": "The experiments show that the proposed AnoSeg outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset and compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks. Anomaly segmentation, which localizes defective areas, is an important component in large-scale industrial manufacturing. However, most recent researches have focused on anomaly detection. This paper proposes a novel anomaly segmentation network (AnoSeg) that can directly generate an accurate anomaly map using self-supervised learning. For highly accurate anomaly segmentation, the proposed AnoSeg considers three novel techniques: Anomaly data generation based on hard augmentation, self-supervised learning with pixel-wise and adversarial losses, and coordinate channel concatenation. First, to generate synthetic anomaly images and reference masks for normal data, the proposed method uses hard augmentation to change the normal sample distribution. Then, the proposed AnoSeg is trained in a self-supervised learning manner from the synthetic anomaly data and normal data. Finally, the coordinate channel, which represents the pixel location information, is concatenated to an input of AnoSeg to consider the positional relationship of each pixel in the image. The estimated anomaly map can also be utilized to improve the performance of anomaly detection. Our experiments show that the proposed method outperforms the state-of-the-art anomaly detection and anomaly segmentation methods for the MVTec AD dataset. In addition, we compared the proposed method with the existing methods through the intersection over union (IoU) metric commonly used in segmentation tasks and demonstrated the superiority of our method for anomaly segmentation.",
        "references": [
            "211b7caa1d47f22dc1f1b2f90b7710fc74e32d5d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "1a00dc525da31292e3734cbae2de681f114e30b1",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "eb60fe884c53b420edbce57059b242cfcbae0f7c",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "3447d8b47a8cf7ce9f04ede314f0ded8172fa470",
            "e13d3f39cb9d03c57fef1344a825c163160dd8e7"
        ],
        "related_topics": [
            "AnoSeg",
            "Synthetic Anomaly Data",
            "Anomaly Segmentation",
            "Self-Supervised Learning",
            "Normal Data",
            "Anomaly Detection",
            "Intersection Over Union",
            "Adversarial Loss",
            "MVTec AD Dataset",
            "Segmentation Task"
        ],
        "reference_count": "27",
        "citation_count": "30"
    },
    {
        "Id": "6517f92d519fc126cc18924231bafd8945a554d1",
        "title": "Reconstruction Student with Attention for Student-Teacher Pyramid Matching",
        "authors": [
            "Shinji Yamada",
            "Kazuhiro Hotta"
        ],
        "date": "30 November 2021",
        "abstract": "A powerful method which compensates for the shortcomings of Student-Teacher Feature Pyramid Matching (STPM), which can be trained from only normal images with small number of epochs is proposed. Anomaly detection and localization are important problems in computer vision. Recently, Convolutional Neural Network (CNN) has been used for visual inspection. In particular, the scarcity of anomalous samples increases the difficulty of this task, and unsupervised leaning based methods are attracting attention. We focus on Student-Teacher Feature Pyramid Matching (STPM) which can be trained from only normal images with small number of epochs. Here we proposed a powerful method which compensates for the shortcomings of STPM. Proposed method consists of two students and two teachers that a pair of student-teacher network is the same as STPM. The other student-teacher network has a role to reconstruct the features of normal products. By reconstructing the features of normal products from an abnormal image, it is possible to detect abnormalities with higher accuracy by taking the difference between them. The new student-teacher network uses attention modules and different teacher network from the original STPM. Attention mechanism acts to successfully reconstruct the normal regions in an input image. Different teacher network prevents looking at the same regions as the original STPM. Six anomaly maps obtained from the two student-teacher networks are used to calculate the final anomaly map. Student-teacher network for reconstructing features improved AUC scores for pixel level and image level in comparison with the original STPM.",
        "references": [
            "931dffdd56d60b6b9619a9f5ea8b533ec1ccdf04",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "82527ee075d2f7bf731da80edd8d4a92b01c2b8b",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "317c172f314f8cb634f7569ed5bf3ae7dd25c313",
            "e8874d7d585ae1c355e186efdcc9f704b3d43b49",
            "ad655c25e052fa4eeed53421344aca6f239c4c9d",
            "a8f3dc53e321fbb2565f5925def4365b9f68d1af",
            "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "31f9eb39d840821979e5df9f34a6e92dd9c879f2"
        ],
        "related_topics": [
            "STPM",
            "Convolutional Neural Network",
            "Image Level",
            "Pixel Level",
            "Attention Modules",
            "Anomaly Map",
            "Computer Vision",
            "Normal Images",
            "AUC Scores",
            "Anomaly Detection"
        ],
        "reference_count": "33",
        "citation_count": "17"
    },
    {
        "Id": "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
        "title": "CutPaste: Self-Supervised Learning for Anomaly Detection and Localization",
        "authors": [
            "Chun-Liang Li",
            "Kihyuk Sohn",
            "Jinsung Yoon",
            "Tomas Pfister"
        ],
        "date": "8 April 2021",
        "abstract": "This work proposes a two-stage framework for building anomaly detectors using normal training data only, which first learns self-supervised deep representations and then builds a generative one-class classifier on learned representations. We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We first learn self-supervised deep representations and then build a generative one-class classifier on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-the-art 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.",
        "references": [
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "5db790198b9acf4e5efe350acdd814238fcacaa7",
            "37595f7a51982d776e57c7280b9445474d90f0be",
            "41747cbdbed84762dfbfc305254c97021279dc6e",
            "16a67491ed4bdb6293d1c2be35b0e8bae962cdeb",
            "0535625be630c6a67f4c244ebf3aa61ad088fc70",
            "206c2e79b5f1b4541b85f47517666961ed49500e",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "30b99ae0682d42a2010be401dd1d8f7baca9bb5f"
        ],
        "related_topics": [
            "CutPaste",
            "Anomaly Score Map",
            "CutPaste Augmentation",
            "Pseudo Anomalies",
            "Uninformed Students",
            "Cut-Paste",
            "Semantic Anomaly Detection",
            "Image-level Anomaly Detection",
            "Gaussian Density Estimator",
            "MVTec Anomaly Detection Dataset"
        ],
        "reference_count": "67",
        "citation_count": "389"
    },
    {
        "Id": "b7d57cb75058728ba141b0fe4056d78d272c0d24",
        "title": "Few-shot Anomaly Detection in Text with Deviation Learning",
        "authors": [
            "Anindya Sundar Das",
            "Aravind Ajay",
            "Sriparna Saha",
            "Monowar H. Bhuyan"
        ],
        "date": "22 August 2023",
        "abstract": "FATE is introduced, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning and is optimized to learn the distinct behavior of anomalies. Most current methods for detecting anomalies in text concentrate on constructing models solely relying on unlabeled data. These models operate on the presumption that no labeled anomalous examples are available, which prevents them from utilizing prior knowledge of anomalies that are typically present in small numbers in many real-world applications. Furthermore, these models prioritize learning feature embeddings rather than optimizing anomaly scores directly, which could lead to suboptimal anomaly scoring and inefficient use of data during the learning process. In this paper, we introduce FATE, a deep few-shot learning-based framework that leverages limited anomaly examples and learns anomaly scores explicitly in an end-to-end method using deviation learning. In this approach, the anomaly scores of normal examples are adjusted to closely resemble reference scores obtained from a prior distribution. Conversely, anomaly samples are forced to have anomalous scores that considerably deviate from the reference score in the upper tail of the prior. Additionally, our model is optimized to learn the distinct behavior of anomalies by utilizing a multi-head self-attention layer and multiple instance learning approaches. Comprehensive experiments on several benchmark datasets demonstrate that our proposed approach attains a new level of state-of-the-art performance.",
        "references": [
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed",
            "1f5cfe35ada7bba999508ebc216deab4df77840b",
            "763e280d27e3ea65bc80eaeb7e5054e23ddd5ff2",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9",
            "6ec00ff233c19b47ef44dd57cdb22a7385586c0c",
            "96ed8ce9ef9fc475db9e02c79f984dc110409b62",
            "92584dac09356c3c2e915932956bdc06f91d453f"
        ],
        "related_topics": [
            "Anomaly Score",
            "Detecting Anomalies",
            "Multi-head Self Attention Layers",
            "Normal Examples",
            "Few Shot Anomaly Detection",
            "Feature Embeddings",
            "Prior Distribution",
            "Benchmark Dataset",
            "Federated AI Technology Enabler"
        ],
        "reference_count": "0",
        "citation_count": "38"
    },
    {
        "Id": "30aa23a6a32312666f2609339582744203024993",
        "title": "Deep Weakly-supervised Anomaly Detection",
        "authors": [
            "Guansong Pang",
            "Chunhua Shen",
            "Huidong Jin",
            "Anton van den Hengel"
        ],
        "date": "30 October 2019",
        "abstract": "Empirical results on 12 real-world datasets show that PReNet significantly outperforms nine competing methods in detecting seen and unseen anomalies, and theoretically and empirically justify the robustness of the model w.r.t. anomaly contamination in the unlabeled data. Recent semi-supervised anomaly detection methods that are trained using small labeled anomaly examples and large unlabeled data (mostly normal data) have shown largely improved performance over unsupervised methods. However, these methods often focus on fitting abnormalities illustrated by the given anomaly examples only (i.e., seen anomalies), and consequently they fail to generalize to those that are not, i.e., new types/classes of anomaly unseen during training. To detect both seen and unseen anomalies, we introduce a novel deep weakly-supervised approach, namely Pairwise Relation prediction Network (PReNet), that learns pairwise relation features and anomaly scores by predicting the relation of any two randomly sampled training instances, in which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and normal-normal pairwise discriminative patterns, respectively. PReNet can then detect any seen/unseen abnormalities that fit the learned pairwise abnormal patterns, or deviate from the normal patterns. Further, this pairwise approach also seamlessly and significantly augments the training anomaly data. Empirical results on 12 real-world datasets show that PReNet significantly outperforms nine competing methods in detecting seen and unseen anomalies. We also theoretically and empirically justify the robustness of our model w.r.t. anomaly contamination in the unlabeled data. The code is available at https://github.com/mala-lab/PReNet.",
        "references": [
            "8ed98bd58c799718d6fd389e2218bb89b1ecb9d7",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9",
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
            "d9e8925a5ccbb1db50c69dd6ccc18fe567f7fe12",
            "f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed",
            "de638f32e6e5762328357b855a9af3de8c20ea29",
            "cf2e99c821af11966bf415e8c277ae3eb9f8677c",
            "add459d9d37743dc2baad703fe17f794cb6b5d3f"
        ],
        "related_topics": [
            "Anomaly Score Learning",
            "DevNet",
            "Deviation Loss",
            "PReNet",
            "Anomaly Score",
            "Unlabeled-unlabeled",
            "Anomaly Contamination",
            "Generalize",
            "Unseen Anomalies",
            "Semi-supervised Anomaly Detection"
        ],
        "reference_count": "73",
        "citation_count": "47"
    },
    {
        "Id": "9f667d6cec1d607d729ac3a4b6ff9b673d634887",
        "title": "Few-shot Anomaly Detection and Classification Through Reinforced Data Selection",
        "authors": [
            "Xiao Han",
            "Depeng Xu",
            "Shuhan Yuan",
            "Xintao Wu"
        ],
        "date": "1 November 2022",
        "abstract": "This work proposes a few-shot anomaly detection and classification model through reinforced data selection (FADS), a novel framework that iteratively improves the performance of anomaly detectionand classification by exploring the unlabeled dataset to augment the training set. Due to the scarcity of anomalies, deep anomaly detection models are predominately trained in an unsupervised or semi-supervised manner depending on the availability of a small number of labeled samples. Currently, most unsupervised approaches detect anomalies by identifying the deviate patterns, and some semi-supervised studies also use labeled anomalies to improve performance. However, few studies have focused on how to take advantage of potential anomalies in an easily obtained and large-scale unlabeled dataset. Meanwhile, in a semi-supervised setting, although we assume having a small number of labeled anomalies, the task of anomaly classification is under-exploited. In this work, considering the problem of anomaly detection and classification by giving limited labeled samples as well as a large number of unlabeled samples, we propose a few-shot anomaly detection and classification model through reinforced data selection (FADS), a novel framework that iteratively improves the performance of anomaly detection and classification by exploring the unlabeled dataset to augment the training set. Experimental results show that FADS is able to improve the performance of anomaly detection and classification with only a few labeled samples initially.",
        "references": [
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "5365948c073e51a0568b20931c56dc8d6a6f94cb",
            "a2e667e4382aaa8e02a17d0522c1a910790ab65b",
            "6af440915b8a0718c93be1cf61905e41e620484a",
            "f4b74295a2aeaff7fab34a414e1feb5e52d52cb8",
            "c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3",
            "71d1ac92ad36b62a04f32ed75a10ad3259a7218d",
            "00a1077d298f2917d764eb729ab1bc86af3bd241",
            "08d2a01979a273766178c734e1478c68910a8e2b"
        ],
        "related_topics": [
            "Few Shot Anomaly Detection",
            "Unlabeled Dataset",
            "Factor Analysis Of Dynamic Structure",
            "Deep Anomaly Detection",
            "Samples",
            "Training Set",
            "Anomaly Detection",
            "Classification"
        ],
        "reference_count": "0",
        "citation_count": "24"
    },
    {
        "Id": "1922cfc82f3d3817b7dcca95adb609c607525979",
        "title": "RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision",
        "authors": [
            "Hongzuo Xu",
            "Yijie Wang",
            "Guansong Pang",
            "Songlei Jian",
            "Ninghui Liu",
            "Yongjun Wang"
        ],
        "date": "25 July 2023",
        "abstract": "Semantic Scholar extracted view of \"RoSAS: Deep Semi-Supervised Anomaly Detection with Contamination-Resilient Continuous Supervision\" by Hongzuo Xu et al.",
        "references": [
            "a4dc61de356af114bb18cc247f8b108558d157f7",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "16550cce229c84682cab742af75b28cde0cca731",
            "cf2e99c821af11966bf415e8c277ae3eb9f8677c",
            "23751b74d9fa42a8a041149018ddfe9efc0d4b7b",
            "30aa23a6a32312666f2609339582744203024993",
            "0b4bffc1f173147a732d2f2766cb1ccdf242e146",
            "7ac3e09d74ca15fdafa5f730617cd65059a144f3",
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0"
        ],
        "related_topics": [
            "Anomaly Contamination",
            "Labeled Anomalies",
            "Semi-supervised Anomaly Detection",
            "Deep Semi-supervised Anomaly Detection",
            "Unsupervised Models",
            "Inliers",
            "Unlabeled Anomalies",
            "Anomaly Score",
            "AUC-PR"
        ],
        "reference_count": "51",
        "citation_count": "2"
    },
    {
        "Id": "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
        "title": "Catching Both Gray and Black Swans: Open-set Supervised Anomaly Detection*",
        "authors": [
            "Choubo Ding",
            "Guansong Pang",
            "Chunhua Shen"
        ],
        "date": "28 March 2022",
        "abstract": "This paper proposes a novel approach that learns disentangled representations of abnormalities illustrated by seen anomalies, pseudo anomalies, and latent residual anomalies (i.e., samples that have unusual residuals compared to the normal data in a latent space), with the last two abnormalities designed to detect unseen anomalies. Despite most existing anomaly detection studies assume the availability of normal training samples only, a few labeled anomaly examples are often available in many real-world applications, such as defect samples identified during random quality inspection, lesion images confirmed by radiologists in daily medical screening, etc. These anomaly examples provide valuable knowledge about the application-specific abnormality, enabling significantly improved detection of similar anomalies in some recent models. However, those anomalies seen during training often do not illustrate every possible class of anomaly, rendering these models ineffective in generalizing to unseen anomaly classes. This paper tackles open-set supervised anomaly detection, in which we learn detection models using the anomaly examples with the objective to detect both seen anomalies (\u2018gray swans\u2019) and unseen anomalies (\u2018black swans\u2019). We propose a novel approach that learns disentangled representations of abnormalities illustrated by seen anomalies, pseudo anomalies, and latent residual anomalies (i.e., samples that have unusual residuals compared to the normal data in a latent space), with the last two abnormalities designed to detect unseen anomalies. Extensive experiments on nine real-world anomaly detection datasets show superior performance of our model in detecting seen and unseen anomalies under diverse settings. Code and data are available at: https://github.com/choubo/DRA",
        "references": [
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "8ed98bd58c799718d6fd389e2218bb89b1ecb9d7",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "c48def9076e58095c4aea49a8daa931af1990701",
            "2d8c97db4bae00ff243d122b957091a236a697a7",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "0c5ed0c30375703306f36d341d31772f3bd5af47",
            "f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9"
        ],
        "related_topics": [
            "Seen Anomalies",
            "Unseen Anomalies",
            "DevNet",
            "Fractional Lower Order Statistics",
            "Latent Space",
            "Pseudo Anomalies",
            "Disentangled Representations",
            "Supervised Anomaly Detection",
            "Radiologist",
            "Anomaly Class"
        ],
        "reference_count": "76",
        "citation_count": "42"
    },
    {
        "Id": "21484ffc19b1c5e41cbd49aff061db4dfb5286c2",
        "title": "Self-Supervised Normalizing Flows for Image Anomaly Detection and Localization",
        "authors": [
            "Li-Ling Chiu",
            "Shang-Hong Lai"
        ],
        "date": "1 June 2023",
        "abstract": "This work presents a novel self-supervised normalizing flow-based density estimation model, which is trained by maximizing the likelihood of normal images and minimizing thelihood of synthetic anomalous images, and improves the transformation subnet of the affine coupling layers in the flow- based model by dynamic stacking convolution and self-attention blocks. Image anomaly detection aims to detect out-of-distribution instances. Most existing methods treat anomaly detection as an unsupervised task because anomalous training data and labels are usually scarce or unavailable. Recently, image synthesis has been used to generate anomalous samples which deviate from normal sample distribution for model training. By using the synthesized anomalous training samples, we present a novel self-supervised normalizing flow-based density estimation model, which is trained by maximizing the likelihood of normal images and minimizing the likelihood of synthetic anomalous images. By adding constraints to abnormal samples in our loss function, our model training is focused on normal samples rather than synthetic samples. Moreover, we improve the transformation subnet of the affine coupling layers in our flow-based model by dynamic stacking convolution and self-attention blocks. We evaluate our method on MVTec-AD, BTAD, and DAGM datasets and achieve state-of-the-art performance compared to flow-based and self-supervised methods on both anomaly detection and localization tasks.",
        "references": [
            "11709bfadfd6bbb371f4077bccb7c26d93c39cdd",
            "78d80c343d36baaf89f18e12d325cf6309fb6c8f",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "2c89b183df320c3ef698989bdc5d1d4731c4d65d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "fc086bf5f6d1627153b68abdd5a4450e141b4ca3",
            "d08775cf2bebcffa05c6fa506f687ef56953f128",
            "5435a9ab36a308cef10bc725104e8f778ed3a328",
            "62b77e5cb85fc61b84edd532f6d65714be152596",
            "da003ed2925e3c8347b33de4e388cb9e6cd06893"
        ],
        "related_topics": [
            "Image Anomaly Detection",
            "Anomaly Detection",
            "MVTec AD",
            "Loss Function",
            "DAGM Dataset",
            "Localization Task",
            "Normal Images",
            "Affine Coupling Layer",
            "Image Synthesis"
        ],
        "reference_count": "49",
        "citation_count": "2"
    },
    {
        "Id": "941501b63b767a87a9727ad21f0f854ccd35ea73",
        "title": "A weakly supervised anomaly detection method based on deep anomaly scoring network",
        "authors": [
            "Xin Xie",
            "Zixi Li",
            "Yuhui Huang",
            "Dengquan Wu"
        ],
        "date": "19 May 2023",
        "abstract": "Comprehensive experiments on the challenging MVTec AD, KolektorSDD and ELPV datasets show that the proposed weakly supervised anomaly detection model achieves better results in anomaly detection and location, and has better robustness. Recently most anomaly detection methods mainly use normal samples or unlabeled data for training. Due to the lack of prior anomaly knowledge, normal samples with noisy data are easy to be misjudged as anomalies. Therefore, this paper proposes a weakly supervised anomaly detection model based on a deep anomaly scoring network. In this model, ResNet is used as a feature extraction network, and the Res2Net module is added to ResNet, which extracts multi-scale features at a fine-grained level to improve the multi-scale representation ability of the network. At the same time, efficient channel attention is introduced to enhance feature extraction performance by allocating the attention to feature channels. In addition, the anomaly score network calculates the anomaly score directly according to the extracted feature representation and optimizes the anomaly score in an end-to-end way. Comprehensive experiments on the challenging MVTec AD, KolektorSDD and ELPV datasets show that compared with the current advanced anomaly detection methods, our model achieves better results in anomaly detection and location, and has better robustness.",
        "references": [
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "78661cecf81340be9bd5720ac5ae97dc0e037bb9",
            "18abe29e2c50fa0b5c113a2e9458b89fb1197a8d",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "57b2198f9a8df773425aa6cc88c9870cb07779e2",
            "182d11020bf2842f135f1ec1dcac20237e0dc8b7",
            "d65eb30e5f0d2013fd5e4f45d1413bc2969ee803",
            "6af440915b8a0718c93be1cf61905e41e620484a"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "34"
    },
    {
        "Id": "553b25f1e371ae6bd7126af54206444043ee7da3",
        "title": "AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection",
        "authors": [
            "Qihang Zhou",
            "Guansong Pang",
            "Yu Tian",
            "Shibo He",
            "Jiming Chen"
        ],
        "date": "29 October 2023",
        "abstract": "A novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains, to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, \\eg, data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. However, their ZSAD performance is weak since the VLMs focus more on modeling the class semantics of the foreground objects rather than the abnormality/normality in the images. In this paper we introduce a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across different domains. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains. Code will be made available at https://github.com/zqhang/AnomalyCLIP.",
        "references": [
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "4b182347b943548fe6479393bb24adac21740675",
            "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "aa207668318fec38d60b79f407fb64982e46fce9",
            "7d90243c5a46430a36c5ba88627b5d254450a1e1",
            "6885c45614f78f9d2e7cc8ef11b3c38b34e67f7d",
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
            "2c1837be2e4e6c4f47d51c42773e398897c372a6",
            "0e8446c00ed21c19f62d71ab208a7b3601671766",
            "8e37b3b30a1fce1a3f41374ceaf168d2e79bf53f"
        ],
        "related_topics": [],
        "reference_count": "59",
        "citation_count": "6"
    },
    {
        "Id": "70672c29c0e48201e9d3e740255242e1a2fb4bf1",
        "title": "Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection",
        "authors": [
            "Jiawen Zhu",
            "Choubo Ding",
            "Yu Tian",
            "Guansong Pang"
        ],
        "date": "19 October 2023",
        "abstract": "A novel approach, namely Anomaly Heterogeneity Learning (AHL), that simulates a diverse set of heterogeneous anomaly distributions and then utilizes them to learn a unified heterogeneous abnormality model and is a generic framework that existing OSAD models can plug and play for enhancing their abnormality modeling. Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly detection area - aims at utilizing a few samples of anomaly classes seen during training to detect unseen anomalies (i.e., samples from open-set anomaly classes), while effectively identifying the seen anomalies. Benefiting from the prior knowledge illustrated by the seen anomalies, current OSAD methods can often largely reduce false positive errors. However, these methods treat the anomaly examples as from a homogeneous distribution, rendering them less effective in generalizing to unseen anomalies that can be drawn from any distribution. In this paper, we propose to learn heterogeneous anomaly distributions using the limited anomaly examples to address this issue. To this end, we introduce a novel approach, namely Anomaly Heterogeneity Learning (AHL), that simulates a diverse set of heterogeneous (seen and unseen) anomaly distributions and then utilizes them to learn a unified heterogeneous abnormality model. Further, AHL is a generic framework that existing OSAD models can plug and play for enhancing their abnormality modeling. Extensive experiments on nine real-world anomaly detection datasets show that AHL can 1) substantially enhance different state-of-the-art (SOTA) OSAD models in detecting both seen and unseen anomalies, achieving new SOTA performance on a large set of datasets, and 2) effectively generalize to unseen anomalies in new target domains.",
        "references": [
            "6a4f7514cf25a36b746b09eab4a2576a12961cb0",
            "8ed98bd58c799718d6fd389e2218bb89b1ecb9d7",
            "df4b7476c206621f14264ca421686b4adafd8506",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "30aa23a6a32312666f2609339582744203024993",
            "62d49fa60b54fed1e2a2cde3cb49d3639db76768",
            "3e90e30b2e8c11c4c459b32fbbe0cd0dbe95d2c3",
            "c48def9076e58095c4aea49a8daa931af1990701",
            "568a93409f91e959b075ffee9435204b4f15569c"
        ],
        "related_topics": [],
        "reference_count": "70",
        "citation_count": "One"
    },
    {
        "Id": "a78430349519ce3d3a4cf5e7dce7846740b4940a",
        "title": "Anomaly Detection with Score Distribution Discrimination",
        "authors": [
            "Minqi Jiang",
            "Songqiao Han",
            "Hailiang Huang"
        ],
        "date": "26 June 2023",
        "abstract": "This paper proposes to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. Recent studies give more attention to the anomaly detection (AD) methods that can leverage a handful of labeled anomalies along with abundant unlabeled data. These existing anomaly-informed AD methods rely on manually predefined score target(s), e.g., prior constant or margin hyperparameter(s), to realize discrimination in anomaly scores between normal and abnormal data. However, such methods would be vulnerable to the existence of anomaly contamination in the unlabeled data, and also lack adaptation to different data scenarios. In this paper, we propose to optimize the anomaly scoring function from the view of score distribution, thus better retaining the diversity and more fine-grained information of input data, especially when the unlabeled data contains anomaly noises in more practical AD scenarios. We design a novel loss function called Overlap loss that minimizes the overlap area between the score distributions of normal and abnormal samples, which no longer depends on prior anomaly score targets and thus acquires adaptability to various datasets. Overlap loss consists of Score Distribution Estimator and Overlap Area Calculation, which are introduced to overcome challenges when estimating arbitrary score distributions, and to ensure the boundness of training loss. As a general loss component, Overlap loss can be effectively integrated into multiple network architectures for constructing AD models. Extensive experimental results indicate that Overlap loss based AD models significantly outperform their state-of-the-art counterparts, and achieve better performance on different types of anomalies.",
        "references": [
            "180300ff8282220c76c7c41ded4d9d8c1be4d3fc",
            "1b44498c7bde6bc34d6489834eb5dcfbd3df1eb1",
            "f24dc0f3c1cc1da6c4fd12e1a65f55a6d82bb8ed",
            "4c1abd8969fc1c360f50373f6552bcfb3cc408b7",
            "cf2e99c821af11966bf415e8c277ae3eb9f8677c",
            "f1305bbd54db0345533906726e3425f742312c55",
            "30aa23a6a32312666f2609339582744203024993",
            "0bff4af924788d9779041513b6894385eac51ffd",
            "f5a951b9596be0df5ad7ede180b405c9e97a65c9",
            "8d53096bdf5c0b387fbad537a755151e015518ec"
        ],
        "related_topics": [
            "Anomaly Detection",
            "Scoring Function",
            "Anomaly Score",
            "Loss Function",
            "Anomaly Contamination",
            "Labeled Anomalies",
            "Overlap Area",
            "Training Loss"
        ],
        "reference_count": "77",
        "citation_count": "One"
    },
    {
        "Id": "514f596af7c89a49ba7e3abff03fe610718a3dbe",
        "title": "Morphological Profiling for Drug Discovery in the Era of Deep Learning",
        "authors": [
            "Qiaosi Tang",
            "Ranjala Ratnayake",
            "Gustavo Seabra",
            "Zhe Jiang",
            "Ruogu Fang",
            "Lina Cui",
            "Yousong Ding",
            "Tamer Kahveci",
            "Jiang Bian",
            "Chenglong Li",
            "Hendrik Luesch",
            "Yanjun Li"
        ],
        "date": "13 December 2023",
        "abstract": "This review provides a comprehensive overview of the recent advances in the field of morphological profiling, summarized the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering- and deep learning-based approaches, and introduce publicly available benchmark datasets. Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high-throughput. These efforts have facilitated understanding of compound mechanism-of-action (MOA), drug repurposing, characterization of cell morphodynamics under perturbation, and ultimately contributing to the development of novel therapeutics. In this review, we provide a comprehensive overview of the recent advances in the field of morphological profiling. We summarize the image profiling analysis workflow, survey a broad spectrum of analysis strategies encompassing feature engineering- and deep learning-based approaches, and introduce publicly available benchmark datasets. We place a particular emphasis on the application of deep learning in this pipeline, covering cell segmentation, image representation learning, and multimodal learning. Additionally, we illuminate the application of morphological profiling in phenotypic drug discovery and highlight potential challenges and opportunities in this field.",
        "references": [
            "b464bd244357544eed1a07106ff1d69ca9c6f17d",
            "930a3aabbaf3476807dd0fa0cd2830e371cb5b1c",
            "e76a11e7c081832c4865aa190e522d012faea590",
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "5df005c1a7e18d5b506b74896fbc4433103da744",
            "929a490198770abcb8c123d68a59384879b69adb",
            "cf533695bb16e80a922b3a6b3c5b420c2307afc5",
            "8f8a0de72c285f10bd3af311899026193eda632e",
            "0e3471582d6c32378677faeccaba6f080e44c6a4",
            "baf5b7d8e89c0bc2d85ed7b89ffc21017f2e0b66"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "175"
    },
    {
        "Id": "46276273cf92c31e103feab0cdc2ad0adda0dcc9",
        "title": "Interpreting Image\u2010based Profiles using Similarity Clustering and Single\u2010Cell Visualization",
        "authors": [
            "Fernanda Garcia-Fossa",
            "Mario Costa Cruz",
            "Marzieh Haghighi",
            "Marcelo Bispo de Jesus",
            "Shantanu Singh",
            "Anne E Carpenter",
            "Beth A. Cimini"
        ],
        "date": "1 March 2023",
        "abstract": "Two complementary protocols to help explore and interpret data from image\u2010based profiling experiments and provide scripts to create visualizations of representative single cells and image sites to understand how changes in features are reflected in the images are reflected. Image\u2010based profiling quantitatively assesses the effects of perturbations on cells by capturing a breadth of changes via microscopy. Here, we provide two complementary protocols to help explore and interpret data from image\u2010based profiling experiments. In the first protocol, we examine the similarity among perturbed cell samples using data from compounds that cluster by their mechanisms of action. The protocol includes steps to examine feature\u2010driving differences between samples and to visualize correlations between features and treatments to create interpretable heatmaps using the open\u2010source web tool Morpheus. In the second protocol, we show how to interactively explore images together with the numerical data, and we provide scripts to create visualizations of representative single cells and image sites to understand how changes in features are reflected in the images. Together, these two tutorials help researchers interpret image\u2010based data to speed up research. \u00a9 2023 The Authors. Current Protocols published by Wiley Periodicals LLC.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "4d0506d3e7f5dd23e1620cf8c48650bf1552bd3e",
            "36748de338909976f72ffbadaf097470ec040da0",
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "0e9789615e4f3a55d300b8ca46070efab8a93513",
            "0a82315290e1a527895075a5471d0a3c96c6027b",
            "faf891f3f9bd76dcdd348c0ce8c389782cd49c90",
            "650531d909502e0ac2a54feb5539ba1877bef141",
            "849a24bf0a00783e7d564c26c99ed8e2bf50b9f6"
        ],
        "related_topics": [],
        "reference_count": "29",
        "citation_count": "5"
    },
    {
        "Id": "881b3732b72fc42968e8ccda0c256f3f5d3a9540",
        "title": "Analysis and modeling of cancer drug responses using cell cycle phase-specific rate effects",
        "authors": [
            "Sean M. Gross",
            "Farnaz Mohammadi",
            "Crystal Sanchez-Aguila",
            "Paulina J. Zhan",
            "Tiera A. Liby",
            "Mark A. Dane",
            "Aaron S. Meyer",
            "Laura M. Heiser"
        ],
        "date": "15 January 2023",
        "abstract": "A linear chain trick computational model was developed, in which the cell cycle was partitioned into subphases that faithfully captured drug-induced dynamic responses and was used to predict the effect of unseen drug combinations that target cells in different cell cycle phases. Identifying effective therapeutic strategies that can prevent tumor cell proliferation is a major challenge to improving outcomes for patients with breast cancer. Here we sought to deepen our understanding of how clinically relevant anti-cancer agents modulate cell cycle progression. We genetically engineered breast cancer cell lines to express a cell cycle reporter and then tracked drug-induced changes in cell number and cell cycle phase, which revealed drug-specific cell cycle effects that varied across time. This suggested that a computational model that could account for cell cycle phase durations would provide a framework to explore drug-induced changes in cell cycle changes. Toward that goal, we developed a linear chain trick (LCT) computational model, in which the cell cycle was partitioned into subphases that faithfully captured drug-induced dynamic responses. The model inferred drug effects and localized them to specific cell cycle phases, which we confirmed experimentally. We then used our LCT model to predict the effect of unseen drug combinations that target cells in different cell cycle phases. Experimental testing confirmed several model predictions and identified combination treatment strategies that may improve therapeutic response in breast cancer patients. Overall, this integrated experimental and modeling approach opens new avenues for assessing drug responses, predicting effective drug combinations, and identifying optimal drug sequencing strategies.",
        "references": [
            "c4e50f18f6b597dc6a9221aaccc955372ed931bc",
            "c5ef4b6a41118145c5249c4c0c1854ed24cb46d9",
            "0950cf335baf14c08ae98140b8c2169347238bb8",
            "89f7dc171bd8e7a2b6bd3606c8b85bf1499e133d",
            "730de3e76071ce7c82a85de82d7bc359fecf0c68",
            "a673ed2042094fb8a1c94e04b3d143f410bc9a7b",
            "a07363c7bd1e1ee39f2aa22e3e0add9eb8fd4ca7",
            "65c488465b82e1aa46677008455044c1ac469043",
            "c4e4d7bf100121b3ad6feccde0fc040f7bb71c7f",
            "0babdc5493acddb6091a2c52b73fd07c66b6c1d6"
        ],
        "related_topics": [],
        "reference_count": "56",
        "citation_count": "4"
    },
    {
        "Id": "709596a6eac159b34a4b009fa8800c55c7e4e12a",
        "title": "Analysis and review of the possibility of using the generative model as a compression technique in DNA data storage: review and future research agenda",
        "authors": [
            "Muhammad Rafi Muttaqin",
            "Yeni Herdiyeni",
            "Agus Buono",
            "Karlisa Priandana",
            "Iskandar Zulkarnaen Siregar"
        ],
        "date": "1 November 2023",
        "abstract": "This paper aims to see if compression using this generative model allows it to be integrated into data storage methods on DNA, and highlights open problems that need to be solved and provides an identified research direction. The amount of data in this world is getting higher, and overwriting technology also has severe challenges. Data growth is expected to grow to 175 ZB by 2025. Data storage technology in DNA is an alternative technology with potential in information storage, mainly digital data. One of the stages of storing information on DNA is synthesis. This synthesis process costs very high, so it is necessary to integrate compression techniques for digital data to minimize the costs incurred. One of the models used in compression techniques is the generative model. This paper aims to see if compression using this generative model allows it to be integrated into data storage methods on DNA. To this end, we have conducted a Systematic Literature Review using the PRISMA method in selecting papers. We took the source of the papers from four leading databases and other additional databases. Out of 2440 papers, we finally decided on 34 primary papers for detailed analysis. This systematic literature review (SLR) presents and categorizes based on research questions, namely discussing machine learning methods applied in DNA storage, identifying compression techniques for DNA storage, knowing the role of deep learning in the compression process for DNA storage, knowing how generative models are associated with deep learning, knowing how generative models are applied in the compression process, and knowing latent space can be formed. The study highlights open problems that need to be solved and provides an identified research direction.",
        "references": [
            "68771b9d6b9451c36792c7a65264932a36a84d34",
            "c9cc6a3c50b32e801f20b4f0ed5daf3e9550f9c1",
            "9a02d920541471d4ff8d9ba6a59cfa4e753371ac",
            "4f65b8fd6643d2e13a05ebc07153a6eb18b46a32",
            "f19a2323f85a56b0f4c27c79234df291895c42cf",
            "ab2d58dc8129b8a237723eeccb91a91efed050a4",
            "ebe2036a5dcfbc744c2d503cd4e43de7f014e5b0",
            "64bd9e6a1d23caf72c1caf40eafc018175449127",
            "d2efa91bd7d076a66bc1d1c570f6aa2da944dd88",
            "d7900b14d7b379d3c9c2eed3a998b16e6df2c03b"
        ],
        "related_topics": [],
        "reference_count": "0",
        "citation_count": "57"
    },
    {
        "Id": "929a490198770abcb8c123d68a59384879b69adb",
        "title": "Comparison of Methods for Image-Based Profiling of Cellular Morphological Responses to Small-Molecule Treatment",
        "authors": [
            "Vebjorn Ljosa",
            "Peter David Caie",
            "Rob ter Horst",
            "Katherine L. Sokolnicki",
            "Emma L. Jenkins",
            "Sandeep Daya",
            "Mark E. Roberts",
            "Thouis Raymond Jones",
            "Shantanu Singh",
            "Auguste Genovesio",
            "Paul A. Clemons",
            "Neil O. Carragher",
            "Anne E Carpenter"
        ],
        "date": "17 September 2013",
        "abstract": "This work provides the complete ground-truth and test data sets, as well as open-source implementations of the various methods in a common software framework to facilitate the ready application and future development of image-based phenotypic profiling methods. Quantitative microscopy has proven a versatile and powerful phenotypic screening technique. Recently, image-based profiling has shown promise as a means for broadly characterizing molecules\u2019 effects on cells in several drug-discovery applications, including target-agnostic screening and predicting a compound\u2019s mechanism of action (MOA). Several profiling methods have been proposed, but little is known about their comparative performance, impeding the wider adoption and further development of image-based profiling. We compared these methods by applying them to a widely applicable assay of cultured cells and measuring the ability of each method to predict the MOA of a compendium of drugs. A very simple method that is based on population means performed as well as methods designed to take advantage of the measurements of individual cells. This is surprising because many treatments induced a heterogeneous phenotypic response across the cell population in each sample. Another simple method, which performs factor analysis on the cellular measurements before averaging them, provided substantial improvement and was able to predict MOA correctly for 94% of the treatments in our ground-truth set. To facilitate the ready application and future development of image-based phenotypic profiling methods, we provide our complete ground-truth and test data sets, as well as open-source implementations of the various methods in a common software framework.",
        "references": [
            "bfc8a8724b36cd2b1d068d1f997400e74791a68d",
            "8125727fe5b62492f4ccb1e25c66d473dd5c83c9",
            "3e5e4e1550416646ad33814ba3d6f20935ac742f",
            "140b7fab48719b56e933216594eaa8b5fc361c1b",
            "74b6fa545810f7953ca37ec9b765152d345a7081",
            "8b73514fd708f6bdd3786d47a8673a97fdce4dfe",
            "fdb0014af8197c8f303dca3fccad5d27bd264dcd",
            "7d1096bc057b6e5e08a6ae821768d4f742d37b77",
            "b4440c42d5ef6d0ae40f5646d1a6a1011333e7d3",
            "c1317f812bfaec25958ec7b5b583eb035fe6d5a1"
        ],
        "related_topics": [],
        "reference_count": "27",
        "citation_count": "143"
    },
    {
        "Id": "05ba3c532b7b98c9448da0bee52d7cba0ef29621",
        "title": "A dataset of images and morphological profiles of 30 000 small-molecule treatments using the Cell Painting assay",
        "authors": [
            "Mark-Anthony Bray",
            "Sigr{\\&#x27;u}n Margr{\\&#x27;e}t G{\\&#x27;u}stafsd{\\&#x27;o}ttir",
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Vebjorn Ljosa",
            "Katherine L. Sokolnicki",
            "Joshua A. Bittker",
            "Nicole E. Bodycombe",
            "Vlado Danc{\\&#x27;i}k",
            "Thomas P. Hasaka",
            "C. Suk-Yee Hon",
            "Melissa M. Kemp",
            "Kejie Li",
            "Deepika Walpita",
            "Mathias Wawer",
            "Todd R. Golub",
            "Stuart L. Schreiber",
            "Paul A. Clemons",
            "Alykhan F. Shamji",
            "Anne E Carpenter"
        ],
        "date": "7 January 2017",
        "abstract": "This microscopy dataset includes 919 265 five-channel fields of view, representing 30 616 tested compounds, available at \u201cThe Cell Image Library\u201d (CIL) repository, and includes data files containing morphological features derived from each cell in each image, both at the single-cell level and population-averaged level. Abstract Background Large-scale image sets acquired by automated microscopy of perturbed samples enable a detailed comparison of cell states induced by each perturbation, such as a small molecule from a diverse library. Highly multiplexed measurements of cellular morphology can be extracted from each image and subsequently mined for a number of applications. Findings This microscopy dataset includes 919 265 five-channel fields of view, representing 30 616 tested compounds, available at \u201cThe Cell Image Library\u201d (CIL) repository. It also includes data files containing morphological features derived from each cell in each image, both at the single-cell level and population-averaged (i.e., per-well) level; the image analysis workflows that generated the morphological features are also provided. Quality-control metrics are provided as metadata, indicating fields of view that are out-of-focus or containing highly fluorescent material or debris. Lastly, chemical annotations are supplied for the compound treatments applied. Conclusions Because computational algorithms and methods for handling single-cell morphological measurements are not yet routine, the dataset serves as a useful resource for the wider scientific community applying morphological (image-based) profiling. The dataset can be mined for many purposes, including small-molecule library enrichment and chemical mechanism-of-action studies, such as target identification. Integration with genetically perturbed datasets could enable identification of small-molecule mimetics of particular disease- or gene-related phenotypes that could be useful as probes or potential starting points for development of future therapeutics.",
        "references": [
            "36748de338909976f72ffbadaf097470ec040da0",
            "929a490198770abcb8c123d68a59384879b69adb",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "29c736eb38861ecf346ce49eedf163c03974566b",
            "aa0875ccc516862edc0b6bd2181ee27ce882933d",
            "0470122b71eeae19a9ae730b4d78243d8c4ffe1f",
            "214668b7baae58ac7df48f7ae0927a1701412d63",
            "006facfffb5b151536c4145e4b12a9fa534131c0",
            "80803cb840ed88b28a8cfb11794e946018eb4b78"
        ],
        "related_topics": [
            "Cell Painting Assay",
            "Morphological Profiles",
            "Cell State"
        ],
        "reference_count": "21",
        "citation_count": "111"
    },
    {
        "Id": "c4f82fe429d50064c371eb0ea97caa37b4a879cf",
        "title": "Capturing single-cell heterogeneity via data fusion improves image-based profiling",
        "authors": [
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "22 May 2018",
        "abstract": "The authors fuse information from the dispersion profiles with the average profiles at the level of profiles\u2019 similarity matrices for single cell imaging data, finding that data fusion is critical for these metrics to improve results over the prior alternatives. Single-cell resolution technologies warrant computational methods that capture cell heterogeneity while allowing efficient comparisons of populations. Here, we summarize cell populations by adding features\u2019 dispersion and covariances to population averages, in the context of image-based profiling. We find that data fusion is critical for these metrics to improve results over the prior alternatives, providing at least ~20% better performance in predicting a compound\u2019s mechanism of action (MoA) and a gene\u2019s pathway. A challenge with single-cell resolution methods is that cell heterogeneity should be captured while allowing for comparisons between populations. Here the authors fuse information from the dispersion profiles with the average profiles at the level of profiles\u2019 similarity matrices for single cell imaging data.",
        "references": [
            "ab0a1d18cec4925efd015ccb65a8e55b0cf545f9",
            "929a490198770abcb8c123d68a59384879b69adb",
            "36748de338909976f72ffbadaf097470ec040da0",
            "ef5b94fe62791dd2d3609fdaf9864d7b1908ae6b",
            "796f794f3aa5366e8f525a0d2c814fa19c917db4",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "05ba3c532b7b98c9448da0bee52d7cba0ef29621",
            "bd3f273590a34e121ddf5c2669c00750c8792e2b",
            "463b1c43dfd689841d2fd43d24058e68b7224dcf",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48"
        ],
        "related_topics": [
            "Image-based Profiling",
            "Mathematics Of Arrays"
        ],
        "reference_count": "21",
        "citation_count": "39"
    },
    {
        "Id": "8b73514fd708f6bdd3786d47a8673a97fdce4dfe",
        "title": "Multidimensional Drug Profiling By Automated Microscopy",
        "authors": [
            "Zachary E. Perlman",
            "Michael D. Slack",
            "Yan Feng",
            "Timothy J. Mitchison",
            "Lani F. Wu",
            "Steven J. Altschuler"
        ],
        "date": "12 November 2004",
        "abstract": "This method successfully categorized blinded drugs and suggested targets for drugs of uncertain mechanism with a titration-invariant similarity score (TISS) and is useful for discovering the mechanism and predicting the toxicity of new drugs. We present a method for high-throughput cytological profiling by microscopy. Our system provides quantitative multidimensional measures of individual cell states over wide ranges of perturbations. We profile dose-dependent phenotypic effects of drugs in human cell culture with a titration-invariant similarity score (TISS). This method successfully categorized blinded drugs and suggested targets for drugs of uncertain mechanism. Multivariate single-cell analysis is a starting point for identifying relationships among drug effects at a systems level and a step toward phenotypic profiling at the single-cell level. Our methods will be useful for discovering the mechanism and predicting the toxicity of new drugs.",
        "references": [
            "c9ce08df6e9e008538284be17ee35a7e743688dd",
            "b4626f94678714c172cbaa273564afb68e9937cd",
            "9217da1362b16c77c2576df7d0a11244438b7605",
            "feae36b19d8f8566aefbb8b30e9fb55c1592f0a7",
            "40331a31b6c0d045c5879db9505e6ce83423902f",
            "c6a180d752228b859b77cb6fdc65a043cbd2192e",
            "fe0db487bd82e00cca20024f3952d590bf4e2d90",
            "2cf150bbc19e78ee3ea297cc447616922a1894c7",
            "1a3d2bed793521813b5e561712d1835ce47e1623",
            "f2c26f632cae778d5acc7a221299dbdb789e4303"
        ],
        "related_topics": [],
        "reference_count": "59",
        "citation_count": "617"
    },
    {
        "Id": "c6bf43a2bfffec64989baa2d193a6a845defa59e",
        "title": "Systematic morphological profiling of human gene and allele function via Cell Painting",
        "authors": [
            "Mohammad Hossein Rohban",
            "Shantanu Singh",
            "Xiaoyun Wu",
            "Julia B Berthet",
            "Mark-Anthony Bray",
            "Yashaswi Shrestha",
            "Xaralabos Varelas",
            "Jesse S. Boehm",
            "Anne E Carpenter"
        ],
        "date": "18 March 2017",
        "abstract": "This work hypothesized that human genes and disease-associated alleles might be systematically functionally annotated using morphological profiling of cDNA constructs, via a microscopy-based Cell Painting assay, and confirmed the discovery of functional connectivity between the NF-\u03baB pathway and Hippo pathway effectors at the transcriptional level. We hypothesized that human genes and disease-associated alleles might be systematically functionally annotated using morphological profiling of cDNA constructs, via a microscopy-based Cell Painting assay. Indeed, 50% of the 220 tested genes yielded detectable morphological profiles, which grouped into biologically meaningful gene clusters consistent with known functional annotation (e.g., the RAS-RAF-MEK-ERK cascade). We used novel subpopulation-based visualization methods to interpret the morphological changes for specific clusters. This unbiased morphologic map of gene function revealed TRAF2/c-REL negative regulation of YAP1/WWTR1-responsive pathways. We confirmed this discovery of functional connectivity between the NF-\u03baB pathway and Hippo pathway effectors at the transcriptional level, thereby expanding knowledge of these two signaling pathways that critically regulate tumor initiation and progression. We make the images and raw data publicly available, providing an initial morphological map of major biological pathways for future study. DOI: http://dx.doi.org/10.7554/eLife.24060.001",
        "references": [
            "cd989ec0c4b17b06921668b2030020ce51736f7c",
            "1081628802c6362a39d9cde15499c232d98954f5",
            "bd12c6029622f68ca96c319a6d6c9a4d95059b15",
            "3e8d19aab7541c5ad404ceb514c819ed9d8b4352",
            "4cebfa50e9e48afc6260df161582f0271c84d86a",
            "36748de338909976f72ffbadaf097470ec040da0",
            "6d7fc69b2668ba7e9ed09797615da6e4cee0fcd1",
            "15ef9d4ffbd3006dd3683a793b35ddb57120c226",
            "cc87cd4e1431b405ac4a69237f9aa5a892431194",
            "929a490198770abcb8c123d68a59384879b69adb"
        ],
        "related_topics": [],
        "reference_count": "81",
        "citation_count": "103"
    },
    {
        "Id": "c6bb1291dc4d69d8362d05737dcd97020832c8d4",
        "title": "PhenoRipper: software for rapidly profiling microscopy images",
        "authors": [
            "Satwik Rajaram",
            "Benjamin Pavie",
            "Lani F. Wu",
            "Steven J. Altschuler"
        ],
        "date": "28 June 2012",
        "abstract": "PhenoRipper is an open-source software tool designed for rapid exploration of high-content microscopy images and does not replace traditional single cell\u2013based analysis approaches as it does not quantify properties such as area or average nuclear biomarker intensity. To the Editor: Recent advances in fluorescence microscopy have enabled unprecedented progress in many areas of biology. With the technology to perform high-content image-based screens now accessible to many labs, the analysis of the resulting large and complex data sets has become a bottleneck. Existing image analysis platforms1\u20133 offer flexible and sophisticated toolboxes for extracting biological information from image data. However, they can require steep learning curves, tuning of many parameters and long computational runtimes. There is an unmet need for easyto-use tools that enable bench scientists to rapidly interpret their image data sets. Here we describe PhenoRipper (Supplementary Software; updated versions available at http://www.phenoripper. org/), an open-source software tool designed for rapid exploration of high-content microscopy images (Fig. 1a and Supplementary Fig. 1). PhenoRipper permits rapid and intuitive comparison of images obtained under different experimental conditions based on image phenotype similarity. To minimize user input, PhenoRipper automatically identifies features from the images; users may only be required to modify default values of a few, visually interpretable, parameters. To increase speed, we chose a segmentation-free approach4,5: the software breaks images down into a square grid of blocks6\u20138 and performs analysis on these blocks rather than on individual cells. To capture heterogeneity, PhenoRipper identifies characteristic patterns of neighboring blocks and describes each image in terms of the occurrence frequencies of these patterns6,8. Finally, a simple graphical user interface, PhenoBrowser, is used to tie together images, features and profiles. Profiles can be annotated or combined (for example, by experimental or replicate conditions) to help interpret and explore their visual grouping. These design choices let users analyze their images an order of magnitude faster than existing unsupervised platforms (Supplementary Fig. 2). PhenoRipper does not replace traditional single cell\u2013based analysis approaches2,9,10 as it does not quantify properties such as area or average nuclear biomarker intensity. Nevertheless, the statistical properties of subcellular-scale phenotypes captured by PhenoRipper can be sufficient to accurately group cellular perturbations and identify outliers (Supplementary Fig. 3a). PhenoRipper\u2019s engine performs four major steps (Fig. 1a and Supplementary Fig. 1). (i) PhenoRipper identifies foreground blocks. Images are gridded to a user-specified block size (20\u201330 blocks per cell works well), and blocks are selected when the intensities of >50% of their pixels exceed a foreground threshold. This threshold is precalculated based on a small subset of images (Supplementary Methods), but it can easily be changed by the user. (ii) PhenoRipper identifies the most common foreground block types. To do this, it characterizes blocks by their distributions of assigned pixel colors and applies cluster analysis to classify them into different block types. This measurement is not sensitive to cell orientation and captures more information than simple averages (for example, a block with 50% red and 50% blue pixels would be different from a block with 100% purple pixels). (iii) PhenoRipper uses cluster analysis to identify superblock types, which represent the most common block type co-occurrence patterns within 3 \u00d7 3 block neighborhoods. The use of blocks and superblocks helps range of phenotypes, encompassing nontrivial population-level effects such as cell-type heterogeneity or local cell-density effects (Fig. 1c). Although realistic synthetic data cannot replace true experimental data6, SimuCell can be a useful part of the algorithm developer\u2019s toolbox by generating rich, flexible test image data sets containing specified, parameterized \u2018biological\u2019 effects.",
        "references": [
            "c2a7df55c168a3d719eb85248da41734d7a8cf30",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "cd989ec0c4b17b06921668b2030020ce51736f7c",
            "60a1a5b868e3a669b0e916506f3dda7636b2a4dd",
            "b58657eccdec07bd6b35ef00dab30670d1c9d5ed",
            "3c97f5a9d5df0ae0cf7085f0623032bdc85b6c77",
            "a9827fa24813c4366ae42dad7c404815bad68682",
            "23a3c3537a682767c5500fd0139081e9a417c84a",
            "f85bf84e3ffdce49f2d1c9cf12e79ad532eb8338"
        ],
        "related_topics": [
            "PhenoRipper"
        ],
        "reference_count": "9",
        "citation_count": "75"
    },
    {
        "Id": "36748de338909976f72ffbadaf097470ec040da0",
        "title": "Cell Painting, a high-content image-based assay for morphological profiling using multiplexed fluorescent dyes",
        "authors": [
            "Mark-Anthony Bray",
            "Shantanu Singh",
            "Han Han",
            "Chadwick T. Davis",
            "Blake Borgeson",
            "Cathy L. Hartland",
            "Maria Kost-Alimova",
            "Sigr{\\&#x27;u}n Margr{\\&#x27;e}t G{\\&#x27;u}stafsd{\\&#x27;o}ttir",
            "Christopher C. Gibson",
            "Anne E Carpenter"
        ],
        "date": "25 April 2016",
        "abstract": "This protocol describes the design and execution of experiments using Cell Painting, which is a morphological profiling assay that multiplexes six fluorescent dyes, imaged in five channels, to reveal eight broadly relevant cellular components or organelles. In morphological profiling, quantitative data are extracted from microscopy images of cells to identify biologically relevant similarities and differences among samples based on these profiles. This protocol describes the design and execution of experiments using Cell Painting, which is a morphological profiling assay that multiplexes six fluorescent dyes, imaged in five channels, to reveal eight broadly relevant cellular components or organelles. Cells are plated in multiwell plates, perturbed with the treatments to be tested, stained, fixed, and imaged on a high-throughput microscope. Next, an automated image analysis software identifies individual cells and measures \u223c1,500 morphological features (various measures of size, shape, texture, intensity, and so on) to produce a rich profile that is suitable for the detection of subtle phenotypes. Profiles of cell populations treated with different experimental perturbations can be compared to suit many goals, such as identifying the phenotypic impact of chemical or genetic perturbations, grouping compounds and/or genes into functional pathways, and identifying signatures of disease. Cell culture and image acquisition takes 2 weeks; feature extraction and data analysis take an additional 1\u20132 weeks.",
        "references": [
            "929a490198770abcb8c123d68a59384879b69adb",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "bfc8a8724b36cd2b1d068d1f997400e74791a68d",
            "4cebfa50e9e48afc6260df161582f0271c84d86a",
            "9e93ab699a70701f8100a7730b9f6ee250bbaa18",
            "a1e06f125aeb3899c0eda9e9286820ccff76a494",
            "ff8a3d41bf4a3a6415d77a84ba58cd9b0b2c4869",
            "fdb0014af8197c8f303dca3fccad5d27bd264dcd",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "cd989ec0c4b17b06921668b2030020ce51736f7c"
        ],
        "related_topics": [],
        "reference_count": "54",
        "citation_count": "514"
    },
    {
        "Id": "29c736eb38861ecf346ce49eedf163c03974566b",
        "title": "Applications in image-based profiling of perturbations.",
        "authors": [
            "Juan C. Caicedo",
            "Shantanu Singh",
            "Anne E Carpenter"
        ],
        "date": "1 June 2016",
        "abstract": "Semantic Scholar extracted view of \"Applications in image-based profiling of perturbations.\" by Juan C. Caicedo et al.",
        "references": [
            "aa0875ccc516862edc0b6bd2181ee27ce882933d",
            "929a490198770abcb8c123d68a59384879b69adb",
            "36748de338909976f72ffbadaf097470ec040da0",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "b90d44f59fcb74c71d3e31f67a3f09efab187a4e",
            "bfc8a8724b36cd2b1d068d1f997400e74791a68d",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "040b4f257b8b048ffb0e61e5ee4a8d16ce2d32da",
            "a878e1a752403b4cfb425c9b4d9459b9eb2b2cfe",
            "cd989ec0c4b17b06921668b2030020ce51736f7c"
        ],
        "related_topics": [],
        "reference_count": "106",
        "citation_count": "122"
    },
    {
        "Id": "0a87ac278987bbf3667fcc1ecefd378234c71628",
        "title": "BioProfiling.jl: profiling biological perturbations with high-content imaging in single cells and heterogeneous populations",
        "authors": [
            "Loan Vulliard",
            "Joel Hancock",
            "Anton Kamnev",
            "Christopher W Fell",
            "Joana Ferreira da Silva",
            "Joanna I. Loizou",
            "Vanja Nagy",
            "Lo{\\&quot;i}c Dupr{\\&#x27;e}",
            "J{\\&quot;o}rg Menche"
        ],
        "date": "18 June 2021",
        "abstract": "This work introduces BioProfiling.jl, an efficient end-to-end solution for compiling and filtering informative morphological profiles in Julia that simplifies visual artifact diagnostics, thus streamlining a bottleneck of morphological analyses. Motivation High-content imaging screens provide a cost-effective and scalable way to assess cell states across diverse experimental conditions. The analysis of the acquired microscopy images involves assembling and curating morphological measurements of individual cells into morphological profiles suitable for testing biological hypotheses. Despite being a critical step, there is currently no standard approach to morphological profiling and no solution is available for the high-performance Julia programming language. Results Here, we introduce BioProfiling.jl, an efficient end-to-end solution for compiling and filtering informative morphological profiles in Julia. The package contains all the necessary data structures to curate morphological measurements and helper functions to transform, normalize and visualize profiles. Robust statistical distances and permutation tests enable quantification of the significance of the observed changes despite the high fraction of outliers inherent to high-content screens. This package also simplifies visual artifact diagnostics, thus streamlining a bottleneck of morphological analyses. We showcase the features of the package by analyzing a chemical imaging screen, in which the morphological profiles prove to be informative about the compounds\u2019 mechanisms of action and can be conveniently integrated with the network localization of molecular targets. Availability The Julia package is available on GitHub: https://github.com/menchelab/BioProfiling.jl We also provide Jupyter notebooks reproducing our analyses: https://github.com/menchelab/BioProfilingNotebooks Contact joerg.menche@univie.ac.at",
        "references": [
            "36748de338909976f72ffbadaf097470ec040da0",
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "df775e0093d4e96e2690fa9f48f60d76e4fede19",
            "a830e3263742bc8a0fe93053142ee54b607bf042",
            "929a490198770abcb8c123d68a59384879b69adb",
            "463b1c43dfd689841d2fd43d24058e68b7224dcf",
            "8a7ce466e8f119f033aeace4210cc348ed62520b",
            "c6bf43a2bfffec64989baa2d193a6a845defa59e",
            "51f2eaeab4323871dfc899eac9fd9a2809811a58",
            "43ce7d2a4dc5fcba89371bb7d90863055a636f4a"
        ],
        "related_topics": [
            "Morphological Profiles",
            "Julia Programming Language",
            "Statistical Distance",
            "Morphological Profiling",
            "Single-cell",
            "Network Localization",
            "Cell State",
            "Jupyter Notebooks"
        ],
        "reference_count": "55",
        "citation_count": "5"
    },
    {
        "Id": "0e9789615e4f3a55d300b8ca46070efab8a93513",
        "title": "CellProfiler 4: improvements in speed, utility and usability",
        "authors": [
            "David R. Stirling",
            "Madison J. Swain-Bowden",
            "Alice M. Lucas",
            "Anne E Carpenter",
            "Beth A. Cimini",
            "Allen Goodman"
        ],
        "date": "30 June 2021",
        "abstract": "CellProfiler 4 provides significantly improved performance in complex workflows compared to previous versions, and introduced new modules to expand the capabilities of the software. Background Imaging data contains a substantial amount of information which can be difficult to evaluate by eye. With the expansion of high throughput microscopy methodologies producing increasingly large datasets, automated and objective analysis of the resulting images is essential to effectively extract biological information from this data. CellProfiler is a free, open source image analysis program which enables researchers to generate modular pipelines with which to process microscopy images into interpretable measurements. Results Herein we describe CellProfiler 4, a new version of this software with expanded functionality. Based on user feedback, we have made several user interface refinements to improve the usability of the software. We introduced new modules to expand the capabilities of the software. We also evaluated performance and made targeted optimizations to reduce the time and cost associated with running common large-scale analysis pipelines. Conclusions CellProfiler 4 provides significantly improved performance in complex workflows compared to previous versions. This release will ensure that researchers will have continued access to CellProfiler\u2019s powerful computational tools in the coming years.",
        "references": [
            "713e881b5c3134debf934026edf6f0ba3cb42c3c",
            "b66561622170d97f4127f4131d485faefe3833f7",
            "ff8a3d41bf4a3a6415d77a84ba58cd9b0b2c4869",
            "a08c0c4a7f4c0bbba714782b5696785856df8471",
            "8f1a8b82c7be223f195b4f03ffa1943391fd428b",
            "0a78085721f70d82c1284c124c3137bb7c2b34e7",
            "36748de338909976f72ffbadaf097470ec040da0",
            "a2e75b7d9727b7ea7cefb4060d93ac75813e3c48",
            "8aec428822299c52dc924bb45fcf37e6b20b7f52",
            "5d433da6d0f143f20936379910104d2bb139d4ae"
        ],
        "related_topics": [
            "CellProfiler",
            "Optimization",
            "Modular Pipeline"
        ],
        "reference_count": "19",
        "citation_count": "318"
    },
    {
        "Id": "46276273cf92c31e103feab0cdc2ad0adda0dcc9",
        "title": "Interpreting Image\u2010based Profiles using Similarity Clustering and Single\u2010Cell Visualization",
        "authors": [
            "Fernanda Garcia-Fossa",
            "Mario Costa Cruz",
            "Marzieh Haghighi",
            "Marcelo Bispo de Jesus",
            "Shantanu Singh",
            "Anne E Carpenter",
            "Beth A. Cimini"
        ],
        "date": "1 March 2023",
        "abstract": "Two complementary protocols to help explore and interpret data from image\u2010based profiling experiments and provide scripts to create visualizations of representative single cells and image sites to understand how changes in features are reflected in the images are reflected. Image\u2010based profiling quantitatively assesses the effects of perturbations on cells by capturing a breadth of changes via microscopy. Here, we provide two complementary protocols to help explore and interpret data from image\u2010based profiling experiments. In the first protocol, we examine the similarity among perturbed cell samples using data from compounds that cluster by their mechanisms of action. The protocol includes steps to examine feature\u2010driving differences between samples and to visualize correlations between features and treatments to create interpretable heatmaps using the open\u2010source web tool Morpheus. In the second protocol, we show how to interactively explore images together with the numerical data, and we provide scripts to create visualizations of representative single cells and image sites to understand how changes in features are reflected in the images. Together, these two tutorials help researchers interpret image\u2010based data to speed up research. \u00a9 2023 The Authors. Current Protocols published by Wiley Periodicals LLC.",
        "references": [
            "1e1f905c5d8c6a2ad18b09ce8eb5d2b4c6b174f5",
            "4d0506d3e7f5dd23e1620cf8c48650bf1552bd3e",
            "36748de338909976f72ffbadaf097470ec040da0",
            "42be3e7f9b95fcfa4455988a27dd17cf01d68507",
            "1f7bb6a080c1db96c778793514bf2b1638b623ab",
            "0e9789615e4f3a55d300b8ca46070efab8a93513",
            "0a82315290e1a527895075a5471d0a3c96c6027b",
            "faf891f3f9bd76dcdd348c0ce8c389782cd49c90",
            "650531d909502e0ac2a54feb5539ba1877bef141",
            "849a24bf0a00783e7d564c26c99ed8e2bf50b9f6"
        ],
        "related_topics": [],
        "reference_count": "29",
        "citation_count": "5"
    }
]